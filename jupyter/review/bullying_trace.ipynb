{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from six import string_types\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# import emoji\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ASCII(text):\n",
    "    return re.sub(\"([^\\x00-\\x7F])+\",\" \", text)\n",
    "\n",
    "def prediction(X, y, n, cv=False, k=10):\n",
    "\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform( np.asarray(y) )#.reshape(-1,1)\n",
    "    names = labels.classes_\n",
    "    print(\"shape of X:\", X.shape)\n",
    "    print(\"shape of y:\", y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=n)\n",
    "    print(\"shape of X_train:\", X_train.shape)\n",
    "    print(\"shape of y_train:\", y_train.shape)\n",
    "    print(\"shape of X_test:\", X_test.shape)\n",
    "    print(\"shape of y_test:\", y_test.shape)\n",
    "    \n",
    "    if cv:\n",
    "        lg = LogisticRegressionCV(cv=k, random_state=0)        \n",
    "    else:\n",
    "        lg = LogisticRegression()\n",
    "        \n",
    "    lg.fit(X_train, y_train)\n",
    "    y_pred = lg.predict(X_test).reshape(-1,1)\n",
    "    print(\"shape of y_pred:\", y_pred.shape)\n",
    "    print(clsr(y_test, y_pred, target_names=names))\n",
    "    print(cm(y_test, y_pred, labels=[0,1]))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print('naive model (only no)')\n",
    "    y_naive = np.array(['no']*len(y_test))\n",
    "    y_naive = labels.fit_transform(y_naive)#.reshape(-1,1)\n",
    "    print(clsr(y_test, y_naive, target_names=names))\n",
    "    print(cm(y_test, y_naive, labels=[1,0]))\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_3a.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_6a.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_2a.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_8a.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_4b.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_1b.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_7b.json',\n",
       " '/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab/random_tweets_5b.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_json = Path('C:\\\\Users\\\\niti.mishra\\\\Documents\\\\Personal\\\\cyberbullying\\\\data\\\\labelled_tweets')\n",
    "path_to_json = Path('/Users/peaceforlives/Documents/Projects/cyberbullying/data/labelled_tweets/ab')\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "# file_list = file_list[:-2] \n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>bullying_trace</th>\n",
       "      <th>bullying_role</th>\n",
       "      <th>form_of_bullying</th>\n",
       "      <th>bullying_post_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1175775344231628800</td>\n",
       "      <td>everyone take note!!!! most people forget they...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1160966582656327680</td>\n",
       "      <td>@chaewona_ @royalbiink hey not generalizing y'...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171979110802849792</td>\n",
       "      <td>@breaking911 bully gives clothes and films it ...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164840982149586944</td>\n",
       "      <td>the god of the old testament is \"jealous&amp;amp;p...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168206761553055744</td>\n",
       "      <td>yet the war criminal wants to spunk away £129 ...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         full_tweet  \\\n",
       "0  1175775344231628800  everyone take note!!!! most people forget they...   \n",
       "1  1160966582656327680  @chaewona_ @royalbiink hey not generalizing y'...   \n",
       "2  1171979110802849792  @breaking911 bully gives clothes and films it ...   \n",
       "3  1164840982149586944  the god of the old testament is \"jealous&amp;p...   \n",
       "4  1168206761553055744  yet the war criminal wants to spunk away £129 ...   \n",
       "\n",
       "  bullying_trace bullying_role form_of_bullying bullying_post_type  \n",
       "0             no          None             None               None  \n",
       "1             no          None             None               None  \n",
       "2             no          None             None               None  \n",
       "3             no          None             None               None  \n",
       "4             no          None             None               None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_json(file_list[0], orient=\"records\", lines=True, encoding='utf-8-sig')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bullying_trace'\n",
    "tweets = tweets[['id', 'full_tweet', target]]\n",
    "\n",
    "# tweets['full_tweet'] = [ [strip_ASCII(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_emoji(text):\n",
    "# #     print(emoji.emoji_count(text))\n",
    "#     new_text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "#     return new_text\n",
    "\n",
    "\n",
    "# tweets['full_tweet'] = [ [strip_emoji(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_repeat(text):  \n",
    "# #     return re.sub(r'(.)\\1+', r'\\1\\1', text) \n",
    "#     return re.sub(r'(\\w)\\1+', r'\\1', text)\n",
    "\n",
    "# strip_repeat('heheehehe')\n",
    "# # tweets['full_tweet'] = [ [strip_ASCII(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "# # tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets['len'] = tweets['full_tweet'].apply(len)\n",
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets.groupby('bullying_trace')['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>bullying_trace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1175775344231628800</td>\n",
       "      <td>everyone take note!!!! most people forget they...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1160966582656327680</td>\n",
       "      <td>@chaewona_ @royalbiink hey not generalizing y'...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171979110802849792</td>\n",
       "      <td>@breaking911 bully gives clothes and films it ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1164840982149586944</td>\n",
       "      <td>the god of the old testament is \"jealous&amp;amp;p...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168206761553055744</td>\n",
       "      <td>yet the war criminal wants to spunk away £129 ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         full_tweet  \\\n",
       "0  1175775344231628800  everyone take note!!!! most people forget they...   \n",
       "1  1160966582656327680  @chaewona_ @royalbiink hey not generalizing y'...   \n",
       "2  1171979110802849792  @breaking911 bully gives clothes and films it ...   \n",
       "3  1164840982149586944  the god of the old testament is \"jealous&amp;p...   \n",
       "4  1168206761553055744  yet the war criminal wants to spunk away £129 ...   \n",
       "\n",
       "  bullying_trace  \n",
       "0             no  \n",
       "1             no  \n",
       "2             no  \n",
       "3             no  \n",
       "4             no  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a53d01e1ed88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/twitter_cyberbullying/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/twitter_cyberbullying/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform([' '.join(tweet) for tweet in tweets['full_tweet'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.asarray(cv.get_feature_names())\n",
    "count = np.asarray( cv_fit.toarray().sum(axis=0) )\n",
    "corpusdictionary = dict(zip(words,count))\n",
    "\n",
    "count = pd.DataFrame.from_dict(corpusdictionary, orient='index', columns=['count'])\n",
    "count = count.sort_values(by=['count'], ascending=False)\n",
    "# count.to_csv('count_'+target+'.csv', index=True)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv_fit\n",
    "X.shape\n",
    "\n",
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, cv=True)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(100, X.shape[0], 500)\n",
    "scores = { }\n",
    "for i in idx: \n",
    "    print(i)\n",
    "    acc, f1 = prediction(X[:i,], y[:i], n=0.2, cv=True)\n",
    "    scores[i] = [acc , f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = LabelEncoder()\n",
    "y = labels.fit_transform( np.asarray(y) )\n",
    "names = labels.classes_\n",
    "print(\"shape of X:\", X.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test =tts(X, y, random_state=0, stratify=y, shuffle=True)\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)\n",
    "\n",
    "scores_copy = { }\n",
    "idx = range(200, X_train.shape[0], 300)\n",
    "for i in idx:\n",
    "    lg = LogisticRegressionCV(cv=10, random_state=0, max_iter=1000)         \n",
    "    lg.fit(X_train[:i,], y_train[:i])\n",
    "    y_pred = lg.predict(X_test)\n",
    "    print(\"shape of y_pred:\", y_pred.shape)\n",
    "    \n",
    "    print(clsr(y_test, y_pred))\n",
    "    print(cm(y_test, y_pred))#, labels=names))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "    \n",
    "    scores_copy[i] = [acc , f1]\n",
    "\n",
    "scores_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDIDF Vectorizer ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(words):\n",
    "    return words\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity, encoding='utf-8', preprocessor=None, use_idf=True,\n",
    "                             lowercase=False, ngram_range=(1,2)\n",
    "#                              , stop_words='english',\n",
    "#                              min_df=5, max_df=0.8\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fit = vectorizer.fit_transform([tweet for tweet in tweets['full_tweet']])\n",
    "X = tfidf_fit\n",
    "X.shape\n",
    "\n",
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.DataFrame(X)\n",
    "# X = pd.concat( [X, tweets['len']], axis=1)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(), columns=['idf_weights'])\n",
    "weights = df_idf.sort_values(by=['idf_weights'], ascending=False)\n",
    "weights.to_csv('tdidfweights_'+target+'.csv', index=True)\n",
    "# weights.to_csv('tdidfweights_noemoji.csv', index=True)\n",
    "weights\n",
    "# the lower the idf value of a word, the less unique it is to any particular document\n",
    "# terms with higher weight scores are considered to be more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdidf score of first tweet\n",
    "# if a word occurs multiple times in a document, we should boost its relevance as it should be \n",
    "# more meaningful than other words that appear fewer times (TF)\n",
    "# On the other hand, if a word occurs many times in all documents, maybe it is just a frequent word\n",
    "vector = pd.DataFrame(X[1].T.todense(), index=vectorizer.get_feature_names(), columns=['tdidf'])\n",
    "vector.sort_values(by=['tdidf'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = vectorizer.vocabulary_\n",
    "max_word = max(D, key=D.get)\n",
    "max_value = max(D.values())\n",
    "print(max_word, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, cv=True)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(100, X.shape[0], 500)\n",
    "scores_tfidf = { }\n",
    "for i in idx: \n",
    "    print(i)\n",
    "    acc, f1 = prediction(X[:i,], y[:i], n=0.2, cv=True)\n",
    "    scores_tfidf[i] = [acc , f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fit = vectorizer.fit_transform([tweet for tweet in tweets['full_tweet']])\n",
    "X = tfidf_fit\n",
    "X.shape\n",
    "\n",
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100  \n",
    "\n",
    "labels = LabelEncoder()\n",
    "y = labels.fit_transform( np.asarray(y) )\n",
    "names = labels.classes_\n",
    "print(\"shape of X:\", X.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test =tts(X, y, random_state=0, stratify=y, shuffle=True)\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)\n",
    "\n",
    "scores_copy = { }\n",
    "idx = range(200, X_train.shape[0], 200)\n",
    "for i in idx:\n",
    "    lg = LogisticRegressionCV(cv=10, random_state=0, max_iter=1000)         \n",
    "    lg.fit(X_train[:i,], y_train[:i])\n",
    "    y_pred = lg.predict(X_test)\n",
    "    print(\"shape of y_pred:\", y_pred.shape)\n",
    "    \n",
    "    print(clsr(y_test, y_pred))\n",
    "    print(cm(y_test, y_pred))#, labels=names))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "    \n",
    "    scores_copy[i] = [acc , f1]\n",
    "\n",
    "scores_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clsr(y_test, y_pred))\n",
    "print(cm(y_test, y_pred))#, labels=names))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, labels=np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ' '+ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  \n",
    "\n",
    "text = [ listToString(tweet) for tweet in tweets['full_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(n_features=5000)\n",
    "hash_fit = hash_vectorizer.transform(text)\n",
    "\n",
    "X = hash_fit\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, cv=True)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
