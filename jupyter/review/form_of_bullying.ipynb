{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "pd.set_option('display.max_columns', None)\n",
    "import emoji\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ASCII(text):\n",
    "    return re.sub(\"([^\\x00-\\x7F])+\",\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>bullying_trace</th>\n",
       "      <th>bullying_role</th>\n",
       "      <th>form_of_bullying</th>\n",
       "      <th>bullying_post_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1168174144724160518</td>\n",
       "      <td>[@user, bullying, and, help, be, two, differen...</td>\n",
       "      <td>yes</td>\n",
       "      <td>bully</td>\n",
       "      <td>general</td>\n",
       "      <td>denial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1162980168232853504</td>\n",
       "      <td>[@user, @user, i, mean, it, have, some, really...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1161905202527526912</td>\n",
       "      <td>[my, take, on, social, issue, like, this, be, ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>reporter</td>\n",
       "      <td>general</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1162217363023880193</td>\n",
       "      <td>[i, want, to, watch, a, group, of, character, ...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168710966659272705</td>\n",
       "      <td>[i, d, never, let, a, fucking, fetus, on, twit...</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         full_tweet  \\\n",
       "0  1168174144724160518  [@user, bullying, and, help, be, two, differen...   \n",
       "1  1162980168232853504  [@user, @user, i, mean, it, have, some, really...   \n",
       "2  1161905202527526912  [my, take, on, social, issue, like, this, be, ...   \n",
       "3  1162217363023880193  [i, want, to, watch, a, group, of, character, ...   \n",
       "4  1168710966659272705  [i, d, never, let, a, fucking, fetus, on, twit...   \n",
       "\n",
       "  bullying_trace bullying_role form_of_bullying bullying_post_type  \n",
       "0            yes         bully          general             denial  \n",
       "1             no          None             None               None  \n",
       "2            yes      reporter          general             report  \n",
       "3             no          None             None               None  \n",
       "4             no          None             None               None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_json = Path('C:\\\\Users\\\\niti.mishra\\\\Documents\\\\Personal\\\\cyberbullying\\\\data\\\\labelled_tweets')\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "# file_list = file_list[:-2] \n",
    "tweets = pd.read_json(file_list[0],lines=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3089, 3)\n",
      "general     1830\n",
      "cyber       1059\n",
      "verbal       106\n",
      "physical      94\n",
      "Name: form_of_bullying, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>form_of_bullying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1168174144724160518</td>\n",
       "      <td>[@user, bullying, and, help, be, two, differen...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1161905202527526912</td>\n",
       "      <td>[my, take, on, social, issue, like, this, be, ...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1163814523683270656</td>\n",
       "      <td>[my, sister, be, bully, this, girl, yang, baru...</td>\n",
       "      <td>cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1160014315828723713</td>\n",
       "      <td>[@user, you, re, give, someone, point, for, de...</td>\n",
       "      <td>cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1170474542415745024</td>\n",
       "      <td>[if, i, say, something, about, you, i, will, s...</td>\n",
       "      <td>cyber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                         full_tweet  \\\n",
       "0   1168174144724160518  [@user, bullying, and, help, be, two, differen...   \n",
       "2   1161905202527526912  [my, take, on, social, issue, like, this, be, ...   \n",
       "8   1163814523683270656  [my, sister, be, bully, this, girl, yang, baru...   \n",
       "11  1160014315828723713  [@user, you, re, give, someone, point, for, de...   \n",
       "14  1170474542415745024  [if, i, say, something, about, you, i, will, s...   \n",
       "\n",
       "   form_of_bullying  \n",
       "0           general  \n",
       "2           general  \n",
       "8             cyber  \n",
       "11            cyber  \n",
       "14            cyber  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'form_of_bullying'\n",
    "tweets = tweets[['id', 'full_tweet', target]]\n",
    "tweets.dropna(inplace=True)\n",
    "print(tweets.shape)\n",
    "print(tweets[target].value_counts())\n",
    "tweets['full_tweet'] = [ [strip_ASCII(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_emoji(text):\n",
    "# #     print(emoji.emoji_count(text))\n",
    "#     new_text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "#     return new_text\n",
    "\n",
    "\n",
    "# tweets['full_tweet'] = [ [strip_emoji(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_repeat(text):  \n",
    "# #     return re.sub(r'(.)\\1+', r'\\1\\1', text) \n",
    "#     return re.sub(r'(\\w)\\1+', r'\\1', text)\n",
    "\n",
    "# strip_repeat('heheehehe')\n",
    "# # tweets['full_tweet'] = [ [strip_ASCII(token) for token in tweet] for tweet in tweets['full_tweet'] ]\n",
    "# # tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets['len'] = tweets['full_tweet'].apply(len)\n",
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets.groupby('bullying_trace')['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform([' '.join(tweet) for tweet in tweets['full_tweet'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>3662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>2985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bully</th>\n",
       "      <td>2930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imitate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imessage</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbecile</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zumo</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7239 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "be         4800\n",
       "user       3662\n",
       "to         2985\n",
       "and        2956\n",
       "bully      2930\n",
       "...         ...\n",
       "imitate       1\n",
       "imi           1\n",
       "imessage      1\n",
       "imbecile      1\n",
       "zumo          1\n",
       "\n",
       "[7239 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.asarray(cv.get_feature_names())\n",
    "count = np.asarray( cv_fit.toarray().sum(axis=0) )\n",
    "corpusdictionary = dict(zip(words,count))\n",
    "\n",
    "count = pd.DataFrame.from_dict(corpusdictionary, orient='index', columns=['count'])\n",
    "count = count.sort_values(by=['count'], ascending=False)\n",
    "count.to_csv('count_'+target+'.csv', index=True)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "general     59.242473\n",
       "cyber       34.282939\n",
       "verbal       3.431531\n",
       "physical     3.043056\n",
       "Name: form_of_bullying, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cv_fit\n",
    "X.shape\n",
    "\n",
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, y, n=0.2, cv=False, k=10, binary=True):\n",
    "\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform( np.asarray(y) )#.reshape(-1,1)\n",
    "    names = labels.classes_\n",
    "    print(\"shape of X:\", X.shape)\n",
    "    print(\"shape of y:\", y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, random_state=0, stratify=y, shuffle=True)\n",
    "    print(\"shape of X_train:\", X_train.shape)\n",
    "    print(\"shape of y_train:\", y_train.shape)\n",
    "    print(\"shape of X_test:\", X_test.shape)\n",
    "    print(\"shape of y_test:\", y_test.shape)\n",
    "    \n",
    "#     if cv:\n",
    "#         clf = LogisticRegressionCV(cv=k, random_state=0, max_iter=1000)         \n",
    "#     else:\n",
    "#         clf = LogisticRegression()\n",
    "\n",
    "    clf = svm.SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "              decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
    "              probability=False, random_state=None, shrinking=True, tol=0.001,\n",
    "              verbose=False)\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)#.reshape(-1,1)\n",
    "    print(\"shape of y_pred:\", y_pred.shape)\n",
    "    print(clsr(y_test, y_pred))#, target_names=names))\n",
    "    print(cm(y_test, y_pred))#, labels=[0,1,2,3]))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    if not binary:\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "    else:\n",
    "        f1 = f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "\n",
    "    print('naive model (only no)')\n",
    "    y_naive = np.array(['general']*len(y_test))\n",
    "    y_naive = labels.fit_transform(y_naive)#.reshape(-1,1)\n",
    "    print(clsr(y_test, y_naive))#, target_names=names))\n",
    "    print(cm(y_test, y_naive, labels=[1,0]))\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (3089, 7239)\n",
      "shape of y: (3089,)\n",
      "shape of X_train: (2316, 7239)\n",
      "shape of y_train: (2316,)\n",
      "shape of X_test: (773, 7239)\n",
      "shape of y_test: (773,)\n",
      "shape of y_pred: (773, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.70      0.75      0.72       265\n",
      "     general       0.77      0.79      0.78       458\n",
      "    physical       0.36      0.22      0.27        23\n",
      "      verbal       0.33      0.07      0.12        27\n",
      "\n",
      "    accuracy                           0.74       773\n",
      "   macro avg       0.54      0.46      0.47       773\n",
      "weighted avg       0.72      0.74      0.72       773\n",
      "\n",
      "[[198  65   2   0]\n",
      " [ 83 364   7   4]\n",
      " [  0  18   5   0]\n",
      " [  1  24   0   2]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       265\n",
      "     general       0.00      0.00      0.00       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.34       773\n",
      "   macro avg       0.09      0.25      0.13       773\n",
      "weighted avg       0.12      0.34      0.18       773\n",
      "\n",
      "[[  0 458]\n",
      " [  0 265]]\n",
      "0.7360931435963778 0.7247629100963185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, binary=False)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (3089, 7239)\n",
      "shape of y: (3089,)\n",
      "shape of X_train: (2316, 7239)\n",
      "shape of y_train: (2316,)\n",
      "shape of X_test: (773, 7239)\n",
      "shape of y_test: (773,)\n",
      "shape of y_pred: (773, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.70      0.75      0.72       265\n",
      "     general       0.77      0.79      0.78       458\n",
      "    physical       0.36      0.22      0.27        23\n",
      "      verbal       0.33      0.07      0.12        27\n",
      "\n",
      "    accuracy                           0.74       773\n",
      "   macro avg       0.54      0.46      0.47       773\n",
      "weighted avg       0.72      0.74      0.72       773\n",
      "\n",
      "[[198  65   2   0]\n",
      " [ 83 364   7   4]\n",
      " [  0  18   5   0]\n",
      " [  1  24   0   2]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       265\n",
      "     general       0.00      0.00      0.00       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.34       773\n",
      "   macro avg       0.09      0.25      0.13       773\n",
      "weighted avg       0.12      0.34      0.18       773\n",
      "\n",
      "[[  0 458]\n",
      " [  0 265]]\n",
      "0.7360931435963778 0.7247629100963185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, cv=True, binary=False)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "shape of X: (100, 7239)\n",
      "shape of y: (100,)\n",
      "shape of X_train: (75, 7239)\n",
      "shape of y_train: (75,)\n",
      "shape of X_test: (25, 7239)\n",
      "shape of y_test: (25,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (25, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.54      0.78      0.64         9\n",
      "     general       0.75      0.64      0.69        14\n",
      "    physical       0.00      0.00      0.00         1\n",
      "      verbal       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.32      0.36      0.33        25\n",
      "weighted avg       0.61      0.64      0.62        25\n",
      "\n",
      "[[7 2 0 0]\n",
      " [5 9 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.36      1.00      0.53         9\n",
      "     general       0.00      0.00      0.00        14\n",
      "    physical       0.00      0.00      0.00         1\n",
      "      verbal       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.36        25\n",
      "   macro avg       0.09      0.25      0.13        25\n",
      "weighted avg       0.13      0.36      0.19        25\n",
      "\n",
      "[[ 0 14]\n",
      " [ 0  9]]\n",
      "400\n",
      "shape of X: (400, 7239)\n",
      "shape of y: (400,)\n",
      "shape of X_train: (300, 7239)\n",
      "shape of y_train: (300,)\n",
      "shape of X_test: (100, 7239)\n",
      "shape of y_test: (100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (100, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.60      0.50      0.55        30\n",
      "     general       0.69      0.87      0.77        60\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.67       100\n",
      "   macro avg       0.32      0.34      0.33       100\n",
      "weighted avg       0.60      0.67      0.63       100\n",
      "\n",
      "[[15 15  0  0]\n",
      " [ 8 52  0  0]\n",
      " [ 1  4  0  0]\n",
      " [ 1  4  0  0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.30      1.00      0.46        30\n",
      "     general       0.00      0.00      0.00        60\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.30       100\n",
      "   macro avg       0.07      0.25      0.12       100\n",
      "weighted avg       0.09      0.30      0.14       100\n",
      "\n",
      "[[ 0 60]\n",
      " [ 0 30]]\n",
      "700\n",
      "shape of X: (700, 7239)\n",
      "shape of y: (700,)\n",
      "shape of X_train: (525, 7239)\n",
      "shape of y_train: (525,)\n",
      "shape of X_test: (175, 7239)\n",
      "shape of y_test: (175,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (175, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.63      0.60      0.62        53\n",
      "     general       0.77      0.86      0.81       112\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.73       175\n",
      "   macro avg       0.35      0.37      0.36       175\n",
      "weighted avg       0.69      0.73      0.71       175\n",
      "\n",
      "[[32 21  0  0]\n",
      " [16 96  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 3  2  0  0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.30      1.00      0.46        53\n",
      "     general       0.00      0.00      0.00       112\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.30       175\n",
      "   macro avg       0.08      0.25      0.12       175\n",
      "weighted avg       0.09      0.30      0.14       175\n",
      "\n",
      "[[  0 112]\n",
      " [  0  53]]\n",
      "1000\n",
      "shape of X: (1000, 7239)\n",
      "shape of y: (1000,)\n",
      "shape of X_train: (750, 7239)\n",
      "shape of y_train: (750,)\n",
      "shape of X_test: (250, 7239)\n",
      "shape of y_test: (250,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (250, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.67      0.68      0.68        78\n",
      "     general       0.78      0.83      0.81       157\n",
      "    physical       0.00      0.00      0.00         7\n",
      "      verbal       0.50      0.12      0.20         8\n",
      "\n",
      "    accuracy                           0.74       250\n",
      "   macro avg       0.49      0.41      0.42       250\n",
      "weighted avg       0.72      0.74      0.72       250\n",
      "\n",
      "[[ 53  25   0   0]\n",
      " [ 24 131   1   1]\n",
      " [  0   7   0   0]\n",
      " [  2   5   0   1]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.31      1.00      0.48        78\n",
      "     general       0.00      0.00      0.00       157\n",
      "    physical       0.00      0.00      0.00         7\n",
      "      verbal       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.31       250\n",
      "   macro avg       0.08      0.25      0.12       250\n",
      "weighted avg       0.10      0.31      0.15       250\n",
      "\n",
      "[[  0 157]\n",
      " [  0  78]]\n",
      "1300\n",
      "shape of X: (1300, 7239)\n",
      "shape of y: (1300,)\n",
      "shape of X_train: (975, 7239)\n",
      "shape of y_train: (975,)\n",
      "shape of X_test: (325, 7239)\n",
      "shape of y_test: (325,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (325, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.70      0.58      0.63       106\n",
      "     general       0.72      0.88      0.79       194\n",
      "    physical       1.00      0.09      0.17        11\n",
      "      verbal       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.71       325\n",
      "   macro avg       0.60      0.39      0.40       325\n",
      "weighted avg       0.69      0.71      0.68       325\n",
      "\n",
      "[[ 61  45   0   0]\n",
      " [ 24 170   0   0]\n",
      " [  0  10   1   0]\n",
      " [  2  12   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.49       106\n",
      "     general       0.00      0.00      0.00       194\n",
      "    physical       0.00      0.00      0.00        11\n",
      "      verbal       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.33       325\n",
      "   macro avg       0.08      0.25      0.12       325\n",
      "weighted avg       0.11      0.33      0.16       325\n",
      "\n",
      "[[  0 194]\n",
      " [  0 106]]\n",
      "1600\n",
      "shape of X: (1600, 7239)\n",
      "shape of y: (1600,)\n",
      "shape of X_train: (1200, 7239)\n",
      "shape of y_train: (1200,)\n",
      "shape of X_test: (400, 7239)\n",
      "shape of y_test: (400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (400, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.74      0.63      0.68       132\n",
      "     general       0.72      0.86      0.78       233\n",
      "    physical       0.00      0.00      0.00        14\n",
      "      verbal       0.20      0.05      0.08        21\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.41      0.38      0.38       400\n",
      "weighted avg       0.67      0.71      0.68       400\n",
      "\n",
      "[[ 83  48   0   1]\n",
      " [ 27 201   2   3]\n",
      " [  1  13   0   0]\n",
      " [  1  19   0   1]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       132\n",
      "     general       0.00      0.00      0.00       233\n",
      "    physical       0.00      0.00      0.00        14\n",
      "      verbal       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.33       400\n",
      "   macro avg       0.08      0.25      0.12       400\n",
      "weighted avg       0.11      0.33      0.16       400\n",
      "\n",
      "[[  0 233]\n",
      " [  0 132]]\n",
      "1900\n",
      "shape of X: (1900, 7239)\n",
      "shape of y: (1900,)\n",
      "shape of X_train: (1425, 7239)\n",
      "shape of y_train: (1425,)\n",
      "shape of X_test: (475, 7239)\n",
      "shape of y_test: (475,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (475, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.80      0.68      0.73       156\n",
      "     general       0.75      0.89      0.81       280\n",
      "    physical       0.67      0.12      0.21        16\n",
      "      verbal       0.25      0.04      0.07        23\n",
      "\n",
      "    accuracy                           0.76       475\n",
      "   macro avg       0.61      0.44      0.46       475\n",
      "weighted avg       0.74      0.76      0.73       475\n",
      "\n",
      "[[106  49   0   1]\n",
      " [ 27 250   1   2]\n",
      " [  0  14   2   0]\n",
      " [  0  22   0   1]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.49       156\n",
      "     general       0.00      0.00      0.00       280\n",
      "    physical       0.00      0.00      0.00        16\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.33       475\n",
      "   macro avg       0.08      0.25      0.12       475\n",
      "weighted avg       0.11      0.33      0.16       475\n",
      "\n",
      "[[  0 280]\n",
      " [  0 156]]\n",
      "2200\n",
      "shape of X: (2200, 7239)\n",
      "shape of y: (2200,)\n",
      "shape of X_train: (1650, 7239)\n",
      "shape of y_train: (1650,)\n",
      "shape of X_test: (550, 7239)\n",
      "shape of y_test: (550,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (550, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.78      0.67      0.73       184\n",
      "     general       0.76      0.89      0.82       326\n",
      "    physical       0.67      0.12      0.20        17\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.76       550\n",
      "   macro avg       0.55      0.42      0.44       550\n",
      "weighted avg       0.73      0.76      0.73       550\n",
      "\n",
      "[[124  59   0   1]\n",
      " [ 31 291   1   3]\n",
      " [  1  14   2   0]\n",
      " [  2  21   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       184\n",
      "     general       0.00      0.00      0.00       326\n",
      "    physical       0.00      0.00      0.00        17\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.33       550\n",
      "   macro avg       0.08      0.25      0.13       550\n",
      "weighted avg       0.11      0.33      0.17       550\n",
      "\n",
      "[[  0 326]\n",
      " [  0 184]]\n",
      "2500\n",
      "shape of X: (2500, 7239)\n",
      "shape of y: (2500,)\n",
      "shape of X_train: (1875, 7239)\n",
      "shape of y_train: (1875,)\n",
      "shape of X_test: (625, 7239)\n",
      "shape of y_test: (625,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (625, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.77      0.66      0.71       208\n",
      "     general       0.75      0.88      0.81       372\n",
      "    physical       0.17      0.05      0.08        20\n",
      "      verbal       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.75       625\n",
      "   macro avg       0.42      0.40      0.40       625\n",
      "weighted avg       0.71      0.75      0.72       625\n",
      "\n",
      "[[138  70   0   0]\n",
      " [ 37 329   5   1]\n",
      " [  2  17   1   0]\n",
      " [  2  23   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       208\n",
      "     general       0.00      0.00      0.00       372\n",
      "    physical       0.00      0.00      0.00        20\n",
      "      verbal       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.33       625\n",
      "   macro avg       0.08      0.25      0.12       625\n",
      "weighted avg       0.11      0.33      0.17       625\n",
      "\n",
      "[[  0 372]\n",
      " [  0 208]]\n",
      "2800\n",
      "shape of X: (2800, 7239)\n",
      "shape of y: (2800,)\n",
      "shape of X_train: (2100, 7239)\n",
      "shape of y_train: (2100,)\n",
      "shape of X_test: (700, 7239)\n",
      "shape of y_test: (700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (700, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.74      0.68      0.71       237\n",
      "     general       0.75      0.86      0.80       415\n",
      "    physical       0.43      0.14      0.21        22\n",
      "      verbal       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.74       700\n",
      "   macro avg       0.48      0.42      0.43       700\n",
      "weighted avg       0.71      0.74      0.72       700\n",
      "\n",
      "[[160  76   0   1]\n",
      " [ 54 355   4   2]\n",
      " [  0  19   3   0]\n",
      " [  2  24   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       237\n",
      "     general       0.00      0.00      0.00       415\n",
      "    physical       0.00      0.00      0.00        22\n",
      "      verbal       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.34       700\n",
      "   macro avg       0.08      0.25      0.13       700\n",
      "weighted avg       0.11      0.34      0.17       700\n",
      "\n",
      "[[  0 415]\n",
      " [  0 237]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "idx = range(100, X.shape[0], 300)\n",
    "scores = { }\n",
    "for i in idx: \n",
    "    print(i)\n",
    "    acc, f1 = prediction(X[:i,], y[:i], n=0.2, cv=True, binary=False)\n",
    "    scores[i] = [acc , f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: [0.64, 0.6167832167832168],\n",
       " 400: [0.67, 0.6258585858585859],\n",
       " 700: [0.7314285714285714, 0.7070515924753213],\n",
       " 1000: [0.74, 0.7233142969132779],\n",
       " 1300: [0.7138461538461538, 0.682701408725957],\n",
       " 1600: [0.7125, 0.6841205882208626],\n",
       " 1900: [0.7557894736842106, 0.7308429207121454],\n",
       " 2200: [0.7581818181818182, 0.7339622204725468],\n",
       " 2500: [0.7488, 0.7227192354381871],\n",
       " 2800: [0.74, 0.7191553752906781]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDIDF Vectorizer ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(words):\n",
    "    return words\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity, encoding='utf-8', preprocessor=None, use_idf=True,\n",
    "                             lowercase=False, ngram_range=(1,2)\n",
    "                             , stop_words='english',\n",
    "                             min_df=5, max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3089, 2084)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_fit = vectorizer.fit_transform([tweet for tweet in tweets['full_tweet']])\n",
    "X = tfidf_fit\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lie until</th>\n",
       "      <td>8.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online persona</th>\n",
       "      <td>8.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one support</th>\n",
       "      <td>8.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one syllable</th>\n",
       "      <td>8.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one target</th>\n",
       "      <td>8.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1.545397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1.518949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@user</th>\n",
       "      <td>1.497964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>1.278875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bully</th>\n",
       "      <td>1.164234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57874 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                idf_weights\n",
       "lie until          8.342779\n",
       "online persona     8.342779\n",
       "one support        8.342779\n",
       "one syllable       8.342779\n",
       "one target         8.342779\n",
       "...                     ...\n",
       "and                1.545397\n",
       "to                 1.518949\n",
       "@user              1.497964\n",
       "be                 1.278875\n",
       "bully              1.164234\n",
       "\n",
       "[57874 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(), columns=['idf_weights'])\n",
    "weights = df_idf.sort_values(by=['idf_weights'], ascending=False)\n",
    "weights.to_csv('tdidfweights_'+target+'.csv', index=True)\n",
    "# weights.to_csv('tdidfweights_noemoji.csv', index=True)\n",
    "weights\n",
    "# the lower the idf value of a word, the less unique it is to any particular document\n",
    "# terms with higher weight scores are considered to be more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>suck</th>\n",
       "      <td>0.301374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everything suck</th>\n",
       "      <td>0.284080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everything</th>\n",
       "      <td>0.187606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because</th>\n",
       "      <td>0.157586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because everything</th>\n",
       "      <td>0.142040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from female</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from get</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from happen</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from head</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~ you</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57874 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tdidf\n",
       "suck                0.301374\n",
       "everything suck     0.284080\n",
       "everything          0.187606\n",
       "because             0.157586\n",
       "because everything  0.142040\n",
       "...                      ...\n",
       "from female         0.000000\n",
       "from get            0.000000\n",
       "from happen         0.000000\n",
       "from head           0.000000\n",
       "~ you               0.000000\n",
       "\n",
       "[57874 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdidf score of first tweet\n",
    "# if a word occurs multiple times in a document, we should boost its relevance as it should be \n",
    "# more meaningful than other words that appear fewer times (TF)\n",
    "# On the other hand, if a word occurs many times in all documents, maybe it is just a frequent word\n",
    "vector = pd.DataFrame(X[1].T.todense(), index=vectorizer.get_feature_names(), columns=['tdidf'])\n",
    "vector.sort_values(by=['tdidf'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ you 57873\n"
     ]
    }
   ],
   "source": [
    "D = vectorizer.vocabulary_\n",
    "max_word = max(D, key=D.get)\n",
    "max_value = max(D.values())\n",
    "print(max_word, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (3089, 2084)\n",
      "shape of y: (3089,)\n",
      "shape of X_train: (2316, 2084)\n",
      "shape of y_train: (2316,)\n",
      "shape of X_test: (773, 2084)\n",
      "shape of y_test: (773,)\n",
      "shape of y_pred: (773,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.70      0.76       265\n",
      "           1       0.76      0.91      0.83       458\n",
      "           2       0.00      0.00      0.00        23\n",
      "           3       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.78       773\n",
      "   macro avg       0.40      0.40      0.40       773\n",
      "weighted avg       0.73      0.78      0.75       773\n",
      "\n",
      "[[185  80   0   0]\n",
      " [ 39 419   0   0]\n",
      " [  0  23   0   0]\n",
      " [  1  26   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      1.00      0.51       265\n",
      "           1       0.00      0.00      0.00       458\n",
      "           2       0.00      0.00      0.00        23\n",
      "           3       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.34       773\n",
      "   macro avg       0.09      0.25      0.13       773\n",
      "weighted avg       0.12      0.34      0.18       773\n",
      "\n",
      "[[  0 458]\n",
      " [  0 265]]\n",
      "0.7813712807244502 0.8044494486211698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, binary=False)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (3089, 2084)\n",
      "shape of y: (3089,)\n",
      "shape of X_train: (2316, 2084)\n",
      "shape of y_train: (2316,)\n",
      "shape of X_test: (773, 2084)\n",
      "shape of y_test: (773,)\n"
     ]
    }
   ],
   "source": [
    "def identity(words):\n",
    "    return words\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity, encoding='utf-8', preprocessor=None, use_idf=True,\n",
    "                             lowercase=False, ngram_range=(1,2)\n",
    "                             , stop_words='english',\n",
    "                             min_df=5, max_df=0.7)\n",
    "\n",
    "tfidf_fit = vectorizer.fit_transform([tweet for tweet in tweets['full_tweet']])\n",
    "X = tfidf_fit\n",
    "X.shape\n",
    "\n",
    "y = tweets[target]\n",
    "freq = y.value_counts()           # count frequency of different classes in loan status\n",
    "freq/sum(freq)*100   \n",
    "\n",
    "labels = LabelEncoder()\n",
    "y = labels.fit_transform( np.asarray(y) )#.reshape(-1,1)\n",
    "names = labels.classes_\n",
    "print(\"shape of X:\", X.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, random_state=0, stratify=y, shuffle=True)\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    \n",
    "    return grid_search.best_params_\n",
    "\n",
    "param_dict = svc_param_selection(X, y,nfolds=10)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=param_dict['C'], gamma=param_dict['gamma'])\n",
    "# SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#     decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
    "#     probability=False, random_state=None, shrinking=True, tol=0.001,\n",
    "#     verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (773, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.82      0.70      0.76       265\n",
      "     general       0.76      0.91      0.83       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.78       773\n",
      "   macro avg       0.40      0.40      0.40       773\n",
      "weighted avg       0.73      0.78      0.75       773\n",
      "\n",
      "[[185  80   0   0]\n",
      " [ 39 419   0   0]\n",
      " [  0  23   0   0]\n",
      " [  1  26   0   0]]\n",
      "0.7813712807244502 0.8044494486211698\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       265\n",
      "     general       0.00      0.00      0.00       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.34       773\n",
      "   macro avg       0.09      0.25      0.13       773\n",
      "weighted avg       0.12      0.34      0.18       773\n",
      "\n",
      "[[  0 458]\n",
      " [  0 265]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test).reshape(-1,1)\n",
    "print(\"shape of y_pred:\", y_pred.shape)\n",
    "print(clsr(y_test, y_pred, target_names=names))\n",
    "print(cm(y_test, y_pred, labels=[0,1,2,3]))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "# f1 = f1_score(y_test, y_pred, labels=np.unique(y_pred))\n",
    "print(acc, f1)\n",
    "\n",
    "print('naive model (only no)')\n",
    "y_naive = np.array(['general']*len(y_test))\n",
    "y_naive = labels.fit_transform(y_naive)#.reshape(-1,1)\n",
    "print(clsr(y_test, y_naive, target_names=names))\n",
    "print(cm(y_test, y_naive, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (3089, 57874)\n",
      "shape of y: (3089,)\n",
      "shape of X_train: (2316, 57874)\n",
      "shape of y_train: (2316,)\n",
      "shape of X_test: (773, 57874)\n",
      "shape of y_test: (773,)\n",
      "shape of y_pred: (773, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.84      0.57      0.68       265\n",
      "     general       0.72      0.93      0.82       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.75       773\n",
      "   macro avg       0.39      0.38      0.37       773\n",
      "weighted avg       0.72      0.75      0.72       773\n",
      "\n",
      "[[152 113   0   0]\n",
      " [ 30 428   0   0]\n",
      " [  0  23   0   0]\n",
      " [  0  27   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       265\n",
      "     general       0.00      0.00      0.00       458\n",
      "    physical       0.00      0.00      0.00        23\n",
      "      verbal       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.34       773\n",
      "   macro avg       0.09      0.25      0.13       773\n",
      "weighted avg       0.12      0.34      0.18       773\n",
      "\n",
      "[[  0 458]\n",
      " [  0 265]]\n",
      "0.7503234152652005 0.7661946049062237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc, f1 = prediction(X, y, n=0.2, cv=True, binary=False)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "shape of X: (100, 57874)\n",
      "shape of y: (100,)\n",
      "shape of X_train: (75, 57874)\n",
      "shape of y_train: (75,)\n",
      "shape of X_test: (25, 57874)\n",
      "shape of y_test: (25,)\n",
      "shape of y_pred: (25, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.00      0.00      0.00         9\n",
      "     general       0.56      1.00      0.72        14\n",
      "    physical       0.00      0.00      0.00         1\n",
      "      verbal       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56        25\n",
      "   macro avg       0.14      0.25      0.18        25\n",
      "weighted avg       0.31      0.56      0.40        25\n",
      "\n",
      "[[ 0  9  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 0  1  0  0]\n",
      " [ 0  1  0  0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.36      1.00      0.53         9\n",
      "     general       0.00      0.00      0.00        14\n",
      "    physical       0.00      0.00      0.00         1\n",
      "      verbal       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.36        25\n",
      "   macro avg       0.09      0.25      0.13        25\n",
      "weighted avg       0.13      0.36      0.19        25\n",
      "\n",
      "[[ 0 14]\n",
      " [ 0  9]]\n",
      "400\n",
      "shape of X: (400, 57874)\n",
      "shape of y: (400,)\n",
      "shape of X_train: (300, 57874)\n",
      "shape of y_train: (300,)\n",
      "shape of X_test: (100, 57874)\n",
      "shape of y_test: (100,)\n",
      "shape of y_pred: (100, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.00      0.00      0.00        30\n",
      "     general       0.60      0.98      0.74        60\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.15      0.25      0.19       100\n",
      "weighted avg       0.36      0.59      0.45       100\n",
      "\n",
      "[[ 0 30  0  0]\n",
      " [ 1 59  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 0  5  0  0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.30      1.00      0.46        30\n",
      "     general       0.00      0.00      0.00        60\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.30       100\n",
      "   macro avg       0.07      0.25      0.12       100\n",
      "weighted avg       0.09      0.30      0.14       100\n",
      "\n",
      "[[ 0 60]\n",
      " [ 0 30]]\n",
      "700\n",
      "shape of X: (700, 57874)\n",
      "shape of y: (700,)\n",
      "shape of X_train: (525, 57874)\n",
      "shape of y_train: (525,)\n",
      "shape of X_test: (175, 57874)\n",
      "shape of y_test: (175,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (175, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.91      0.19      0.31        53\n",
      "     general       0.68      0.99      0.80       112\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.69       175\n",
      "   macro avg       0.40      0.29      0.28       175\n",
      "weighted avg       0.71      0.69      0.61       175\n",
      "\n",
      "[[ 10  43   0   0]\n",
      " [  1 111   0   0]\n",
      " [  0   5   0   0]\n",
      " [  0   5   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.30      1.00      0.46        53\n",
      "     general       0.00      0.00      0.00       112\n",
      "    physical       0.00      0.00      0.00         5\n",
      "      verbal       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.30       175\n",
      "   macro avg       0.08      0.25      0.12       175\n",
      "weighted avg       0.09      0.30      0.14       175\n",
      "\n",
      "[[  0 112]\n",
      " [  0  53]]\n",
      "1000\n",
      "shape of X: (1000, 57874)\n",
      "shape of y: (1000,)\n",
      "shape of X_train: (750, 57874)\n",
      "shape of y_train: (750,)\n",
      "shape of X_test: (250, 57874)\n",
      "shape of y_test: (250,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (250, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.96      0.33      0.50        78\n",
      "     general       0.70      0.99      0.82       157\n",
      "    physical       0.00      0.00      0.00         7\n",
      "      verbal       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.73       250\n",
      "   macro avg       0.42      0.33      0.33       250\n",
      "weighted avg       0.74      0.73      0.67       250\n",
      "\n",
      "[[ 26  52   0   0]\n",
      " [  1 156   0   0]\n",
      " [  0   7   0   0]\n",
      " [  0   8   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.31      1.00      0.48        78\n",
      "     general       0.00      0.00      0.00       157\n",
      "    physical       0.00      0.00      0.00         7\n",
      "      verbal       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.31       250\n",
      "   macro avg       0.08      0.25      0.12       250\n",
      "weighted avg       0.10      0.31      0.15       250\n",
      "\n",
      "[[  0 157]\n",
      " [  0  78]]\n",
      "1300\n",
      "shape of X: (1300, 57874)\n",
      "shape of y: (1300,)\n",
      "shape of X_train: (975, 57874)\n",
      "shape of y_train: (975,)\n",
      "shape of X_test: (325, 57874)\n",
      "shape of y_test: (325,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (325, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.91      0.40      0.55       106\n",
      "     general       0.68      0.98      0.80       194\n",
      "    physical       0.00      0.00      0.00        11\n",
      "      verbal       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.71       325\n",
      "   macro avg       0.40      0.34      0.34       325\n",
      "weighted avg       0.70      0.71      0.66       325\n",
      "\n",
      "[[ 42  64   0   0]\n",
      " [  4 190   0   0]\n",
      " [  0  11   0   0]\n",
      " [  0  14   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.49       106\n",
      "     general       0.00      0.00      0.00       194\n",
      "    physical       0.00      0.00      0.00        11\n",
      "      verbal       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.33       325\n",
      "   macro avg       0.08      0.25      0.12       325\n",
      "weighted avg       0.11      0.33      0.16       325\n",
      "\n",
      "[[  0 194]\n",
      " [  0 106]]\n",
      "1600\n",
      "shape of X: (1600, 57874)\n",
      "shape of y: (1600,)\n",
      "shape of X_train: (1200, 57874)\n",
      "shape of y_train: (1200,)\n",
      "shape of X_test: (400, 57874)\n",
      "shape of y_test: (400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (400, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.86      0.42      0.57       132\n",
      "     general       0.67      0.96      0.79       233\n",
      "    physical       0.00      0.00      0.00        14\n",
      "      verbal       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.38      0.35      0.34       400\n",
      "weighted avg       0.67      0.70      0.65       400\n",
      "\n",
      "[[ 56  76   0   0]\n",
      " [  9 224   0   0]\n",
      " [  0  14   0   0]\n",
      " [  0  21   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       132\n",
      "     general       0.00      0.00      0.00       233\n",
      "    physical       0.00      0.00      0.00        14\n",
      "      verbal       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.33       400\n",
      "   macro avg       0.08      0.25      0.12       400\n",
      "weighted avg       0.11      0.33      0.16       400\n",
      "\n",
      "[[  0 233]\n",
      " [  0 132]]\n",
      "1900\n",
      "shape of X: (1900, 57874)\n",
      "shape of y: (1900,)\n",
      "shape of X_train: (1425, 57874)\n",
      "shape of y_train: (1425,)\n",
      "shape of X_test: (475, 57874)\n",
      "shape of y_test: (475,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (475, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.88      0.48      0.62       156\n",
      "     general       0.69      0.96      0.81       280\n",
      "    physical       0.00      0.00      0.00        16\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.73       475\n",
      "   macro avg       0.39      0.36      0.36       475\n",
      "weighted avg       0.70      0.73      0.68       475\n",
      "\n",
      "[[ 75  81   0   0]\n",
      " [ 10 270   0   0]\n",
      " [  0  16   0   0]\n",
      " [  0  23   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.49       156\n",
      "     general       0.00      0.00      0.00       280\n",
      "    physical       0.00      0.00      0.00        16\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.33       475\n",
      "   macro avg       0.08      0.25      0.12       475\n",
      "weighted avg       0.11      0.33      0.16       475\n",
      "\n",
      "[[  0 280]\n",
      " [  0 156]]\n",
      "2200\n",
      "shape of X: (2200, 57874)\n",
      "shape of y: (2200,)\n",
      "shape of X_train: (1650, 57874)\n",
      "shape of y_train: (1650,)\n",
      "shape of X_test: (550, 57874)\n",
      "shape of y_test: (550,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (550, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.88      0.48      0.62       184\n",
      "     general       0.70      0.96      0.81       326\n",
      "    physical       0.00      0.00      0.00        17\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.73       550\n",
      "   macro avg       0.39      0.36      0.36       550\n",
      "weighted avg       0.71      0.73      0.69       550\n",
      "\n",
      "[[ 88  96   0   0]\n",
      " [ 12 314   0   0]\n",
      " [  0  17   0   0]\n",
      " [  0  23   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       184\n",
      "     general       0.00      0.00      0.00       326\n",
      "    physical       0.00      0.00      0.00        17\n",
      "      verbal       0.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.33       550\n",
      "   macro avg       0.08      0.25      0.13       550\n",
      "weighted avg       0.11      0.33      0.17       550\n",
      "\n",
      "[[  0 326]\n",
      " [  0 184]]\n",
      "2500\n",
      "shape of X: (2500, 57874)\n",
      "shape of y: (2500,)\n",
      "shape of X_train: (1875, 57874)\n",
      "shape of y_train: (1875,)\n",
      "shape of X_test: (625, 57874)\n",
      "shape of y_test: (625,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (625, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.91      0.51      0.66       208\n",
      "     general       0.71      0.97      0.82       372\n",
      "    physical       0.00      0.00      0.00        20\n",
      "      verbal       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.75       625\n",
      "   macro avg       0.41      0.37      0.37       625\n",
      "weighted avg       0.73      0.75      0.71       625\n",
      "\n",
      "[[107 101   0   0]\n",
      " [ 10 362   0   0]\n",
      " [  0  20   0   0]\n",
      " [  0  25   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.33      1.00      0.50       208\n",
      "     general       0.00      0.00      0.00       372\n",
      "    physical       0.00      0.00      0.00        20\n",
      "      verbal       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.33       625\n",
      "   macro avg       0.08      0.25      0.12       625\n",
      "weighted avg       0.11      0.33      0.17       625\n",
      "\n",
      "[[  0 372]\n",
      " [  0 208]]\n",
      "2800\n",
      "shape of X: (2800, 57874)\n",
      "shape of y: (2800,)\n",
      "shape of X_train: (2100, 57874)\n",
      "shape of y_train: (2100,)\n",
      "shape of X_test: (700, 57874)\n",
      "shape of y_test: (700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y_pred: (700, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.86      0.57      0.69       237\n",
      "     general       0.72      0.94      0.82       415\n",
      "    physical       0.00      0.00      0.00        22\n",
      "      verbal       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.75       700\n",
      "   macro avg       0.39      0.38      0.38       700\n",
      "weighted avg       0.72      0.75      0.72       700\n",
      "\n",
      "[[136 101   0   0]\n",
      " [ 23 392   0   0]\n",
      " [  0  22   0   0]\n",
      " [  0  26   0   0]]\n",
      "naive model (only no)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cyber       0.34      1.00      0.51       237\n",
      "     general       0.00      0.00      0.00       415\n",
      "    physical       0.00      0.00      0.00        22\n",
      "      verbal       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.34       700\n",
      "   macro avg       0.08      0.25      0.13       700\n",
      "weighted avg       0.11      0.34      0.17       700\n",
      "\n",
      "[[  0 415]\n",
      " [  0 237]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niti.mishra\\AppData\\Local\\Continuum\\anaconda3\\envs\\cyber_bullying\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "idx = range(100, X.shape[0], 300)\n",
    "scores_tfidf = { }\n",
    "for i in idx: \n",
    "    print(i)\n",
    "    acc, f1 = prediction(X[:i,], y[:i], n=0.2, cv=True, binary=False)\n",
    "    scores_tfidf[i] = [acc , f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: [0.56, 0.717948717948718],\n",
       " 400: [0.59, 0.49475890985324933],\n",
       " 700: [0.6914285714285714, 0.6463603425559947],\n",
       " 1000: [0.728, 0.7129099344104943],\n",
       " 1300: [0.7138461538461538, 0.7147839471829678],\n",
       " 1600: [0.7, 0.7090968031959227],\n",
       " 1900: [0.7263157894736842, 0.7402914621002112],\n",
       " 2200: [0.730909090909091, 0.7408880613375015],\n",
       " 2500: [0.7504, 0.7638181818181817],\n",
       " 2800: [0.7542857142857143, 0.7716604399100484]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_searchdef svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
