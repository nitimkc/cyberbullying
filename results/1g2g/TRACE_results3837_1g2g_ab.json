{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.703125, 0.74375, 0.74375, 0.703125, 0.70625, 0.725, 0.71875, 0.709375, 0.7125, 0.7272727272727273, 0.677115987460815, 0.7523510971786834], "precision": [0.6892688679245282, 0.7450827205882353, 0.7317698736509607, 0.6996565656565655, 0.7001691017316019, 0.7179849960515925, 0.7177286505007908, 0.7006690705128206, 0.6982843137254903, 0.7042585021730999, 0.6608316980467385, 0.7650163946240047], "recall": [0.703125, 0.74375, 0.74375, 0.703125, 0.70625, 0.725, 0.71875, 0.709375, 0.7125, 0.7272727272727273, 0.677115987460815, 0.7523510971786834], "f1_valid": [0.6708333333333333, 0.7107857677302122, 0.7202604166666666, 0.6603697696192851, 0.674264705882353, 0.697588781275222, 0.682421875, 0.6813488662103453, 0.6755157394046283, 0.6999105394955027, 0.6369773406589037, 0.726731926698492], "f1_train": [0.7208700223218588, 0.7085937841800068, 0.7204943946446866, 0.71186195651957, 0.7142538751263822, 0.7098514063422927, 0.7149416194275278, 0.7193381918141274, 0.720395580305329, 0.7144921778303235, 0.7196329046510787, 0.7105731544474974], "time": [36.95981001853943, 34.57340145111084, 32.575623750686646, 35.03889489173889, 33.47183656692505, 35.38597798347473, 35.99083757400513, 35.58298420906067, 33.72119426727295, 35.663838624954224, 33.35556077957153, 33.74993968009949]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000246DC0D25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.709375, 0.70625, 0.715625, 0.728125, 0.7125, 0.73125, 0.6875, 0.71875, 0.728125, 0.7335423197492164, 0.7398119122257053, 0.7304075235109718], "precision": [0.6939322358594714, 0.7107980120030006, 0.7183061749571182, 0.7206818181818182, 0.7019074675324676, 0.7259690028427729, 0.6787443438914027, 0.7043584627159414, 0.7150864160914582, 0.7136761236305804, 0.7262780978035994, 0.736078116373683], "recall": [0.709375, 0.70625, 0.715625, 0.728125, 0.7125, 0.73125, 0.6875, 0.71875, 0.728125, 0.7335423197492164, 0.7398119122257053, 0.7304075235109718], "f1_valid": [0.6817919130027472, 0.6776816361556064, 0.6833966134241076, 0.6990286298568508, 0.68211453408571, 0.6992354509165397, 0.6601473112128147, 0.685761681500318, 0.7149336023543855, 0.707366513255456, 0.7146947709214346, 0.6956580120932307], "f1_train": [0.8135588137169113, 0.8142693050415429, 0.8124922049758342, 0.8136427442972762, 0.818047209502821, 0.8168010032011351, 0.8133657641728081, 0.8142777129657524, 0.8130291597756062, 0.8142196193382593, 0.8123078961564766, 0.8124375703777065], "time": [4.750135898590088, 4.577280521392822, 4.660660028457642, 4.662656307220459, 4.7173683643341064, 4.579901218414307, 5.225168943405151, 5.7499542236328125, 4.926308631896973, 4.70269775390625, 4.604973077774048, 4.978627920150757]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.721875, 0.7375, 0.746875, 0.7375, 0.740625, 0.684375, 0.725, 0.709375, 0.6625, 0.7429467084639498, 0.7053291536050157, 0.6990595611285266], "precision": [0.7353548204693242, 0.7290510204081633, 0.7477020273790914, 0.7223408127096652, 0.7347005208333333, 0.6758448818716676, 0.7096638655462184, 0.7032403471880453, 0.674489122935184, 0.7360376799388637, 0.7024223425477344, 0.6917110645655566], "recall": [0.721875, 0.7375, 0.746875, 0.7375, 0.740625, 0.684375, 0.725, 0.709375, 0.6625, 0.7429467084639498, 0.7053291536050157, 0.6990595611285266], "f1_valid": [0.726092815457495, 0.7225042631803325, 0.7472809622809622, 0.7160758561909191, 0.7275478968596946, 0.6764226699855748, 0.7067978533094813, 0.7042213762622189, 0.6668731051953984, 0.7239598176118553, 0.7037633216026442, 0.6943688386467946], "f1_train": [0.8094900608121637, 0.7741785650773545, 0.7963192952402088, 0.7670747335704117, 0.7709618802655593, 0.8051240278813704, 0.7814018513323374, 0.7960211131760555, 0.8128869271218558, 0.7850608417635813, 0.8043232475092277, 0.7847506357874371], "time": [34.32635188102722, 34.65920281410217, 36.37265205383301, 33.34064054489136, 34.20368003845215, 36.36428260803223, 34.244609117507935, 35.252662658691406, 35.28782892227173, 33.76431369781494, 33.290857791900635, 34.442625999450684]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.665625, 0.703125, 0.68125, 0.7125, 0.725, 0.696875, 0.69375, 0.70625, 0.715625, 0.7021943573667712, 0.6990595611285266, 0.6990595611285266], "precision": [0.6938358516483516, 0.7066419776837245, 0.7061974725633389, 0.7273472429210134, 0.7322261663286004, 0.6949577135711466, 0.6891225961538462, 0.7119717714672076, 0.7096688034188035, 0.701750498183999, 0.7018709260088571, 0.7139112786937142], "recall": [0.665625, 0.703125, 0.68125, 0.7125, 0.725, 0.696875, 0.69375, 0.70625, 0.715625, 0.7021943573667712, 0.6990595611285266, 0.6990595611285266], "f1_valid": [0.6736582924308964, 0.7047119839307121, 0.6893666964741951, 0.7176991150442478, 0.7279042200094832, 0.6958545772461353, 0.6900753585016096, 0.7086710164835164, 0.7117703626816461, 0.7019654783351996, 0.7003554768940019, 0.7047643404640298], "f1_train": [0.9994313335228888, 0.9988626670457776, 0.99914691601581, 0.9991469154561609, 0.9991469150820149, 0.9991469143311869, 0.9994312147403571, 0.9994313335228888, 0.9991469143311869, 0.999431377132122, 0.9991471571588699, 0.999147159024559], "time": [5.0185017585754395, 4.819204568862915, 4.67061185836792, 4.962364912033081, 5.04974102973938, 4.5689918994903564, 4.744138479232788, 5.206906795501709, 4.786885499954224, 4.92233681678772, 4.868371248245239, 5.000552177429199]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000246DC0D25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.66875, 0.709375, 0.746875, 0.728125, 0.746875, 0.70625, 0.725, 0.753125, 0.71875, 0.7115987460815048, 0.768025078369906, 0.7586206896551724], "precision": [0.6551622391593902, 0.69993677240608, 0.7402358647978643, 0.7169856641756901, 0.7432563633075636, 0.6991491715307852, 0.7199404761904763, 0.7470488726228582, 0.7141375858123571, 0.7081406300518717, 0.7624783114060948, 0.7551531400683529], "recall": [0.66875, 0.709375, 0.746875, 0.728125, 0.746875, 0.70625, 0.725, 0.753125, 0.71875, 0.7115987460815048, 0.768025078369906, 0.7586206896551724], "f1_valid": [0.6557895979492715, 0.7037966284731121, 0.7422371836005087, 0.7191610298544463, 0.7446916040364568, 0.6969099406009784, 0.7219999999999999, 0.747211414019703, 0.7161842105263159, 0.7021445986963228, 0.7644330317110948, 0.748791121680539], "f1_train": [0.9707186470813147, 0.9730521254617881, 0.9713131007667724, 0.9701521753516791, 0.9733353291204032, 0.9721748663614164, 0.9724675171626626, 0.9730366719328752, 0.9745382421399165, 0.9730301290364962, 0.972181399699838, 0.9692881837781013], "time": [26.16975998878479, 27.845210075378418, 28.637274265289307, 27.043243885040283, 27.32622003555298, 29.017852783203125, 27.322265625, 28.83199453353882, 26.27808141708374, 26.71276569366455, 27.361021757125854, 26.66092801094055]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000246DC0D25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.675, 0.7125, 0.65625, 0.6375, 0.64375, 0.64375, 0.665625, 0.590625, 0.69375, 0.7053291536050157, 0.6677115987460815, 0.6363636363636364], "precision": [0.45562500000000006, 0.7954153605015675, 0.7747844827586207, 0.40640624999999997, 0.7710619122257054, 0.4144140625, 0.44305664062500005, 0.348837890625, 0.6855481072555205, 0.7924331144890676, 0.44583877910004815, 0.7694293088614855], "recall": [0.675, 0.7125, 0.65625, 0.6375, 0.64375, 0.64375, 0.665625, 0.590625, 0.69375, 0.7053291536050157, 0.6677115987460815, 0.6363636363636364], "f1_valid": [0.5440298507462686, 0.5960301223599096, 0.523251488095238, 0.4963740458015267, 0.5074485061858384, 0.5042300380228137, 0.5320004690431521, 0.43861738703339875, 0.5769557730155259, 0.5866158563623254, 0.5346713177929149, 0.5013221013221013], "f1_train": [0.6043497979506448, 0.6098507892415793, 0.6043107957389517, 0.602227055564964, 0.5979323471404986, 0.5998197132744468, 0.6021172122804427, 0.5934956598179977, 0.608215184459439, 0.6131753669378658, 0.6042619268427593, 0.6030567193626498], "time": [5.023782253265381, 4.629042625427246, 4.909940958023071, 5.171558618545532, 4.73259162902832, 4.813379287719727, 4.623442888259888, 4.6808271408081055, 5.561931610107422, 5.701308727264404, 4.764280319213867, 5.176705837249756]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000246DC0D25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [[3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3517, 320], [3518, 319], [3518, 319], [3518, 319]], "accuracy": [0.671875, 0.6375, 0.678125, 0.696875, 0.68125, 0.7125, 0.659375, 0.7, 0.6875, 0.6520376175548589, 0.6426332288401254, 0.6551724137931034], "precision": [0.641120378040515, 0.6075927734375, 0.6541084607274644, 0.6749999999999999, 0.6439814814814815, 0.7094642857142858, 0.6379647643752703, 0.6489447044217362, 0.6699368018812464, 0.6525456333670204, 0.628323848426563, 0.6218833893710741], "recall": [0.671875, 0.6375, 0.678125, 0.696875, 0.68125, 0.7125, 0.659375, 0.7, 0.6875, 0.6520376175548589, 0.6426332288401254, 0.6551724137931034], "f1_valid": [0.6302617003600111, 0.6062028795174863, 0.6367916356123903, 0.6744487996080354, 0.6443877551020407, 0.6663205888815644, 0.6285276724148378, 0.6525390625, 0.6753771551724138, 0.6075687696139384, 0.5789065910082934, 0.610725364305295], "f1_train": [0.67569501737963, 0.6803716753743582, 0.6765993193090764, 0.6748020584867993, 0.6752032158390954, 0.6775429993482216, 0.6726474713470124, 0.6715387393100987, 0.6647118438713099, 0.6843038026811609, 0.6783580312581923, 0.6757345464228272], "time": [33.81843638420105, 35.851561307907104, 34.54663705825806, 35.2108211517334, 35.077640533447266, 33.352784633636475, 33.91835427284241, 33.864468812942505, 33.542274475097656, 38.107940435409546, 40.24775958061218, 40.12996006011963]}
