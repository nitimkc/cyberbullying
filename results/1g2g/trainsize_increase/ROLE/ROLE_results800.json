{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80]], "accuracy": [0.6375, 0.625, 0.55, 0.5875, 0.575, 0.55, 0.55, 0.6, 0.55, 0.6875], "precision": [0.5889266304347827, 0.5289069923978461, 0.41934865900383145, 0.5448760162601627, 0.501045440821256, 0.4417365967365967, 0.3926282051282051, 0.43318572874493927, 0.43879694154084403, 0.6322916666666666], "recall": [0.6375, 0.625, 0.55, 0.5875, 0.575, 0.55, 0.55, 0.6, 0.55, 0.6875], "f1_valid": [0.584038525041277, 0.5488514588859417, 0.4751201923076923, 0.5386311911128088, 0.50960713362626, 0.4748289800995025, 0.4467981557377049, 0.5020770010131713, 0.4786363636363637, 0.6484605087014726], "f1_train": [0.8558962662277717, 0.8570471047635309, 0.8481044011699399, 0.8587843837058106, 0.8563853023153617, 0.855481520616136, 0.8716309391281842, 0.8689208510652247, 0.8603103921374223, 0.8564495900427851], "time": [0.609201192855835, 0.6092307567596436, 0.6560509204864502, 0.6560614109039307, 0.5936110019683838, 0.6404364109039307, 0.6716887950897217, 0.5623352527618408, 0.6560657024383545, 0.6248207092285156]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80]], "accuracy": [0.5875, 0.5625, 0.6875, 0.55, 0.6625, 0.7, 0.6375, 0.6, 0.6125, 0.6375], "precision": [0.4973544973544973, 0.4238008658008658, 0.6774565972222223, 0.4595959595959596, 0.5668894830659537, 0.6239407467532467, 0.6519827586206896, 0.48781162464985994, 0.5184425223898909, 0.5737179487179487], "recall": [0.5875, 0.5625, 0.6875, 0.55, 0.6625, 0.7, 0.6375, 0.6, 0.6125, 0.6375], "f1_valid": [0.537330875646093, 0.48313789252397843, 0.6436836812997804, 0.4974431818181818, 0.6108504398826978, 0.6431732580037666, 0.589968984962406, 0.5336767305861553, 0.558824257147356, 0.5815518162393162], "f1_train": [0.9840545613319617, 0.9818969884866922, 0.984275273194808, 0.9756659329824721, 0.9851653088825905, 0.9731569320016041, 0.9835099022732352, 0.9899425236073212, 0.9870015413510468, 0.9751241710388996], "time": [0.9372789859771729, 0.9372947216033936, 0.9372775554656982, 0.9372801780700684, 0.9372811317443848, 0.9684944152832031, 0.9841551780700684, 0.9685249328613281, 0.9372806549072266, 0.937281608581543]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80], [720, 80]], "accuracy": [0.5625, 0.5, 0.5625, 0.6, 0.5125, 0.4375, 0.475, 0.45, 0.6125, 0.4], "precision": [0.5946192226890756, 0.46086309523809527, 0.489642497763864, 0.5443749999999999, 0.4949091478696742, 0.41308823529411764, 0.4100929152148664, 0.4520364865313714, 0.6119843391902215, 0.3727793659043659], "recall": [0.5625, 0.5, 0.5625, 0.6, 0.5125, 0.4375, 0.475, 0.45, 0.6125, 0.4], "f1_valid": [0.548329786155873, 0.4471973684210527, 0.5165650997887841, 0.5479194760908316, 0.494312144633973, 0.39032738095238095, 0.42851805728518055, 0.439485347985348, 0.6026515151515152, 0.3605877713057347], "f1_train": [0.6215511976863514, 0.6138799631087887, 0.6135809759678987, 0.6118757063645097, 0.6170320287828224, 0.6078839826522492, 0.6006234906708776, 0.6204690853575362, 0.6310559555983883, 0.6220601627808019], "time": [0.3748972415924072, 0.3905344009399414, 0.3905339241027832, 0.3749046325683594, 0.3749103546142578, 0.40618371963500977, 0.37491321563720703, 0.35928916931152344, 0.40615248680114746, 0.35929203033447266]}
