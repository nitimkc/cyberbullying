{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25]], "accuracy": [0.48, 0.6, 0.6, 0.6, 0.48, 0.52, 0.6, 0.76, 0.48, 0.56, 0.84, 0.6], "precision": [0.3781818181818181, 0.5945454545454545, 0.6746853146853147, 0.53, 0.4492857142857143, 0.43714285714285717, 0.5435897435897435, 0.768, 0.3363636363636363, 0.424, 0.8625, 0.6207272727272728], "recall": [0.48, 0.6, 0.6, 0.6, 0.48, 0.52, 0.6, 0.76, 0.48, 0.56, 0.84, 0.6], "f1_valid": [0.4205714285714285, 0.5861052631578947, 0.5329665071770334, 0.5367272727272727, 0.43449275362318834, 0.4531428571428572, 0.5318095238095238, 0.7476470588235294, 0.3928421052631579, 0.4822222222222222, 0.8204926108374384, 0.5557422969187674], "f1_train": [0.8821074587919508, 0.88745342656505, 0.8874307089349264, 0.9033490809632194, 0.8821179034567747, 0.9031996821338052, 0.9084850190860044, 0.8872264378894213, 0.8979924348298232, 0.8926358019345032, 0.88214950153189, 0.8874960425146993], "time": [0.21236395835876465, 0.24205279350280762, 0.1916353702545166, 0.2327284812927246, 0.19382023811340332, 0.191725492477417, 0.23203682899475098, 0.2240607738494873, 0.20186686515808105, 0.22984075546264648, 0.20192766189575195, 0.22429585456848145]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25]], "accuracy": [0.64, 0.72, 0.52, 0.68, 0.52, 0.72, 0.68, 0.6, 0.52, 0.56, 0.68, 0.48], "precision": [0.612, 0.716, 0.5666666666666667, 0.6524242424242425, 0.6907692307692308, 0.6579999999999999, 0.6757142857142857, 0.576, 0.43174603174603177, 0.4923076923076923, 0.7037575757575758, 0.4254545454545455], "recall": [0.64, 0.72, 0.52, 0.68, 0.52, 0.72, 0.68, 0.6, 0.52, 0.56, 0.68, 0.48], "f1_valid": [0.6222222222222221, 0.7133044733044733, 0.4857142857142857, 0.658857142857143, 0.45166944510597756, 0.6875057208237986, 0.6722994652406418, 0.5799847211611916, 0.4650549450549451, 0.5216, 0.6677142857142857, 0.4427067669172932], "f1_train": [0.9886963601061961, 0.9799256139441456, 0.970434161684722, 0.9732289937184656, 0.9758944855466594, 0.9798481821209093, 0.9732687359921403, 0.9836688430026288, 0.9648615040948317, 0.9770138062526664, 0.9744782228144093, 0.9793264327295742], "time": [0.25952816009521484, 0.2547910213470459, 0.20209217071533203, 0.2508883476257324, 0.2527294158935547, 0.2531769275665283, 0.2221972942352295, 0.2708146572113037, 0.2527353763580322, 0.25461745262145996, 0.20587682723999023, 0.2337348461151123]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25], [275, 25]], "accuracy": [0.52, 0.48, 0.48, 0.6, 0.6, 0.52, 0.52, 0.4, 0.48, 0.6, 0.6, 0.48], "precision": [0.5031746031746032, 0.6358333333333334, 0.40538461538461534, 0.55, 0.64, 0.47345454545454546, 0.56, 0.3861538461538462, 0.4087272727272728, 0.6460606060606061, 0.6257142857142857, 0.4474285714285714], "recall": [0.52, 0.48, 0.48, 0.6, 0.6, 0.52, 0.52, 0.4, 0.48, 0.6, 0.6, 0.48], "f1_valid": [0.4718925518925518, 0.4288969696969697, 0.43725603864734297, 0.5717171717171717, 0.608, 0.4920574162679426, 0.5098947368421053, 0.3813333333333334, 0.42727272727272725, 0.6183333333333333, 0.5991529411764707, 0.4620350877192982], "f1_train": [0.636872630858066, 0.6459233971107768, 0.6694844135717393, 0.626586936942835, 0.6155784934558524, 0.6332995108119813, 0.6656688248608658, 0.6336598832147293, 0.699988921031575, 0.6303046273006748, 0.6788175702165697, 0.6887945121042716], "time": [0.15182209014892578, 0.13960552215576172, 0.13283061981201172, 0.16205406188964844, 0.18852829933166504, 0.18378067016601562, 0.15231084823608398, 0.14150381088256836, 0.15164923667907715, 0.1312875747680664, 0.16180896759033203, 0.14185309410095215]}
