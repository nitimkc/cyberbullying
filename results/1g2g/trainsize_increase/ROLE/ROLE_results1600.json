{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160]], "accuracy": [0.6125, 0.59375, 0.60625, 0.58125, 0.59375, 0.6125, 0.56875, 0.58125, 0.59375, 0.54375], "precision": [0.5746922666885353, 0.5677358195902048, 0.5143386044176707, 0.5200332125603865, 0.49975649350649354, 0.5578246753246753, 0.47584354764107306, 0.5256280310631716, 0.4843831168831169, 0.45933689024390245], "recall": [0.6125, 0.59375, 0.60625, 0.58125, 0.59375, 0.6125, 0.56875, 0.58125, 0.59375, 0.54375], "f1_valid": [0.5558897495776588, 0.537629819474103, 0.5312655217589428, 0.5254242703533026, 0.5229732488925803, 0.5563937282229965, 0.4917426097304146, 0.5273682153227608, 0.5093023673797867, 0.47318777713514565], "f1_train": [0.8373677739308957, 0.8339650591385702, 0.8416990067507748, 0.8433800421339347, 0.8351410855803442, 0.8414038068966416, 0.8423120539094312, 0.840724474350661, 0.8355737103272083, 0.8439022169572488], "time": [1.9526727199554443, 2.1557469367980957, 790.5302581787109, 2.2182302474975586, 2.10892391204834, 1.7027230262756348, 1.8901853561401367, 1.9682888984680176, 1.8901457786560059, 2.0932605266571045]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160]], "accuracy": [0.575, 0.58125, 0.58125, 0.55625, 0.59375, 0.59375, 0.66875, 0.575, 0.59375, 0.61875], "precision": [0.5240031508039983, 0.5110394149657302, 0.5355380772138767, 0.5055578231292517, 0.5243165036141575, 0.5556057656655543, 0.6051569645284884, 0.5272596420643729, 0.5156150793650793, 0.5982414168327705], "recall": [0.575, 0.58125, 0.58125, 0.55625, 0.59375, 0.59375, 0.66875, 0.575, 0.59375, 0.61875], "f1_valid": [0.5305804742192072, 0.5246823773343353, 0.5547149913965649, 0.5217729471049782, 0.5526548423423423, 0.553193704025126, 0.6299468623481781, 0.5299069464201043, 0.5502433497498108, 0.5932227393549822], "f1_train": [0.9709097592055164, 0.9656964644397665, 0.965532922137824, 0.9709134600880758, 0.9696465278943761, 0.9645350300440301, 0.9665714052614581, 0.9679642287510422, 0.967965656620477, 0.9718065857344224], "time": [3.0148839950561523, 2.968065023422241, 3.030582904815674, 2.9993104934692383, 2.9836723804473877, 2.999293088912964, 2.999295473098755, 2.983670711517334, 3.046142101287842, 3.0617547035217285]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160], [1440, 160]], "accuracy": [0.5375, 0.4875, 0.5125, 0.4625, 0.55625, 0.41875, 0.4375, 0.475, 0.44375, 0.4625], "precision": [0.5559470108695652, 0.4641404349033998, 0.4763939950980392, 0.45817822802197805, 0.5488194444444445, 0.47310871518418685, 0.4323569168034272, 0.5097127931502932, 0.4400092026942356, 0.4975570946811236], "recall": [0.5375, 0.4875, 0.5125, 0.4625, 0.55625, 0.41875, 0.4375, 0.475, 0.44375, 0.4625], "f1_valid": [0.5389458525345623, 0.47208732664172165, 0.4866550008222097, 0.44727277579670216, 0.5452422723475355, 0.4081805307418719, 0.43345987741073005, 0.4686480186480185, 0.4351317633865994, 0.46415751846635267], "f1_train": [0.5855443109104701, 0.5982313172479459, 0.6146235781238251, 0.603576276468746, 0.5775937492735674, 0.5966559678414316, 0.6000296643956737, 0.5955897529229006, 0.5967432260526865, 0.5881613976276294], "time": [0.7654585838317871, 0.7966885566711426, 0.9060370922088623, 0.7654495239257812, 0.8279309272766113, 0.7811117172241211, 0.74983811378479, 0.7654421329498291, 0.7654397487640381, 0.8279314041137695]}
