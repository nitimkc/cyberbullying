{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150]], "accuracy": [0.5866666666666667, 0.5466666666666666, 0.56, 0.6133333333333333, 0.5933333333333334, 0.5666666666666667, 0.6133333333333333, 0.5733333333333334, 0.5333333333333333, 0.5733333333333334], "precision": [0.5247157523058851, 0.4973148148148149, 0.5084529550051938, 0.622995951417004, 0.5243693693693694, 0.4780579051991583, 0.5607346661078005, 0.5051976564098069, 0.45753399668325034, 0.469978375118463], "recall": [0.5866666666666667, 0.5466666666666666, 0.56, 0.6133333333333333, 0.5933333333333334, 0.5666666666666667, 0.6133333333333333, 0.5733333333333334, 0.5333333333333333, 0.5733333333333334], "f1_valid": [0.529397143059575, 0.48050420168067226, 0.48388390626383077, 0.5532962374138845, 0.534375307325454, 0.4866222371439763, 0.5733227006911218, 0.509385150456579, 0.46625179382510995, 0.48988700809037594], "f1_train": [0.8406626200459407, 0.8467346462920619, 0.8486216523163334, 0.8366110685750651, 0.8497829339598125, 0.8458294881757676, 0.8454305803956856, 0.8454881283499616, 0.8432764576223983, 0.8281586665702781], "time": [2.0151565074920654, 2.0619850158691406, 2.0463931560516357, 1.8120450973510742, 1.874528408050537, 1.999563455581665, 1.8901822566986084, 2.2026054859161377, 1.874560832977295, 1.9526681900024414]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150]], "accuracy": [0.56, 0.5266666666666666, 0.5733333333333334, 0.66, 0.6133333333333333, 0.62, 0.5533333333333333, 0.56, 0.66, 0.6333333333333333], "precision": [0.5212773892773892, 0.4522404712880348, 0.5284286473760157, 0.602118112244898, 0.6103044871794872, 0.5643124506520732, 0.49329781993561767, 0.49377065853038393, 0.608406581023214, 0.5773247863247862], "recall": [0.56, 0.5266666666666666, 0.5733333333333334, 0.66, 0.6133333333333333, 0.62, 0.5533333333333333, 0.56, 0.66, 0.6333333333333333], "f1_valid": [0.5184062422496518, 0.4821317311971517, 0.5348083081948984, 0.6280322321694788, 0.5715739854687223, 0.5833336683415954, 0.5118888379204893, 0.5125210361326513, 0.6257043083900226, 0.6027579631119454], "f1_train": [0.9718151355352314, 0.9697442704453425, 0.9747749192293899, 0.9671240255303165, 0.9704565260688441, 0.9720877086382455, 0.9733975600146519, 0.9715047414789026, 0.974611503492954, 0.9688220072609809], "time": [2.765010118484497, 2.702493667602539, 2.76497745513916, 2.718109130859375, 2.733736038208008, 2.7181131839752197, 2.7493550777435303, 2.811837673187256, 2.7805941104888916, 2.7181153297424316]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150], [1350, 150]], "accuracy": [0.48, 0.5266666666666666, 0.5133333333333333, 0.4533333333333333, 0.43333333333333335, 0.4666666666666667, 0.5, 0.5133333333333333, 0.5066666666666667, 0.4266666666666667], "precision": [0.4671561921968426, 0.5387142721669037, 0.5199180066637694, 0.5085300415124976, 0.4899037001645697, 0.5204569115095431, 0.4932997289972899, 0.5017442645074224, 0.4993597438975591, 0.39964536340852125], "recall": [0.48, 0.5266666666666666, 0.5133333333333333, 0.4533333333333333, 0.43333333333333335, 0.4666666666666667, 0.5, 0.5133333333333333, 0.5066666666666667, 0.4266666666666667], "f1_valid": [0.47019624852760444, 0.5257619783161952, 0.4890748510500771, 0.46104532501573686, 0.4331946675307652, 0.4476927560599692, 0.49300967216448727, 0.5050756597230377, 0.5004646464646464, 0.4059901540464921], "f1_train": [0.623179242409194, 0.5980647339669298, 0.5821670677850096, 0.609637441886771, 0.5842900846918667, 0.6063852551019641, 0.6078484258417994, 0.6072897583127478, 0.610625157052722, 0.6031947316976729], "time": [0.7342033386230469, 0.7967202663421631, 0.7967169284820557, 0.7342054843902588, 0.7341721057891846, 0.7810671329498291, 0.7498188018798828, 0.7498300075531006, 0.7498245239257812, 0.7810995578765869]}
