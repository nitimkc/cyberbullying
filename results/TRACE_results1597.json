{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6658536585365854, 0.6878048780487804, 0.6463414634146342, 0.6682926829268293, 0.6487804878048781, 0.651219512195122, 0.6634146341463415, 0.6503667481662592, 0.6723716381418093, 0.7090464547677262, 0.6992665036674817, 0.6577017114914425], "precision": [0.6709215914141017, 0.6935192516123081, 0.6688685636856367, 0.673738693975985, 0.6660698374562866, 0.6666971794820492, 0.6743998843150488, 0.6554700098839931, 0.6789921874491637, 0.7090053767222534, 0.6967254628012575, 0.6804516009546745], "recall": [0.6658536585365854, 0.6878048780487804, 0.6463414634146342, 0.6682926829268293, 0.6487804878048781, 0.651219512195122, 0.6634146341463415, 0.6503667481662592, 0.6723716381418093, 0.7090464547677262, 0.6992665036674817, 0.6577017114914425], "f1": [0.6428620917585113, 0.6546971181344039, 0.6160784393498648, 0.6445259095015192, 0.6205695215519931, 0.6227412960993021, 0.6341111064398012, 0.6235194145499198, 0.6534686724414139, 0.6811649355103078, 0.6827209180498073, 0.6269227324148318], "time": [10.735126733779907, 10.827684164047241, 10.143001317977905, 10.245222806930542, 10.849748611450195, 10.979696273803711, 10.544694423675537, 10.970550537109375, 11.036213636398315, 10.640653848648071, 11.15068006515503, 10.171231985092163]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6634146341463415, 0.675609756097561, 0.6463414634146342, 0.7268292682926829, 0.651219512195122, 0.6804878048780488, 0.6609756097560976, 0.6503667481662592, 0.6601466992665037, 0.6699266503667481, 0.6894865525672371, 0.6405867970660146], "precision": [0.6734616683201101, 0.687699864498645, 0.6324561251107575, 0.7225152015190216, 0.6652353628976634, 0.6749231213887884, 0.6774525406068922, 0.6836703375221096, 0.6903508810538755, 0.674925448231601, 0.6919401192467722, 0.654566707836564], "recall": [0.6634146341463415, 0.675609756097561, 0.6463414634146342, 0.7268292682926829, 0.651219512195122, 0.6804878048780488, 0.6609756097560976, 0.6503667481662592, 0.6601466992665037, 0.6699266503667481, 0.6894865525672371, 0.6405867970660146], "f1": [0.6335935406057357, 0.6485918139314644, 0.6204083284323653, 0.7012974472147324, 0.6238872230924144, 0.6539175449607203, 0.6359652639517271, 0.6156070946841986, 0.6368247981285691, 0.6464587014014165, 0.6748204121929448, 0.6140625757238221], "time": [0.6208240985870361, 0.6598105430603027, 0.6205127239227295, 0.6098957061767578, 0.585982084274292, 0.5703392028808594, 0.6798911094665527, 0.5543704032897949, 0.6601133346557617, 0.6147623062133789, 0.5803110599517822, 0.5899889469146729]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7048780487804878, 0.6829268292682927, 0.6634146341463415, 0.6658536585365854, 0.6878048780487804, 0.6878048780487804, 0.7170731707317073, 0.6674816625916871, 0.6919315403422983, 0.684596577017115, 0.6552567237163814, 0.6356968215158925], "precision": [0.7022661801421164, 0.6802097677044258, 0.6761857958725467, 0.6712676380047639, 0.6932831174972526, 0.6868573433935778, 0.7159554823971342, 0.6688054857136391, 0.6916163410094214, 0.6892900742398617, 0.6568037233073908, 0.648135275755883], "recall": [0.7048780487804878, 0.6829268292682927, 0.6634146341463415, 0.6658536585365854, 0.6878048780487804, 0.6878048780487804, 0.7170731707317073, 0.6674816625916871, 0.6919315403422983, 0.684596577017115, 0.6552567237163814, 0.6356968215158925], "f1": [0.702850948509485, 0.6747233330693696, 0.6671682209469153, 0.6668167886899342, 0.6892238102835049, 0.6789198606271777, 0.709964203327087, 0.6680002426188363, 0.6774363111353704, 0.6735671530790303, 0.6559327530598306, 0.6382845983344207], "time": [10.740015268325806, 10.78992486000061, 10.194736957550049, 10.649998664855957, 10.3048677444458, 10.370043754577637, 10.415148258209229, 11.307929277420044, 10.12021255493164, 10.299856424331665, 10.168979167938232, 9.994964122772217]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6682926829268293, 0.7024390243902439, 0.6585365853658537, 0.6634146341463415, 0.6292682926829268, 0.6536585365853659, 0.651219512195122, 0.6894865525672371, 0.7286063569682152, 0.6919315403422983, 0.687041564792176, 0.7114914425427873], "precision": [0.6674765485230126, 0.7028170802240122, 0.6732826437576117, 0.6595224670488866, 0.6323635176060548, 0.6516005234068605, 0.6616087916598882, 0.689170836782766, 0.7335726099360761, 0.6977141962332014, 0.6852314606389301, 0.7099627482488519], "recall": [0.6682926829268293, 0.7024390243902439, 0.6585365853658537, 0.6634146341463415, 0.6292682926829268, 0.6536585365853659, 0.651219512195122, 0.6894865525672371, 0.7286063569682152, 0.6919315403422983, 0.687041564792176, 0.7114914425427873], "f1": [0.6677411163932345, 0.7026113195822783, 0.6628871867231069, 0.6601724364017834, 0.6305790384869893, 0.6524896341463415, 0.6540153942839317, 0.6892254554745533, 0.7298341077014565, 0.6937698554353892, 0.6844654903955321, 0.7098813037727569], "time": [0.563101053237915, 0.6142892837524414, 0.6602208614349365, 0.6320087909698486, 0.6444170475006104, 0.611985445022583, 0.6198186874389648, 0.6289868354797363, 0.6601278781890869, 0.644885778427124, 0.6552557945251465, 0.7301850318908691]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6951219512195121, 0.7, 0.6658536585365854, 0.6780487804878049, 0.6902439024390243, 0.6707317073170732, 0.7, 0.6503667481662592, 0.7041564792176039, 0.6625916870415648, 0.7212713936430318, 0.7017114914425427], "precision": [0.6927693489533321, 0.6979048295454545, 0.6694846574445414, 0.6739062053459054, 0.6894244711753852, 0.6684894402637058, 0.69482634154167, 0.6467609671118948, 0.7026865169317879, 0.6576949595336994, 0.7184477293927418, 0.6996494187773802], "recall": [0.6951219512195121, 0.7, 0.6658536585365854, 0.6780487804878049, 0.6902439024390243, 0.6707317073170732, 0.7, 0.6503667481662592, 0.7041564792176039, 0.6625916870415648, 0.7212713936430318, 0.7017114914425427], "f1": [0.692082025432053, 0.6987458745874587, 0.6593063501330283, 0.6735957229526441, 0.6879947316800936, 0.6692853264788277, 0.6964518210075654, 0.6464755960121265, 0.698636629130447, 0.6559577341973429, 0.7180809152558011, 0.6949607913885755], "time": [2.09502911567688, 1.740082025527954, 1.889941930770874, 1.7100193500518799, 2.1200644969940186, 1.8498752117156982, 1.7902774810791016, 1.874793291091919, 1.7222483158111572, 1.7246921062469482, 1.7009515762329102, 1.8761074542999268]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5878048780487805, 0.5707317073170731, 0.6121951219512195, 0.6146341463414634, 0.5365853658536586, 0.573170731707317, 0.6073170731707317, 0.5403422982885085, 0.6063569682151589, 0.6356968215158925, 0.5843520782396088, 0.6356968215158925], "precision": [0.7598072869617584, 0.5602153943617357, 0.7251585365853659, 0.7330187522980758, 0.7561583845264886, 0.6482565265351147, 0.7645856085426525, 0.7526657575557332, 0.6039193337867897, 0.7707246165261711, 0.664623743547949, 0.7366740347659615], "recall": [0.5878048780487805, 0.5707317073170731, 0.6121951219512195, 0.6146341463414634, 0.5365853658536586, 0.573170731707317, 0.6073170731707317, 0.5403422982885085, 0.6063569682151589, 0.6356968215158925, 0.5843520782396088, 0.6356968215158925], "f1": [0.447558881780261, 0.4375897472952645, 0.48754732653687083, 0.49460873983739845, 0.39686492170351567, 0.44381794761250287, 0.47788569453293445, 0.3843236314968585, 0.4789772792633549, 0.5106261644264297, 0.45127319778981956, 0.5185150294377813], "time": [0.6584482192993164, 0.6105592250823975, 0.5577139854431152, 0.5803937911987305, 0.5890586376190186, 0.5475356578826904, 0.6493983268737793, 0.6076107025146484, 0.6286835670471191, 0.478468656539917, 0.619189977645874, 0.5913047790527344]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4268292682926829, 0.44390243902439025, 0.4, 0.3926829268292683, 0.44390243902439025, 0.4170731707317073, 0.4585365853658537, 0.43765281173594134, 0.4669926650366748, 0.4229828850855746, 0.4034229828850856, 0.3960880195599022], "precision": [0.756157194823782, 0.753903035362872, 0.16136919315403422, 0.7624187488818653, 0.38304868060965624, 0.17395002974419987, 0.7546077135648204, 0.5284707932895979, 0.7538953786712548, 0.7575677477877967, 0.7619569537620292, 0.5593682376415703], "recall": [0.4268292682926829, 0.44390243902439025, 0.4, 0.3926829268292683, 0.44390243902439025, 0.4170731707317073, 0.4585365853658537, 0.43765281173594134, 0.4669926650366748, 0.4229828850855746, 0.4034229828850856, 0.3960880195599022], "f1": [0.25818131685887286, 0.27572083325844315, 0.22996515679442506, 0.224327944341627, 0.278671387745565, 0.2455060660761513, 0.2990901738052016, 0.2775776008416831, 0.30806093054562234, 0.25708214538512086, 0.24043141981473193, 0.231824911408127], "time": [10.244107007980347, 10.457476139068604, 10.493778228759766, 10.500609636306763, 10.315780878067017, 10.450939416885376, 10.49255895614624, 10.293627738952637, 10.525280237197876, 10.578944444656372, 10.240758895874023, 10.36460018157959]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6673448626653102, 0.6622583926754833, 0.6541200406917599, 0.6805696846388606, 0.7060020345879959], "precision": [0.6890004972372412, 0.656403058328645, 0.6588217937328052, 0.6778196371278772, 0.7044542569548299], "recall": [0.6673448626653102, 0.6622583926754833, 0.6541200406917599, 0.6805696846388606, 0.7060020345879959], "f1": [0.6441856037152699, 0.6476308218975467, 0.6352530073425812, 0.6629327568439711, 0.6930179378722646], "time": [9.783688068389893, 10.248724222183228, 9.968825340270996, 9.866889476776123, 9.902105808258057]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7080366225839267, 0.6897253306205493, 0.6734486266531028, 0.6632756866734486, 0.671414038657172], "precision": [0.7073283266570239, 0.7014893980816405, 0.6707236822348991, 0.6637342819048367, 0.6778778525168678], "recall": [0.7080366225839267, 0.6897253306205493, 0.6734486266531028, 0.6632756866734486, 0.671414038657172], "f1": [0.6954440828239236, 0.669695757869129, 0.6599273602945044, 0.6442026560974866, 0.653444887251142], "time": [0.7496037483215332, 0.697577714920044, 0.6830034255981445, 0.7021028995513916, 0.6511874198913574]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6541200406917599, 0.676500508646999, 0.6602238046795524, 0.688708036622584, 0.698880976602238], "precision": [0.6678255327735922, 0.6723235725958362, 0.6679844514622357, 0.6874042351692325, 0.6957591276105082], "recall": [0.6541200406917599, 0.676500508646999, 0.6602238046795524, 0.688708036622584, 0.698880976602238], "f1": [0.6562049724297339, 0.6714632118627295, 0.6621228808207933, 0.6878237069353321, 0.6916850930755399], "time": [10.232176780700684, 10.167831182479858, 10.35117244720459, 10.165839672088623, 9.831181049346924]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6602238046795524, 0.6775178026449644, 0.6683621566632757, 0.6856561546286877, 0.6663275686673449], "precision": [0.661327707307927, 0.6890432091587412, 0.6713373001913707, 0.6960054665053733, 0.6684878454970011], "recall": [0.6602238046795524, 0.6775178026449644, 0.6683621566632757, 0.6856561546286877, 0.6663275686673449], "f1": [0.6607076305221987, 0.6803423502386646, 0.6693840041166309, 0.6874638407532809, 0.6671328100698793], "time": [0.7004055976867676, 0.7330985069274902, 0.7674036026000977, 0.7007122039794922, 0.7172791957855225]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6703967446592065, 0.6866734486266531, 0.6948118006103764, 0.6805696846388606, 0.6856561546286877], "precision": [0.6688793617824546, 0.6844382250015133, 0.6939464013695281, 0.6803158206573334, 0.6829534880541948], "recall": [0.6703967446592065, 0.6866734486266531, 0.6948118006103764, 0.6805696846388605, 0.6856561546286877], "f1": [0.6693004367833479, 0.6815308013047476, 0.6943328174157942, 0.6804399309194048, 0.6832087280039051], "time": [5.045785903930664, 5.774301528930664, 5.765153884887695, 5.62904691696167, 5.582610368728638]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5961342828077314, 0.6134282807731435, 0.6164801627670397, 0.5981688708036622, 0.6063072227873856], "precision": [0.6680219888690992, 0.6837670721720368, 0.7001334049727407, 0.6982019129754512, 0.723190451274447], "recall": [0.5961342828077314, 0.6134282807731435, 0.6164801627670397, 0.5981688708036622, 0.6063072227873856], "f1": [0.4801814006564407, 0.5076430437637208, 0.5065811097712907, 0.486848046208461, 0.4924981299941181], "time": [1.683211326599121, 1.6182670593261719, 1.5979571342468262, 1.6000022888183594, 1.6341824531555176]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.42929806714140384, 0.422177009155646, 0.43540183112919634, 0.427263479145473, 0.422177009155646], "precision": [0.7566638997934207, 0.52452117654808, 0.7548228135178773, 0.756294766126186, 0.7598348885790956], "recall": [0.42929806714140384, 0.422177009155646, 0.43540183112919634, 0.427263479145473, 0.422177009155646], "f1": [0.263698535231125, 0.2599598964351481, 0.2664774254312898, 0.25932317302400154, 0.2632984625106912], "time": [17.0179762840271, 17.08372163772583, 15.605048656463623, 9.850744724273682, 10.10023021697998]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6866734486266531, 0.6754832146490336, 0.6785350966429298, 0.6856561546286877, 0.6856561546286877], "precision": [0.6907722469485091, 0.6769145092759569, 0.6730391436833423, 0.6913481142395508, 0.6942320615807799], "recall": [0.6866734486266531, 0.6754832146490336, 0.6785350966429298, 0.6856561546286877, 0.6856561546286877], "f1": [0.6719428553558331, 0.6600149523786945, 0.6595454547422511, 0.6682690467653394, 0.66867421919633], "time": [10.584021806716919, 10.121266841888428, 10.993265628814697, 10.362537860870361, 10.15393853187561]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6805696846388606, 0.6836215666327569, 0.669379450661241, 0.681586978636826, 0.6846388606307223], "precision": [0.6903829982481078, 0.6775174944842773, 0.6693421348389456, 0.6866194875870144, 0.6923875922609399], "recall": [0.6805696846388606, 0.6836215666327569, 0.669379450661241, 0.681586978636826, 0.6846388606307223], "f1": [0.659751741725292, 0.6725548023760755, 0.6518734624818081, 0.6634678462922672, 0.668727037130657], "time": [0.6873364448547363, 0.7185821533203125, 0.6873753070831299, 0.7029948234558105, 0.7810368537902832]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6724313326551373, 0.6673448626653102, 0.6612410986775178, 0.7060020345879959, 0.6388606307222787], "precision": [0.667645969857021, 0.6684500819655084, 0.663592179551861, 0.7146108311001105, 0.6637819559935407], "recall": [0.6724313326551373, 0.6673448626653102, 0.6612410986775178, 0.7060020345879959, 0.6388606307222787], "f1": [0.6672924405916745, 0.6678140878578446, 0.6620558580996608, 0.7084183921720053, 0.638655818651193], "time": [9.857095718383789, 9.825857162475586, 9.607141494750977, 9.513438701629639, 9.513415575027466]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6500508646998983, 0.6785350966429298, 0.669379450661241, 0.6581892166836215, 0.6907426246185148], "precision": [0.6528850956634958, 0.6776308048407857, 0.6769928928115143, 0.6586063436849378, 0.6991930497135683], "recall": [0.6500508646998983, 0.6785350966429298, 0.669379450661241, 0.6581892166836215, 0.6907426246185148], "f1": [0.6510824174732792, 0.677800826300143, 0.6713958589154247, 0.6583869036023751, 0.6934682662349713], "time": [0.7654094696044922, 0.7029123306274414, 0.6717298030853271, 0.7342257499694824, 0.7342181205749512]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6826042726347915, 0.6917599186164801, 0.6948118006103764, 0.6775178026449644, 0.6836215666327569], "precision": [0.6798722029409625, 0.6889529501525941, 0.6929784452442842, 0.6792845126431991, 0.6815688506171494], "recall": [0.6826042726347915, 0.6917599186164801, 0.6948118006103764, 0.6775178026449644, 0.6836215666327569], "f1": [0.6803099935077217, 0.689248599043133, 0.6920617064627748, 0.6782438029941426, 0.6823377408062926], "time": [2.0933303833007812, 2.06204891204834, 2.0463995933532715, 2.0776309967041016, 2.108922004699707]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6174974567650051, 0.6093591047812817, 0.5971515768056969, 0.5859613428280773, 0.6154628687690743], "precision": [0.698353235194011, 0.6737559976578632, 0.6609261624521034, 0.7068157634135643, 0.649981327748041], "recall": [0.6174974567650051, 0.6093591047812817, 0.5971515768056969, 0.5859613428280773, 0.6154628687690743], "f1": [0.518821208473623, 0.5025999412496417, 0.47899342777962434, 0.47410114539870374, 0.5038405730664688], "time": [0.6561410427093506, 0.6561076641082764, 0.6404914855957031, 0.6248676776885986, 0.6404335498809814]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4252288911495422, 0.41912512716174977, 0.42828077314343843, 0.43540183112919634, 0.4323499491353001], "precision": [0.6752843451456719, 0.17566587221835292, 0.72982546487492, 0.1895747545506572, 0.47057188166069863], "recall": [0.4252288911495422, 0.41912512716174977, 0.42828077314343843, 0.43540183112919634, 0.4323499491353001], "f1": [0.2685881324399978, 0.2475692507392701, 0.277604707077367, 0.2641417203732049, 0.26631226618440584], "time": [9.451069355010986, 9.560482263565063, 9.779030561447144, 9.700892210006714, 10.212125778198242]}
