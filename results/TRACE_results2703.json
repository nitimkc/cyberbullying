{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7195121951219512, 0.6585365853658537, 0.7048780487804878, 0.7073170731707317, 0.6780487804878049, 0.7024390243902439, 0.6829268292682927, 0.6650366748166259, 0.687041564792176, 0.6919315403422983, 0.6894865525672371, 0.706601466992665], "precision": [0.7189451697196045, 0.6608284939992256, 0.7000542005420055, 0.7025435540069687, 0.6738632867737198, 0.6975145180023229, 0.6787771468505917, 0.6666310483564792, 0.6982264684840129, 0.6856888608035506, 0.6860265726457966, 0.7088557446676244], "recall": [0.7195121951219512, 0.6585365853658537, 0.7048780487804878, 0.7073170731707317, 0.6780487804878049, 0.7024390243902439, 0.6829268292682927, 0.6650366748166259, 0.687041564792176, 0.6919315403422983, 0.6894865525672371, 0.706601466992665], "f1": [0.7130217761725688, 0.6502082093991672, 0.7005537723074501, 0.7035239132766241, 0.6691580566883425, 0.6985826451645677, 0.6775625843919878, 0.661405094824068, 0.6753938219996495, 0.6831399640171611, 0.6836402993197763, 0.6990330898867467], "time": [15.54978060722351, 15.776326179504395, 15.659775495529175, 15.54766321182251, 15.775087356567383, 15.358322858810425, 15.89521837234497, 15.850865602493286, 15.71213412284851, 15.866190910339355, 16.34734582901001, 15.849737167358398]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6878048780487804, 0.675609756097561, 0.6951219512195121, 0.7073170731707317, 0.7414634146341463, 0.6853658536585366, 0.7097560975609756, 0.706601466992665, 0.6919315403422983, 0.684596577017115, 0.7041564792176039, 0.6894865525672371], "precision": [0.6837257156690673, 0.6822546862717168, 0.6916437650291996, 0.7060463355242319, 0.7370046161548915, 0.6820273806820476, 0.7118363000445633, 0.7080495846486854, 0.6890520183690049, 0.6856811748585084, 0.7013399031800782, 0.6870378291347656], "recall": [0.6878048780487804, 0.675609756097561, 0.6951219512195121, 0.7073170731707317, 0.7414634146341463, 0.6853658536585366, 0.7097560975609756, 0.706601466992665, 0.6919315403422983, 0.684596577017115, 0.7041564792176039, 0.6894865525672371], "f1": [0.6772914578460854, 0.6659405500434458, 0.6930428468463402, 0.7037541899038416, 0.7343502387404828, 0.6771632317946608, 0.7033068759153122, 0.6977286483052879, 0.6854958850015722, 0.676374723115145, 0.699066577752409, 0.6860307740806999], "time": [1.0838232040405273, 1.100428581237793, 0.9646947383880615, 0.9870069026947021, 1.0430595874786377, 0.9752638339996338, 1.0060641765594482, 1.01535964012146, 0.9925804138183594, 0.9646143913269043, 0.9854135513305664, 0.9949121475219727]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7048780487804878, 0.7, 0.675609756097561, 0.6731707317073171, 0.7170731707317073, 0.6731707317073171, 0.6951219512195121, 0.6503667481662592, 0.726161369193154, 0.7114914425427873, 0.6674816625916871, 0.6674816625916871], "precision": [0.7026869513713817, 0.6991341674593303, 0.6982623943886499, 0.6688650436608509, 0.7220966216520515, 0.6710291493158834, 0.6950835939974659, 0.6736404426027806, 0.7392070351645945, 0.7082902089459067, 0.6630465302162462, 0.6709972388225721], "recall": [0.7048780487804878, 0.7, 0.675609756097561, 0.6731707317073171, 0.7170731707317073, 0.6731707317073171, 0.6951219512195121, 0.6503667481662592, 0.726161369193154, 0.7114914425427873, 0.6674816625916871, 0.6674816625916871], "f1": [0.7001226828704898, 0.699462394984783, 0.6772200025842081, 0.6696985401137917, 0.7184963921031505, 0.6710629799282164, 0.691006651939093, 0.654165251635581, 0.7276707107986627, 0.706413047055923, 0.6632948593747597, 0.6686674994572627], "time": [15.712414503097534, 15.660269260406494, 15.913758277893066, 15.796605825424194, 15.899445056915283, 15.922319173812866, 15.662030696868896, 16.0206196308136, 16.381886959075928, 15.781461954116821, 16.41035485267639, 14.461347341537476]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6804878048780488, 0.7024390243902439, 0.6926829268292682, 0.6829268292682927, 0.7268292682926829, 0.6731707317073171, 0.6731707317073171, 0.6821515892420538, 0.684596577017115, 0.6625916870415648, 0.6894865525672371, 0.6821515892420538], "precision": [0.6880145216546505, 0.7083692013390722, 0.7013115755486714, 0.6834868944741822, 0.7313522127547174, 0.6830704764707634, 0.6804831529146876, 0.6867810937327223, 0.6872663114850365, 0.6788151229282596, 0.690202436848341, 0.6893351183142311], "recall": [0.6804878048780488, 0.7024390243902439, 0.6926829268292682, 0.6829268292682927, 0.7268292682926829, 0.6731707317073171, 0.6731707317073171, 0.6821515892420538, 0.684596577017115, 0.6625916870415648, 0.6894865525672371, 0.6821515892420538], "f1": [0.6821186046576265, 0.7040562036055144, 0.6946359782078358, 0.6831906108094333, 0.7279866320725749, 0.6758334856208096, 0.6748981176298249, 0.6831484461899968, 0.6856009913658224, 0.6655990220048901, 0.6896426109479928, 0.6844322942795058], "time": [0.8242812156677246, 0.8232724666595459, 1.0146727561950684, 0.8331339359283447, 0.9068343639373779, 0.8247416019439697, 0.9035944938659668, 0.9265873432159424, 0.8385937213897705, 0.8145015239715576, 0.8325996398925781, 0.8421037197113037]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6682926829268293, 0.7365853658536585, 0.7024390243902439, 0.7048780487804878, 0.6902439024390243, 0.697560975609756, 0.6951219512195121, 0.7310513447432763, 0.7090464547677262, 0.687041564792176, 0.6772616136919315, 0.706601466992665], "precision": [0.6691260285116414, 0.7362095263516638, 0.7043515710832784, 0.7045983981175619, 0.6963842349145969, 0.6965618353771663, 0.6946011429822353, 0.7299301225387363, 0.7241933388147637, 0.6888758618081342, 0.6788004843052261, 0.7088924463447142], "recall": [0.6682926829268293, 0.7365853658536585, 0.7024390243902439, 0.7048780487804878, 0.6902439024390243, 0.697560975609756, 0.6951219512195121, 0.7310513447432763, 0.7090464547677262, 0.687041564792176, 0.6772616136919315, 0.706601466992665], "f1": [0.6686935275989048, 0.7363797789583953, 0.7031282051583816, 0.7047340059151151, 0.6918386124400158, 0.6969945685263018, 0.6936535318298189, 0.7300482958333558, 0.7125838622116754, 0.6830731385594723, 0.6778856153873324, 0.7074768551322321], "time": [3.8765997886657715, 3.7399728298187256, 3.7222037315368652, 3.8681654930114746, 3.769965171813965, 3.7409205436706543, 3.908879280090332, 3.802208185195923, 3.7538769245147705, 3.951439619064331, 3.7671661376953125, 3.741053581237793]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5853658536585366, 0.6585365853658537, 0.5951219512195122, 0.6439024390243903, 0.6682926829268293, 0.6024390243902439, 0.5853658536585366, 0.6308068459657702, 0.6136919315403423, 0.6014669926650367, 0.6479217603911981, 0.628361858190709], "precision": [0.6626282945116236, 0.7203373821124371, 0.6977303523035231, 0.6678548625426418, 0.7215149712472735, 0.6928524598849031, 0.6745644599303136, 0.7000757851626603, 0.6360194092665108, 0.7316403645254501, 0.6913143300137158, 0.6476304013713298], "recall": [0.5853658536585366, 0.6585365853658537, 0.5951219512195122, 0.6439024390243903, 0.6682926829268293, 0.6024390243902439, 0.5853658536585366, 0.6308068459657702, 0.6136919315403423, 0.6014669926650367, 0.6479217603911981, 0.628361858190709], "f1": [0.47661281807623274, 0.5782795427476567, 0.5042084257206209, 0.5559681680056854, 0.6019533312216239, 0.5082395440488386, 0.4817882370861179, 0.5474993134324903, 0.5316867178274449, 0.5002621638852242, 0.5764906955019765, 0.5505070261581055], "time": [0.7851130962371826, 0.8179447650909424, 0.7733879089355469, 0.8243803977966309, 0.8383092880249023, 0.795196533203125, 0.8950905799865723, 0.7842121124267578, 0.827843427658081, 0.802053689956665, 0.8663420677185059, 0.8078622817993164]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5682926829268292, 0.5975609756097561, 0.6073170731707317, 0.6024390243902439, 0.5780487804878048, 0.5609756097560976, 0.5585365853658537, 0.6014669926650367, 0.628361858190709, 0.4938875305623472, 0.5843520782396088, 0.5158924205378973], "precision": [0.6281923871314716, 0.6543185651754613, 0.6887455072762783, 0.6592962532123946, 0.615999061913696, 0.6849315174154873, 0.6480041735268215, 0.646931493141762, 0.6959752695220159, 0.6353227413789492, 0.6690762198362922, 0.6739885000229775], "recall": [0.5682926829268292, 0.5975609756097561, 0.6073170731707317, 0.6024390243902439, 0.5780487804878048, 0.5609756097560976, 0.5585365853658537, 0.6014669926650367, 0.628361858190709, 0.4938875305623472, 0.5843520782396088, 0.5158924205378973], "f1": [0.5489432382653346, 0.577312081783689, 0.5817972596509181, 0.5958906145121214, 0.5644803859094635, 0.5379168211134999, 0.5294482596189186, 0.5965900703473899, 0.6209824063039114, 0.45745189605014336, 0.576096057875875, 0.4744155578624796], "time": [14.109991073608398, 14.205291509628296, 14.445354700088501, 14.284917116165161, 14.079829931259155, 14.263375043869019, 14.348647356033325, 14.355335712432861, 14.349632978439331, 14.025362014770508, 14.283794164657593, 14.354540348052979]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.681586978636826, 0.669379450661241, 0.6948118006103764, 0.6958290946083419, 0.7070193285859614], "precision": [0.68053862267494, 0.6676520001506114, 0.6920045059390678, 0.6925636290369291, 0.7031566950321441], "recall": [0.681586978636826, 0.669379450661241, 0.6948118006103764, 0.6958290946083419, 0.7070193285859614], "f1": [0.6714523987157529, 0.6628182367542919, 0.6888466164468201, 0.6869889199155796, 0.7023546476883619], "time": [15.350445032119751, 25.50536012649536, 17.946158409118652, 15.266668558120728, 15.047447919845581]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7080366225839267, 0.6978636826042727, 0.676500508646999, 0.6927772126144456, 0.6937945066124109], "precision": [0.7049450147141529, 0.6947722541105756, 0.6714915385397697, 0.6914993491694416, 0.6959381877594564], "recall": [0.7080366225839267, 0.6978636826042727, 0.676500508646999, 0.6927772126144456, 0.6937945066124109], "f1": [0.7019309313179968, 0.6943913367253053, 0.6712279787483787, 0.685763306571593, 0.6843322480681527], "time": [1.0405781269073486, 0.9834961891174316, 1.2597408294677734, 1.0666208267211914, 1.0163054466247559]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.671414038657172, 0.6937945066124109, 0.6948118006103764, 0.6866734486266531, 0.6805696846388606], "precision": [0.6686756438400171, 0.6947746095773009, 0.6962974328693147, 0.6943920275403676, 0.690546309630328], "recall": [0.671414038657172, 0.6937945066124109, 0.6948118006103764, 0.6866734486266531, 0.6805696846388606], "f1": [0.6680574191111546, 0.6942263916139647, 0.6953219506732146, 0.6887674365878604, 0.6832078392036133], "time": [15.749475240707397, 15.349412441253662, 15.434720039367676, 15.866493225097656, 15.183594465255737]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6775178026449644, 0.6622583926754833, 0.6632756866734486, 0.7009155645981688, 0.6866734486266531], "precision": [0.6809491584866507, 0.6779795489904804, 0.6704913414060225, 0.7092185020403865, 0.6856442186166093], "recall": [0.6775178026449644, 0.6622583926754833, 0.6632756866734486, 0.7009155645981688, 0.6866734486266531], "f1": [0.6784722412278863, 0.664829650440647, 0.6651081663942567, 0.7026370909890748, 0.6860874247282202], "time": [1.033470630645752, 0.9826014041900635, 1.0504283905029297, 1.050574779510498, 1.0170395374298096]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7060020345879959, 0.6968463886063072, 0.671414038657172, 0.7080366225839267, 0.6998982706002035], "precision": [0.7091521235405333, 0.7012641513570509, 0.6717442577145485, 0.7072829499338212, 0.6985950805639607], "recall": [0.7060020345879959, 0.6968463886063072, 0.671414038657172, 0.7080366225839267, 0.6998982706002035], "f1": [0.7071213718176206, 0.6984921694533126, 0.6715728995363974, 0.7074939995580632, 0.6989245793917352], "time": [12.299717903137207, 12.366785049438477, 12.349034070968628, 12.522257804870605, 12.350802421569824]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6052899287894201, 0.671414038657172, 0.6083418107833164, 0.6154628687690743, 0.6185147507629705], "precision": [0.6951869053957499, 0.6926071595840124, 0.6881750807716852, 0.6932110770672472, 0.6886384857125117], "recall": [0.6052899287894201, 0.671414038657172, 0.6083418107833164, 0.6154628687690743, 0.6185147507629705], "f1": [0.5087983875990589, 0.6118570853466427, 0.5147452814296701, 0.5244390512681435, 0.5279738482624434], "time": [2.418196678161621, 2.305382013320923, 2.3494374752044678, 1.0312600135803223, 1.0153918266296387]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.6174974567650051, 0.6439471007121058, 0.5768056968463886, 0.595116988809766, 0.5635808748728383], "precision": [0.671327297948882, 0.6589783902093159, 0.6130442318704324, 0.6397231042195437, 0.6392792057478454], "recall": [0.6174974567650051, 0.6439471007121058, 0.5768056968463886, 0.595116988809766, 0.5635808748728383], "f1": [0.6039345744289725, 0.6458174370585246, 0.5776151451744514, 0.5925685605284028, 0.5500667984744491], "time": [13.992794036865234, 13.887353420257568, 13.902976036071777, 14.003891706466675, 14.05922818183899]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.676500508646999, 0.6795523906408952, 0.7060020345879959, 0.6937945066124109, 0.6968463886063072], "precision": [0.6803084420917095, 0.68185242181423, 0.7019265813398605, 0.6896988027119436, 0.6951141793344681], "recall": [0.676500508646999, 0.6795523906408952, 0.7060020345879959, 0.6937945066124109, 0.6968463886063072], "f1": [0.6643860199275002, 0.6704757703329434, 0.7029380784937882, 0.690257230567143, 0.6909181880576929], "time": [15.455268383026123, 16.119423866271973, 15.367629289627075, 15.672823190689087, 15.31898283958435]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7049847405900305, 0.7029501525940997, 0.6846388606307223, 0.698880976602238, 0.6937945066124109], "precision": [0.7029955150019364, 0.7043162624647874, 0.6812782727474125, 0.6943413764828638, 0.6911438107901401], "recall": [0.7049847405900305, 0.7029501525940997, 0.6846388606307223, 0.698880976602238, 0.6937945066124109], "f1": [0.6998309600900249, 0.6934547501703722, 0.6799706993877458, 0.6909532018295048, 0.6889329466568311], "time": [1.031010389328003, 1.171438217163086, 1.046665906906128, 1.031040906906128, 1.0466341972351074]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6724313326551373, 0.6632756866734486, 0.6734486266531028, 0.7222787385554426, 0.6734486266531028], "precision": [0.6873319860378547, 0.6860223697833409, 0.669219261434065, 0.7225856229380916, 0.6746951614281654], "recall": [0.6724313326551373, 0.6632756866734486, 0.6734486266531028, 0.7222787385554426, 0.6734486266531028], "f1": [0.6747494475055832, 0.6648538337798363, 0.6686742809203231, 0.7170296176193018, 0.6739877448934704], "time": [17.99317693710327, 15.823777914047241, 16.08048939704895, 15.51606011390686, 15.703505754470825]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6917599186164801, 0.6927772126144456, 0.6571719226856562, 0.6948118006103764, 0.6846388606307223], "precision": [0.6993587049714136, 0.6989112020871588, 0.6638726333368486, 0.7003421420940965, 0.6904429656496344], "recall": [0.6917599186164801, 0.6927772126144456, 0.6571719226856562, 0.6948118006103764, 0.6846388606307223], "f1": [0.6937076894918968, 0.694586348699556, 0.6590294385024547, 0.6959246247140448, 0.686047651986787], "time": [1.1432325839996338, 1.1136765480041504, 1.145354986190796, 1.0466277599334717, 1.10960054397583]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7212614445574771, 0.6958290946083419, 0.6948118006103764, 0.6927772126144456, 0.7100712105798576], "precision": [0.7210715496778568, 0.6994500262390091, 0.6939769105233135, 0.6912834974956971, 0.7123860423896973], "recall": [0.7212614445574771, 0.6958290946083419, 0.6948118006103764, 0.6927772126144456, 0.7100712105798576], "f1": [0.7211634919969531, 0.6971909595139134, 0.6942903116781827, 0.6916804538515704, 0.7109049105649186], "time": [5.193171739578247, 4.970146656036377, 5.3601484298706055, 5.035435438156128, 4.652715444564819]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6164801627670397, 0.5981688708036622, 0.6581892166836215, 0.5991861648016277, 0.6480162767039674], "precision": [0.6636812035235843, 0.683400047178852, 0.7147586434522646, 0.6902124491174721, 0.7229203466720681], "recall": [0.6164801627670397, 0.5981688708036622, 0.6581892166836215, 0.5991861648016277, 0.6480162767039674], "f1": [0.5245677009664803, 0.4989314262851216, 0.5863903238606528, 0.49630751660862943, 0.5707583704234078], "time": [0.984138011932373, 0.9529330730438232, 0.9997286796569824, 0.9372451305389404, 1.2018389701843262]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5707019328585962, 0.6500508646998983, 0.6205493387589013, 0.5808748728382502, 0.6459816887080366], "precision": [0.652858913904742, 0.679597499131221, 0.651560215150044, 0.6413401110987418, 0.6607238349685933], "recall": [0.5707019328585962, 0.6500508646998983, 0.6205493387589013, 0.5808748728382502, 0.6459816887080366], "f1": [0.5468860443211052, 0.6510283317233614, 0.6219355704166076, 0.5669998023936675, 0.6484227621152748], "time": [16.406723499298096, 15.243061780929565, 16.193686723709106, 15.307660579681396, 14.933995962142944]}
