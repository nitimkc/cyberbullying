{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7019328585961343, 0.698880976602238, 0.688708036622584, 0.6876907426246185, 0.681586978636826], "precision": [0.7038134498637091, 0.6987458756533019, 0.6857964222632803, 0.6842820771964165, 0.6774475382549013], "recall": [0.7019328585961343, 0.698880976602238, 0.688708036622584, 0.6876907426246185, 0.681586978636826], "f1": [0.6904779094503268, 0.6894250150903583, 0.6827416843828173, 0.6832617192145782, 0.6730954928666318], "time": [14.631186723709106, 13.911590099334717, 13.768427610397339, 14.258675575256348, 13.982940912246704]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6968463886063072, 0.6846388606307223, 0.7121057985757884, 0.7009155645981688, 0.7019328585961343], "precision": [0.6921580107142767, 0.6811551209749581, 0.7156092385683884, 0.7035830076648371, 0.6986478776222014], "recall": [0.6968463886063073, 0.6846388606307223, 0.7121057985757884, 0.7009155645981688, 0.7019328585961343], "f1": [0.688014111069985, 0.6784147367029255, 0.7029576224815665, 0.6913747122334418, 0.6969040979596108], "time": [0.9216461181640625, 0.9196858406066895, 0.9274892807006836, 0.935171365737915, 0.9373319149017334]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.688708036622584, 0.688708036622584, 0.6663275686673449, 0.676500508646999, 0.6663275686673449], "precision": [0.6875357744538891, 0.6961106971938161, 0.691154027452628, 0.67612595522222, 0.6846810310068239], "recall": [0.688708036622584, 0.688708036622584, 0.6663275686673449, 0.676500508646999, 0.6663275686673449], "f1": [0.6879814110167223, 0.6705926408796488, 0.6669112502068557, 0.6763020141470659, 0.6694402886816778], "time": [13.734365701675415, 13.978379487991333, 15.427321434020996, 14.45518445968628, 14.314207077026367]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6775178026449644, 0.6653102746693794, 0.6602238046795524, 0.6978636826042727, 0.6754832146490336], "precision": [0.6883264141977808, 0.6672993243010975, 0.6644741993106741, 0.7051247471949692, 0.6770974006397444], "recall": [0.6775178026449644, 0.6653102746693794, 0.6602238046795524, 0.6978636826042727, 0.6754832146490336], "f1": [0.6799793310375059, 0.6660570358429674, 0.66165344205015, 0.699153830347331, 0.6761724796760105], "time": [0.9216585159301758, 0.9216973781585693, 0.9797811508178711, 0.9675991535186768, 0.9216668605804443]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7019328585961343, 0.6958290946083419, 0.688708036622584, 0.6907426246185148, 0.6897253306205493], "precision": [0.7015346599649801, 0.697128007472865, 0.688708036622584, 0.6911614825105509, 0.6914021394960383], "recall": [0.7019328585961343, 0.6958290946083419, 0.688708036622584, 0.6907426246185148, 0.6897253306205493], "f1": [0.7001060507808013, 0.6964201710755189, 0.688708036622584, 0.6909406043010654, 0.6904033691064906], "time": [3.561702013015747, 3.5616626739501953, 3.530451774597168, 3.733505964279175, 3.6085081100463867]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6195320447609359, 0.6378433367243134, 0.6185147507629705, 0.6063072227873856, 0.6144455747711088], "precision": [0.7381869624961084, 0.6801693287112438, 0.6675892235467585, 0.6351002496683567, 0.6786566570331348], "recall": [0.6195320447609359, 0.6378433367243134, 0.6185147507629705, 0.6063072227873856, 0.6144455747711088], "f1": [0.5312039163473002, 0.5597997768447962, 0.5233711176618192, 0.5173893020857702, 0.5217848569254837], "time": [0.8435235023498535, 0.8279368877410889, 1.0466337203979492, 0.8435542583465576, 0.8748302459716797]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4323499491353001, 0.4496439471007121, 0.4883011190233978, 0.47711088504577825, 0.4537131230925738], "precision": [0.6752203312003557, 0.702567119220617, 0.695311489051647, 0.6759117336043728, 0.6668362156663276], "recall": [0.4323499491353001, 0.4496439471007121, 0.4883011190233978, 0.47711088504577825, 0.4537131230925738], "f1": [0.29506819392882466, 0.33241047772393245, 0.38459885347934664, 0.36182594419884684, 0.3138580032085413], "time": [13.668726921081543, 13.593944311141968, 13.903005599975586, 13.903074264526367, 14.113102912902832]}
