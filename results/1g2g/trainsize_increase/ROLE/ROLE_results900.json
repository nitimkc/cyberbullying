{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90]], "accuracy": [0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.7, 0.6666666666666666, 0.6, 0.5777777777777777, 0.6333333333333333, 0.5666666666666667, 0.6], "precision": [0.4855673133450911, 0.49678130511463847, 0.42464550264550266, 0.6122740005092947, 0.5667294610151752, 0.47859078590785914, 0.44264705882352945, 0.5853462157809984, 0.4374719416386083, 0.5024330484330484], "recall": [0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.7, 0.6666666666666666, 0.6, 0.5777777777777777, 0.6333333333333333, 0.5666666666666667, 0.6], "f1_valid": [0.5387072789037748, 0.5452497911445281, 0.4788065401599236, 0.6443408443408444, 0.6055709468891609, 0.5283668107466735, 0.49069507490560127, 0.5828835088094346, 0.4796401362160449, 0.539511095002215], "f1_train": [0.8599355951348705, 0.854078661784227, 0.8585781879557067, 0.8742040229835866, 0.8564613326582201, 0.8690605901225646, 0.8564842944891142, 0.8540072547935227, 0.8641214867973839, 0.855920952724387], "time": [0.9216134548187256, 0.8904163837432861, 0.9997634887695312, 0.9060056209564209, 0.9841136932373047, 1.0309772491455078, 0.9841339588165283, 1.0309629440307617, 0.968489408493042, 1.062253475189209]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90]], "accuracy": [0.6444444444444445, 0.6444444444444445, 0.7, 0.6888888888888889, 0.6, 0.5888888888888889, 0.6555555555555556, 0.6555555555555556, 0.5888888888888889, 0.5666666666666667], "precision": [0.6109491677912731, 0.5864915824915825, 0.6152903091060986, 0.604908160192713, 0.5109135802469136, 0.44601041359662047, 0.6236631393298059, 0.5696765240692889, 0.6383563301863955, 0.5214141414141413], "recall": [0.6444444444444445, 0.6444444444444445, 0.7, 0.6888888888888889, 0.6, 0.5888888888888889, 0.6555555555555556, 0.6555555555555556, 0.5888888888888889, 0.5666666666666667], "f1_valid": [0.6135869507620401, 0.6093482905982905, 0.6546726684040116, 0.6429112554112554, 0.5460273576552647, 0.5057442199377683, 0.6027752323537549, 0.6082398105039614, 0.5283189515122289, 0.5159435626102292], "f1_train": [0.9826458367089637, 0.982513668950952, 0.9817335897493661, 0.9836710556088051, 0.9785417910071862, 0.9770547868935516, 0.986409001488552, 0.979482297213871, 0.9785907937981816, 0.9750575030123061], "time": [1.1559813022613525, 1.1715989112854004, 1.140357255935669, 1.155975580215454, 1.2809481620788574, 1.1872234344482422, 1.171607494354248, 1.1559467315673828, 1.1715900897979736, 1.1872279644012451]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90], [810, 90]], "accuracy": [0.5333333333333333, 0.3888888888888889, 0.5222222222222223, 0.4888888888888889, 0.5777777777777777, 0.5555555555555556, 0.5111111111111111, 0.4777777777777778, 0.6, 0.6111111111111112], "precision": [0.5135612535612535, 0.34282504000837555, 0.5274190280786054, 0.4589329805996472, 0.621080776808331, 0.5216931216931217, 0.4748643845866068, 0.45337301587301587, 0.5569756484650101, 0.712962962962963], "recall": [0.5333333333333333, 0.3888888888888889, 0.5222222222222223, 0.4888888888888889, 0.5777777777777777, 0.5555555555555556, 0.5111111111111111, 0.4777777777777778, 0.6, 0.6111111111111112], "f1_valid": [0.4971827670513212, 0.34694738894738897, 0.5034256544995259, 0.45578802556533193, 0.5781437389770723, 0.5192685820832275, 0.45722630256346186, 0.44667015580448416, 0.5769818658707548, 0.6267383512544803], "f1_train": [0.593346459337109, 0.629639859272716, 0.6398196104846471, 0.6257919438845604, 0.6292706707438444, 0.6257606858512188, 0.6294910714796584, 0.6174786611776757, 0.620671607109786, 0.6372684040189154], "time": [0.42177820205688477, 0.4373958110809326, 0.4061567783355713, 0.42177605628967285, 0.4061579704284668, 0.42177653312683105, 0.43743348121643066, 0.4530186653137207, 0.42177844047546387, 0.43740129470825195]}
