{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60]], "accuracy": [0.5333333333333333, 0.6333333333333333, 0.55, 0.6, 0.48333333333333334, 0.6833333333333333, 0.6333333333333333, 0.6, 0.65, 0.55], "precision": [0.3963317384370016, 0.5622222222222223, 0.4192929292929293, 0.4122222222222222, 0.38529534981147884, 0.6453266787658802, 0.542, 0.450566534914361, 0.577020202020202, 0.48333333333333334], "recall": [0.5333333333333333, 0.6333333333333333, 0.55, 0.6, 0.48333333333333334, 0.6833333333333333, 0.6333333333333333, 0.6, 0.65, 0.55], "f1_valid": [0.44248506571087215, 0.5925757575757575, 0.4625079365079365, 0.48719851576994433, 0.4071106337271751, 0.6488888888888888, 0.5774348763895801, 0.510797146995327, 0.5662327862667421, 0.4965123129492165], "f1_train": [0.8356496618001159, 0.8359657615178945, 0.8579599569163082, 0.849966281789367, 0.836039045480719, 0.8374891330865972, 0.8481448577936307, 0.8275621994945652, 0.8527758133747031, 0.848641876119421], "time": [0.48425936698913574, 0.4530501365661621, 0.5467469692230225, 0.46860623359680176, 0.43736767768859863, 0.49988293647766113, 0.4529855251312256, 0.42174482345581055, 0.48425889015197754, 0.48425841331481934]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60]], "accuracy": [0.6333333333333333, 0.5, 0.6833333333333333, 0.5333333333333333, 0.5833333333333334, 0.65, 0.5833333333333334, 0.6833333333333333, 0.6666666666666666, 0.5833333333333334], "precision": [0.6079793028322439, 0.397796423425714, 0.593064182194617, 0.38140535792709707, 0.5243021346469622, 0.5773199023199023, 0.5004009661835749, 0.5784484228473998, 0.5387304724261245, 0.6439186507936508], "recall": [0.6333333333333333, 0.5, 0.6833333333333333, 0.5333333333333333, 0.5833333333333334, 0.65, 0.5833333333333334, 0.6833333333333333, 0.6666666666666666, 0.5833333333333334], "f1_valid": [0.5722484113379392, 0.4383962780514505, 0.6330438164376457, 0.44412955465587045, 0.5499055330634277, 0.5951478938893128, 0.537359022556391, 0.6232804232804233, 0.5951242674819911, 0.5173389854055903], "f1_train": [0.986433959667918, 0.9861410221363218, 0.9902020792833423, 0.9860044481690918, 0.9866607803147008, 0.9823572748831695, 0.984350956636109, 0.9866406340862509, 0.9880838169632558, 0.9883415603145682], "time": [0.5935783386230469, 0.5936112403869629, 0.5779898166656494, 0.5936126708984375, 0.5935776233673096, 0.5936117172241211, 0.5936110019683838, 0.5936136245727539, 0.6092336177825928, 0.5936126708984375]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60], [540, 60]], "accuracy": [0.43333333333333335, 0.5666666666666667, 0.5, 0.38333333333333336, 0.5166666666666667, 0.5666666666666667, 0.5, 0.65, 0.5166666666666667, 0.48333333333333334], "precision": [0.352662037037037, 0.5335858585858586, 0.5791133844842286, 0.4112861811391223, 0.4675536881419234, 0.5398860398860399, 0.47791666666666666, 0.663888888888889, 0.5103785103785103, 0.46557874762808354], "recall": [0.43333333333333335, 0.5666666666666667, 0.5, 0.38333333333333336, 0.5166666666666667, 0.5666666666666667, 0.5, 0.65, 0.5166666666666667, 0.48333333333333334], "f1_valid": [0.3686850613154961, 0.5281930415263748, 0.4962962962962963, 0.35179303372851756, 0.48413742690058476, 0.5313623188405798, 0.48307148946535133, 0.65, 0.5083994708994709, 0.4413136288998358], "f1_train": [0.6310162828951461, 0.614547590007916, 0.612223248613787, 0.636872237686861, 0.6241646327650975, 0.5997385257556234, 0.6259736993544089, 0.5984591630208613, 0.6001008300704697, 0.5733414127100853], "time": [0.2655608654022217, 0.2811753749847412, 0.2811775207519531, 0.2655637264251709, 0.26554322242736816, 0.2655308246612549, 0.28118181228637695, 0.265531063079834, 0.2811858654022217, 0.2655303478240967]}
