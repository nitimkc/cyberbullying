{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7290167865707434, 0.697841726618705, 0.6930455635491607, 0.7026378896882494, 0.6762589928057554, 0.6810551558752997, 0.6714628297362111, 0.6522781774580336, 0.6730769230769231, 0.6658653846153846, 0.6778846153846154, 0.6850961538461539], "precision": [0.7182382266651157, 0.6886996150206272, 0.6780670953953017, 0.696077100583684, 0.6556688714962097, 0.6643585805512893, 0.6597984714558495, 0.6406745571284906, 0.6622102781450555, 0.6593816489743017, 0.6611472085097712, 0.6688739919645517], "recall": [0.7290167865707434, 0.697841726618705, 0.6930455635491607, 0.7026378896882494, 0.6762589928057554, 0.6810551558752997, 0.6714628297362111, 0.6522781774580336, 0.6730769230769231, 0.6658653846153846, 0.6778846153846154, 0.6850961538461539], "f1": [0.7228613007030272, 0.6914881295987615, 0.6834312390790471, 0.6922077458308267, 0.6613975342965736, 0.6715681207626242, 0.6634733878535362, 0.636957990350043, 0.6644195194340543, 0.6560038278750585, 0.6689651702786379, 0.6761970595260108], "time": [31.363916873931885, 31.49180316925049, 29.221282243728638, 30.606635808944702, 28.457616090774536, 28.453934907913208, 28.025848865509033, 28.804114818572998, 28.057853937149048, 27.55528974533081, 28.487359046936035, 29.1874520778656]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a147d7a70>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7122302158273381, 0.7074340527577938, 0.6882494004796164, 0.7218225419664268, 0.657074340527578, 0.6762589928057554, 0.7002398081534772, 0.7074340527577938, 0.6971153846153846, 0.6610576923076923, 0.7067307692307693, 0.7067307692307693], "precision": [0.6952616055466289, 0.6954757049974993, 0.680318083195781, 0.7140468963571537, 0.6381509511005914, 0.6621054298567104, 0.6879158924516017, 0.6959707517432766, 0.6801952835343246, 0.6396620046620047, 0.7037034163899836, 0.7023675791808025], "recall": [0.7122302158273381, 0.7074340527577938, 0.6882494004796164, 0.7218225419664268, 0.657074340527578, 0.6762589928057554, 0.7002398081534772, 0.7074340527577938, 0.6971153846153846, 0.6610576923076923, 0.7067307692307693, 0.7067307692307693], "f1": [0.7026088220332823, 0.6992831662858797, 0.6823569715656045, 0.7176366726389629, 0.6459778148010971, 0.6668329963672335, 0.6915356603606005, 0.7011653486381545, 0.6871954281078269, 0.6427976522130348, 0.6952556241412545, 0.6952755770179032], "time": [1.9384119510650635, 1.981611967086792, 1.9565787315368652, 2.0033087730407715, 1.89827299118042, 2.2707550525665283, 1.9301881790161133, 1.8899309635162354, 1.949531078338623, 1.896655797958374, 1.97135591506958, 2.1873891353607178]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7074340527577938, 0.6618705035971223, 0.6954436450839329, 0.6546762589928058, 0.6906474820143885, 0.6690647482014388, 0.7362110311750599, 0.6906474820143885, 0.6634615384615384, 0.6971153846153846, 0.6947115384615384, 0.7163461538461539], "precision": [0.7027451209763895, 0.6527283544008782, 0.6794430204545486, 0.640389506576557, 0.6866019179045354, 0.6515886787575926, 0.7157221591148133, 0.6854711115627444, 0.651947161701497, 0.6931616097747616, 0.6830341751621872, 0.6956956225017061], "recall": [0.7074340527577938, 0.6618705035971223, 0.6954436450839329, 0.6546762589928058, 0.6906474820143885, 0.6690647482014388, 0.7362110311750599, 0.6906474820143885, 0.6634615384615384, 0.6971153846153846, 0.6947115384615384, 0.7163461538461539], "f1": [0.7048294056207098, 0.6570286090876516, 0.6864503983612094, 0.6472550599800574, 0.6885644593243774, 0.659173553603969, 0.7241791363763511, 0.6851341621236431, 0.6562287447943391, 0.6923949903085215, 0.6884578517283265, 0.7055068552278692], "time": [33.58422803878784, 31.96433424949646, 30.072391271591187, 33.52184700965881, 31.289138317108154, 31.287107944488525, 30.35893201828003, 30.69302487373352, 30.3047091960907, 31.28079104423523, 30.412925004959106, 30.444951057434082]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.7290167865707434, 0.6786570743405276, 0.6642685851318945, 0.6786570743405276, 0.7050359712230215, 0.6954436450839329, 0.7218225419664268, 0.6810551558752997, 0.6634615384615384, 0.6730769230769231, 0.6826923076923077, 0.7163461538461539], "precision": [0.7285232294694485, 0.6821902529868799, 0.6567818441350334, 0.6796101181418762, 0.7061739720424223, 0.6882574646461898, 0.7090993747267534, 0.6843201495141915, 0.6604609960609267, 0.6710625835502709, 0.6748927515652172, 0.6979793756967669], "recall": [0.7290167865707434, 0.6786570743405276, 0.6642685851318945, 0.6786570743405276, 0.7050359712230215, 0.6954436450839329, 0.7218225419664268, 0.6810551558752997, 0.6634615384615384, 0.6730769230769231, 0.6826923076923077, 0.7163461538461539], "f1": [0.7241287376484878, 0.677105273342593, 0.658972196592253, 0.6764481219556931, 0.7001243386514118, 0.6916457004049527, 0.7152766903880103, 0.6771434800211779, 0.6585747025197156, 0.671966209765412, 0.6778964791006492, 0.7065552074330164], "time": [1.916222095489502, 1.7660751342773438, 1.8930697441101074, 1.8428199291229248, 2.7819602489471436, 2.224919080734253, 6.871908187866211, 1.8067810535430908, 2.0636610984802246, 1.923267126083374, 1.8609819412231445, 2.0118143558502197]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a147d7a70>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6211031175059952, 0.6330935251798561, 0.565947242206235, 0.6258992805755396, 0.6666666666666666, 0.6402877697841727, 0.5875299760191847, 0.6115107913669064, 0.6490384615384616, 0.6418269230769231, 0.5841346153846154, 0.6057692307692307], "precision": [0.6668576944987396, 0.7168737271917885, 0.580081981915354, 0.7008433730441698, 0.7107797933941378, 0.7113869344085172, 0.6517197018666139, 0.6652314050031478, 0.7003491388500627, 0.6882681463273569, 0.6510522098515519, 0.644954300577797], "recall": [0.6211031175059952, 0.6330935251798561, 0.565947242206235, 0.6258992805755396, 0.6666666666666666, 0.6402877697841727, 0.5875299760191847, 0.6115107913669064, 0.6490384615384616, 0.6418269230769231, 0.5841346153846154, 0.6057692307692307], "f1": [0.5391721723442754, 0.5539993224012598, 0.43770568958955536, 0.5501876011152801, 0.5984500759807143, 0.565274665305411, 0.5012360767756451, 0.5257094196414255, 0.5693798305473432, 0.565119569336211, 0.4950492254596731, 0.5101788007238759], "time": [1.9398720264434814, 1.8808891773223877, 1.7731599807739258, 1.7410619258880615, 1.8549299240112305, 1.9412989616394043, 1.954322099685669, 1.791576862335205, 1.7577452659606934, 1.706744909286499, 1.7902650833129883, 1.6640503406524658]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a147d7a70>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.13189448441247004, 0.14148681055155876, 0.11510791366906475, 0.1079136690647482, 0.11510791366906475, 0.13908872901678657, 0.12709832134292565, 0.1486810551558753, 0.15144230769230768, 0.1466346153846154, 0.16346153846153846, 0.10817307692307693], "precision": [0.6140761340507426, 0.5905280642890858, 0.6426923008706664, 0.6082082905104488, 0.5364930012429688, 0.6238715415873689, 0.5604157317085482, 0.6480584078731081, 0.5973469207173194, 0.652330880644147, 0.6154923363524649, 0.5536378066237773], "recall": [0.13189448441247004, 0.14148681055155876, 0.11510791366906475, 0.1079136690647482, 0.11510791366906475, 0.13908872901678657, 0.12709832134292565, 0.1486810551558753, 0.15144230769230768, 0.1466346153846154, 0.16346153846153846, 0.10817307692307693], "f1": [0.19766823627522012, 0.20662382737543109, 0.16063527213942885, 0.16931218927861785, 0.17332617875555067, 0.20500225067489986, 0.1875528350112261, 0.21358476939275128, 0.21739092070389476, 0.22328519281644282, 0.24921467190229243, 0.16405530149462189], "time": [26.59532070159912, 32.31096005439758, 33.331154108047485, 32.143985986709595, 29.95290207862854, 30.5646390914917, 30.182456016540527, 29.402701139450073, 30.490309953689575, 32.806918144226074, 32.37477707862854, 31.601320028305054]}
