{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.5705882352941176, 0.6058823529411764, 0.6352941176470588, 0.5764705882352941, 0.5764705882352941, 0.5941176470588235, 0.5588235294117647, 0.5117647058823529, 0.5470588235294118, 0.6352941176470588, 0.5976331360946746, 0.6272189349112426], "precision": [0.5236532507739937, 0.525109902884839, 0.5759640522875817, 0.5373397007227223, 0.5305453622350403, 0.5337222371786243, 0.5048536999939295, 0.46289286049347966, 0.5433442826846109, 0.637122791424262, 0.5244820205599557, 0.5562099894518819], "recall": [0.5705882352941176, 0.6058823529411764, 0.6352941176470588, 0.5764705882352941, 0.5764705882352941, 0.5941176470588235, 0.5588235294117647, 0.5117647058823529, 0.5470588235294118, 0.6352941176470588, 0.5976331360946746, 0.6272189349112426], "f1": [0.5017219079319937, 0.5354211504694865, 0.5929390837624275, 0.5155096586176481, 0.5228527169703641, 0.5452125051947373, 0.5043494536408923, 0.44967819274667387, 0.47834642541207184, 0.5854448134586742, 0.5473397523634017, 0.5734097985350123], "time": [1.4738306999206543, 1.351024866104126, 1.4465851783752441, 1.4169390201568604, 1.5393338203430176, 1.5394501686096191, 1.7987630367279053, 1.5439627170562744, 1.4712178707122803, 1.6803410053253174, 1.3368198871612549, 1.4257490634918213]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6411764705882353, 0.6, 0.6058823529411764, 0.6, 0.6, 0.6411764705882353, 0.6470588235294118, 0.5294117647058824, 0.5823529411764706, 0.611764705882353, 0.5976331360946746, 0.6094674556213018], "precision": [0.6044929559385129, 0.6000726643598615, 0.5520188632414991, 0.611940895592394, 0.5933370681605975, 0.6166930763184031, 0.5787386851615219, 0.511437908496732, 0.5003456776682117, 0.5656521189120809, 0.5319320015739321, 0.5490181665668583], "recall": [0.6411764705882353, 0.6, 0.6058823529411764, 0.6, 0.6, 0.6411764705882353, 0.6470588235294118, 0.5294117647058824, 0.5823529411764706, 0.611764705882353, 0.5976331360946746, 0.6094674556213018], "f1": [0.6186142117669431, 0.5696594427244582, 0.5651344045579622, 0.5665354570883614, 0.5596723148795455, 0.6261781060202549, 0.5992130682703807, 0.48390292187256134, 0.5328770737836963, 0.578496655222998, 0.5523531059511207, 0.5652707767529531], "time": [3.5608811378479004, 3.4774069786071777, 3.406696081161499, 3.3907551765441895, 3.4479429721832275, 3.3752200603485107, 3.6003918647766113, 3.537954092025757, 3.6823949813842773, 3.6698012351989746, 3.442999839782715, 3.459425926208496]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.4823529411764706, 0.4647058823529412, 0.5235294117647059, 0.5, 0.5235294117647059, 0.4647058823529412, 0.4588235294117647, 0.4411764705882353, 0.5647058823529412, 0.48823529411764705, 0.47337278106508873, 0.5207100591715976], "precision": [0.5332130288012641, 0.4430874751846619, 0.5992512059903522, 0.497345210113012, 0.5108764013004778, 0.4203811874400109, 0.5236469412940001, 0.41694522253345784, 0.5982085561497326, 0.4747623862487361, 0.505814985191643, 0.4978037855698462], "recall": [0.4823529411764706, 0.4647058823529412, 0.5235294117647059, 0.5, 0.5235294117647059, 0.4647058823529412, 0.4588235294117647, 0.4411764705882353, 0.5647058823529412, 0.48823529411764705, 0.47337278106508873, 0.5207100591715976], "f1": [0.48221666511648825, 0.44568602743726043, 0.5034142666654056, 0.486076618288103, 0.5083718882422854, 0.4389917101975926, 0.4724200550479266, 0.4231801811633744, 0.5640349173050933, 0.472003162240506, 0.4725920753955453, 0.4976985405835327], "time": [0.8586008548736572, 0.7080719470977783, 0.7501287460327148, 0.7881650924682617, 0.804689884185791, 0.7390668392181396, 0.7320010662078857, 0.729935884475708, 0.7926528453826904, 0.7741289138793945, 0.7515180110931396, 0.8012077808380127]}
