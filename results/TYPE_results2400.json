{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200]], "accuracy": [0.715, 0.705, 0.685, 0.675, 0.62, 0.645, 0.68, 0.69, 0.64, 0.675, 0.625, 0.71], "precision": [0.7073813620071684, 0.6944653271257044, 0.6634637855142057, 0.6402478111319574, 0.5815194229369334, 0.6256457800511509, 0.6649250813097765, 0.6619447018844609, 0.613127919853272, 0.6468606889564336, 0.5993718361364221, 0.6932266300078556], "recall": [0.715, 0.705, 0.685, 0.675, 0.62, 0.645, 0.68, 0.69, 0.64, 0.675, 0.625, 0.71], "f1_valid": [0.7110268167775334, 0.6946577910891718, 0.6725128088096927, 0.6565271765271765, 0.5977918331511709, 0.6312505630435536, 0.6697155892723206, 0.6745008040739746, 0.62502471492194, 0.659571187702068, 0.6101277472527472, 0.7011872146118722], "f1_train": [0.8990694686790764, 0.9032075835541695, 0.9037420953419295, 0.9050231637245887, 0.9072101357146142, 0.9065845821082693, 0.9003479136249226, 0.9034907208801096, 0.9040985326374729, 0.9028051538810742, 0.9054547962933457, 0.9006049539321953], "time": [2.339730739593506, 2.4401204586029053, 2.46990704536438, 2.4541265964508057, 2.5117645263671875, 2.4918320178985596, 2.448782205581665, 2.434354782104492, 2.933297634124756, 2.3786587715148926, 2.5137932300567627, 2.584303617477417]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200]], "accuracy": [0.755, 0.7, 0.61, 0.695, 0.63, 0.63, 0.645, 0.66, 0.705, 0.68, 0.655, 0.62], "precision": [0.7436548754546078, 0.6489332225370532, 0.5945452256738045, 0.6735480769230768, 0.6010493128875481, 0.6129490968801313, 0.6313425028770836, 0.6428123614190687, 0.6778205128205128, 0.6662904571237905, 0.6378111319574734, 0.5875093843843844], "recall": [0.755, 0.7, 0.61, 0.695, 0.63, 0.63, 0.645, 0.66, 0.705, 0.68, 0.655, 0.62], "f1_valid": [0.7492840021008819, 0.671175972639495, 0.6010326744655103, 0.6789710777928193, 0.614882634824238, 0.6204358637464302, 0.6343780394465706, 0.6476322574908865, 0.6891699367778504, 0.6715723136082034, 0.6436918703538289, 0.6023001437194596], "f1_train": [0.9619284161013082, 0.9601453105666489, 0.9574048349674058, 0.9663701299790085, 0.9595517365776953, 0.963702160678222, 0.95972115180374, 0.9653250851982523, 0.962187990451197, 0.9635519194343765, 0.9635068882348694, 0.963993190724223], "time": [6.072231769561768, 6.589057683944702, 6.747991561889648, 6.058342218399048, 6.634321689605713, 6.990689516067505, 6.4943928718566895, 6.545597076416016, 6.465216875076294, 6.5088512897491455, 6.42389440536499, 5.994313955307007]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200], [2200, 200]], "accuracy": [0.6, 0.545, 0.535, 0.48, 0.495, 0.525, 0.51, 0.59, 0.58, 0.55, 0.565, 0.565], "precision": [0.6077463768115942, 0.6011672240802676, 0.5444456244689889, 0.4671705457789311, 0.5059230769230769, 0.5188780463641767, 0.5008296308954204, 0.6200005012028869, 0.6027261445118588, 0.5668348968105066, 0.5885621189125863, 0.565481181594818], "recall": [0.6, 0.545, 0.535, 0.48, 0.495, 0.525, 0.51, 0.59, 0.58, 0.55, 0.565, 0.565], "f1_valid": [0.5820856257678543, 0.5267901265057503, 0.518439511013724, 0.45333544792586056, 0.46418817204301077, 0.5063573522092573, 0.4880615254951538, 0.5568295383236217, 0.554776797424411, 0.5301964285714286, 0.5327779136347369, 0.5434424957610048], "f1_train": [0.6298885540860768, 0.6265141157885025, 0.6367534542741741, 0.6378845445753825, 0.6274859053627624, 0.6370440285999877, 0.6354775187631344, 0.6219386598900152, 0.6286870404446872, 0.6336079781946009, 0.6313938574625848, 0.6240497433667719], "time": [1.5003154277801514, 1.3693294525146484, 1.489837408065796, 1.5886363983154297, 1.395242691040039, 1.4873366355895996, 1.5876679420471191, 1.3242430686950684, 1.4661810398101807, 1.5961966514587402, 1.4641690254211426, 1.3769209384918213]}
