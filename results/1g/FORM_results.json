{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7630057803468208, 0.7341040462427746, 0.6763005780346821, 0.7456647398843931, 0.7225433526011561, 0.6242774566473989, 0.6878612716763006, 0.7225433526011561, 0.7745664739884393, 0.7167630057803468, 0.6994219653179191, 0.6936416184971098], "precision": [0.7506043089858119, 0.7062417430701299, 0.6644729832551822, 0.695921238626786, 0.6569234343185887, 0.5531188868787598, 0.6259473346178549, 0.6555472208739649, 0.7037721587619582, 0.7053253730503635, 0.6582361940609017, 0.626068060324221], "recall": [0.7630057803468208, 0.7341040462427746, 0.6763005780346821, 0.7456647398843931, 0.7225433526011561, 0.6242774566473989, 0.6878612716763006, 0.7225433526011561, 0.7745664739884393, 0.7167630057803468, 0.6994219653179191, 0.6936416184971098], "f1": [0.7250169500077259, 0.6926852841766363, 0.6317211690044061, 0.6965809874445779, 0.6800371593724195, 0.5586969870206865, 0.6356880114105548, 0.6748185302846181, 0.7313124158894829, 0.6736176507824193, 0.6566837786490966, 0.6429839993298148], "time": [0.8429520130157471, 0.6611430644989014, 0.6226167678833008, 0.7267367839813232, 0.7169349193572998, 0.6717040538787842, 0.6768448352813721, 0.6653878688812256, 0.924821138381958, 0.6977741718292236, 0.6536471843719482, 0.6651089191436768]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7225433526011561, 0.7109826589595376, 0.6820809248554913, 0.7456647398843931, 0.6936416184971098, 0.7514450867052023, 0.7283236994219653, 0.7225433526011561, 0.7745664739884393, 0.7109826589595376, 0.7341040462427746, 0.7745664739884393], "precision": [0.6809498178527689, 0.6732824581898362, 0.6141430823511749, 0.6900522095841879, 0.6553678005813915, 0.6961048344718864, 0.6703878726462408, 0.7230013673579235, 0.7297579208135947, 0.6448963767094318, 0.6726090383604835, 0.7433332278350506], "recall": [0.7225433526011561, 0.7109826589595376, 0.6820809248554913, 0.7456647398843931, 0.6936416184971098, 0.7514450867052023, 0.7283236994219653, 0.7225433526011561, 0.7745664739884393, 0.7109826589595376, 0.7341040462427746, 0.7745664739884393], "f1": [0.6803196372793903, 0.659347134267969, 0.6188804219499855, 0.7072575465639049, 0.6492436042438781, 0.7218586283221069, 0.6801788890127608, 0.6790290753460225, 0.7294040243240815, 0.6685981183105018, 0.6911076368469619, 0.7392201111815628], "time": [1.5702409744262695, 1.5569977760314941, 1.5711619853973389, 1.5802350044250488, 1.5469422340393066, 1.5694940090179443, 1.6431999206542969, 1.5558967590332031, 1.537712812423706, 1.540464162826538, 1.5451109409332275, 1.5665638446807861]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.6358381502890174, 0.6011560693641619, 0.6416184971098265, 0.5549132947976878, 0.5606936416184971, 0.5780346820809249, 0.6473988439306358, 0.5953757225433526, 0.653179190751445, 0.6184971098265896, 0.6184971098265896, 0.6242774566473989], "precision": [0.5792583442103301, 0.5491080096546933, 0.6225049933201942, 0.6020046377201237, 0.4609148498857794, 0.5173059589924601, 0.5887231649978195, 0.5003909903210176, 0.601878612716763, 0.5883209685494516, 0.6101124309216794, 0.5408555239177635], "recall": [0.6358381502890174, 0.6011560693641619, 0.6416184971098265, 0.5549132947976878, 0.5606936416184971, 0.5780346820809249, 0.6473988439306358, 0.5953757225433526, 0.653179190751445, 0.6184971098265896, 0.6184971098265896, 0.6242774566473989], "f1": [0.605184795936241, 0.5600737335578717, 0.61958581813401, 0.5072396517315189, 0.49983606159226696, 0.538428190306803, 0.6156765596411975, 0.5349538451823301, 0.6211972786054786, 0.5784225945480256, 0.5703006049826859, 0.5724996633263771], "time": [0.4912130832672119, 0.4903419017791748, 0.48709726333618164, 0.48909592628479004, 0.48972582817077637, 0.4853179454803467, 0.4857358932495117, 0.4896090030670166, 0.4895763397216797, 0.48847222328186035, 0.49274182319641113, 0.5014979839324951]}
