{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6856561546286877, 0.7100712105798576, 0.6948118006103764, 0.7039674465920651, 0.6673448626653102], "precision": [0.6828862593921223, 0.7180337202604391, 0.6974630309536852, 0.7019741878000357, 0.6618686234239153], "recall": [0.6856561546286877, 0.7100712105798576, 0.6948118006103764, 0.703967446592065, 0.6673448626653103], "f1": [0.6772218428034117, 0.6988833151624497, 0.6828729964419702, 0.6968777066851591, 0.6629178980445583], "time": [12.981695890426636, 12.466195821762085, 13.11498212814331, 13.478094816207886, 12.6380033493042]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6836215666327569, 0.6836215666327569, 0.6866734486266531, 0.6968463886063072, 0.6948118006103764], "precision": [0.6820855523017406, 0.679412845311875, 0.6851930629581818, 0.7013693993919785, 0.6907133020270242], "recall": [0.6836215666327569, 0.6836215666327569, 0.6866734486266531, 0.6968463886063072, 0.6948118006103764], "f1": [0.6735513278656101, 0.6764239078824114, 0.6774445357817583, 0.6849374582817838, 0.6900885829095412], "time": [0.8592002391815186, 0.9373276233673096, 1.0622198581695557, 1.0562293529510498, 0.9372899532318115]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6571719226856562, 0.6856561546286877, 0.688708036622584, 0.698880976602238, 0.6856561546286877], "precision": [0.6577966201086453, 0.6824248042018358, 0.686158397048015, 0.6960490033143345, 0.6905726386040903], "recall": [0.6571719226856562, 0.6856561546286877, 0.688708036622584, 0.698880976602238, 0.6856561546286877], "f1": [0.6574673242277362, 0.6818419003516988, 0.6844297742024654, 0.6956124968453479, 0.687147578489195], "time": [13.029184341430664, 13.338769912719727, 13.28816556930542, 13.114477157592773, 13.031219720840454]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6876907426246185, 0.6866734486266531, 0.6703967446592065, 0.6805696846388606, 0.6866734486266531], "precision": [0.687182403034867, 0.6924006927761707, 0.6751337503664964, 0.6860056624160993, 0.7038941456336166], "recall": [0.6876907426246185, 0.6866734486266531, 0.6703967446592065, 0.6805696846388606, 0.6866734486266531], "f1": [0.68737877770588, 0.6882897478849448, 0.6719679716470317, 0.6821666215014663, 0.6892892965459542], "time": [0.952911376953125, 0.9032630920410156, 0.9518711566925049, 1.0228774547576904, 0.8826818466186523]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6795523906408952, 0.6968463886063072, 0.6876907426246185, 0.6927772126144456, 0.7019328585961343], "precision": [0.6796485759770436, 0.6998289476600016, 0.6919668319919905, 0.6909719428160411, 0.6998587506526822], "recall": [0.6795523906408952, 0.6968463886063072, 0.6876907426246185, 0.6927772126144456, 0.7019328585961343], "f1": [0.6795997796623271, 0.6980498127940848, 0.6890570769362986, 0.6909331711609142, 0.6997719018342445], "time": [3.2431161403656006, 3.263789176940918, 3.2296009063720703, 3.5460612773895264, 3.228053331375122]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.602238046795524, 0.6195320447609359, 0.6225839267548321, 0.6459816887080366, 0.6073245167853509], "precision": [0.6583120771958125, 0.6937645496838525, 0.6847074217718009, 0.674905763095385, 0.6898062944439712], "recall": [0.602238046795524, 0.6195320447609359, 0.6225839267548321, 0.6459816887080366, 0.6073245167853509], "f1": [0.5006331834722824, 0.5286646093011305, 0.5327466676481013, 0.5669231336889944, 0.5240224768079735], "time": [0.7811040878295898, 0.9529376029968262, 0.9934966564178467, 1.2977373600006104, 0.8435871601104736]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4333672431332655, 0.44455747711088506, 0.44659206510681587, 0.4323499491353001, 0.4404883011190234], "precision": [0.6984760753299739, 0.6836243310186209, 0.6719643070305262, 0.7585587322274757, 0.6869683471995774], "recall": [0.4333672431332655, 0.44455747711088506, 0.44659206510681587, 0.4323499491353001, 0.4404883011190234], "f1": [0.2910580444218589, 0.2973103978496588, 0.2962898517034026, 0.27466139253187016, 0.29417749965311796], "time": [12.872007846832275, 12.794587135314941, 12.789757251739502, 14.101938247680664, 13.430320024490356]}
