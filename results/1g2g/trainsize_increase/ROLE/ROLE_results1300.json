{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130]], "accuracy": [0.6538461538461539, 0.6384615384615384, 0.6461538461538462, 0.6076923076923076, 0.5230769230769231, 0.6230769230769231, 0.6, 0.6307692307692307, 0.5615384615384615, 0.6230769230769231], "precision": [0.6120054945054945, 0.6339811912225706, 0.6211463046757164, 0.48601237842617157, 0.4622164250315511, 0.6127908528718246, 0.5498036467977911, 0.6452812551802894, 0.48367276166456497, 0.5945970695970695], "recall": [0.6538461538461539, 0.6384615384615384, 0.6461538461538462, 0.6076923076923076, 0.5230769230769231, 0.6230769230769231, 0.6, 0.6307692307692307, 0.5615384615384615, 0.6230769230769231], "f1_valid": [0.5943762795689773, 0.5952793709594973, 0.5907942464040025, 0.5274625374625375, 0.45307094661897046, 0.5591033411033411, 0.5405690912998509, 0.5714338518821708, 0.503246411565484, 0.5704631068338595], "f1_train": [0.8552850497508263, 0.8551585928731708, 0.8563158076250046, 0.8559789888484336, 0.8594787004099926, 0.8530004320414502, 0.8523529506245998, 0.8486818030157031, 0.8601578512399095, 0.846544223569669], "time": [1.6871018409729004, 1.4059531688690186, 1.4684054851531982, 1.546480417251587, 1.7808349132537842, 1.6871061325073242, 1.7495899200439453, 1.6402416229248047, 1.608966588973999, 1.6558938026428223]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130]], "accuracy": [0.6846153846153846, 0.5230769230769231, 0.6307692307692307, 0.6461538461538462, 0.6461538461538462, 0.6692307692307692, 0.6538461538461539, 0.6, 0.6153846153846154, 0.5923076923076923], "precision": [0.6250493096646942, 0.535350377730639, 0.6046757164404223, 0.5603579881656806, 0.5911298425812214, 0.6217459142073448, 0.5852053776976377, 0.5151893544576471, 0.5615384615384615, 0.6131775671993064], "recall": [0.6846153846153846, 0.5230769230769231, 0.6307692307692307, 0.6461538461538462, 0.6461538461538462, 0.6692307692307692, 0.6538461538461539, 0.6, 0.6153846153846154, 0.5923076923076923], "f1_valid": [0.651513102282333, 0.46787042129449163, 0.5962820133739709, 0.5915277911325967, 0.6112379255236398, 0.6307172542741784, 0.6062511562511562, 0.551793339844124, 0.580182267531665, 0.5466772215824469], "f1_train": [0.968901177750853, 0.9695502760349808, 0.9702480746970734, 0.9729283750501977, 0.9700065748424623, 0.9726748096556993, 0.9786383282519839, 0.9741042540094546, 0.9734032357328232, 0.9737978888774506], "time": [2.1557466983795166, 2.202605724334717, 2.140141248703003, 2.108877182006836, 2.1401689052581787, 2.1401560306549072, 2.1088802814483643, 2.249462842941284, 2.1244983673095703, 2.1401193141937256]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130], [1170, 130]], "accuracy": [0.4461538461538462, 0.5076923076923077, 0.5538461538461539, 0.4846153846153846, 0.5230769230769231, 0.47692307692307695, 0.5, 0.46923076923076923, 0.49230769230769234, 0.5769230769230769], "precision": [0.43733508339959953, 0.5202508916765741, 0.5879197981137636, 0.5028849552709574, 0.533701015965167, 0.49203902993558163, 0.5111545591968285, 0.42363248862196967, 0.5153132013874719, 0.5905091044221479], "recall": [0.4461538461538462, 0.5076923076923077, 0.5538461538461539, 0.4846153846153846, 0.5230769230769231, 0.47692307692307695, 0.5, 0.46923076923076923, 0.49230769230769234, 0.5769230769230769], "f1_valid": [0.43677177105660075, 0.5082470085878039, 0.5588620581669241, 0.47815458510493974, 0.5038987608136545, 0.4685924782641723, 0.49390410812429153, 0.43768816836209073, 0.49998251374323244, 0.5751313152256549], "f1_train": [0.6218524950475502, 0.629889562362893, 0.6369879579924324, 0.6249225263819562, 0.5974820447059445, 0.6431379998454714, 0.627747471320091, 0.6539269514171803, 0.6068612875936633, 0.6329692482126942], "time": [0.7029299736022949, 0.6248519420623779, 0.6404719352722168, 0.640472412109375, 0.656092643737793, 0.6248514652252197, 0.6560957431793213, 0.6248481273651123, 0.6717157363891602, 0.6404752731323242]}
