{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.6927835051546392, 0.7237113402061855, 0.7010309278350515, 0.6515463917525773, 0.6927835051546392, 0.6948453608247422, 0.7175257731958763, 0.6907216494845361, 0.6859504132231405, 0.7066115702479339, 0.7086776859504132, 0.7086776859504132], "precision": [0.6849214780117522, 0.7165400098183603, 0.6935077454388118, 0.6422976291108159, 0.6818977058129343, 0.6914827523656922, 0.7134695417579512, 0.6828728112832169, 0.6807758082840503, 0.7006598940696362, 0.7019790230954265, 0.7051040650573284], "recall": [0.6927835051546392, 0.7237113402061855, 0.7010309278350515, 0.6515463917525773, 0.6927835051546392, 0.6948453608247422, 0.7175257731958763, 0.6907216494845361, 0.6859504132231405, 0.7066115702479339, 0.7086776859504132, 0.7086776859504132], "f1_valid": [0.6836124299358372, 0.7136193142416026, 0.6888429163121549, 0.6405512637011689, 0.678333386279625, 0.6838739109247938, 0.7035861966833703, 0.6705052584238567, 0.6710197527061822, 0.6947300243167789, 0.7003536043424727, 0.696864381883588], "f1_train": [0.7287576636209339, 0.7280916185060231, 0.728207956403908, 0.7324399732057089, 0.7305526482791266, 0.7314715297346803, 0.7256642738882709, 0.7237930536317485, 0.7281214786165843, 0.7310810742370061, 0.7298704458395502, 0.724709158130341], "time": [9.505178213119507, 7.69523024559021, 8.363577842712402, 8.509999513626099, 8.438330888748169, 9.208256006240845, 8.099933385848999, 8.195134401321411, 8.6499764919281, 8.370097398757935, 7.707581043243408, 7.8951497077941895]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001DE893825E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.6927835051546392, 0.7051546391752578, 0.6948453608247422, 0.7216494845360825, 0.6948453608247422, 0.6721649484536083, 0.6969072164948453, 0.6783505154639176, 0.7066115702479339, 0.6838842975206612, 0.7066115702479339, 0.7107438016528925], "precision": [0.6837537046577942, 0.7153523513628018, 0.6832338578404774, 0.7234977294865342, 0.6957328765632589, 0.6600465105294671, 0.691982326951399, 0.6766142159522517, 0.6972309299895507, 0.673221114028441, 0.7011144330331992, 0.7013511911816998], "recall": [0.6927835051546392, 0.7051546391752578, 0.6948453608247422, 0.7216494845360825, 0.6948453608247422, 0.6721649484536083, 0.6969072164948453, 0.6783505154639176, 0.7066115702479339, 0.6838842975206612, 0.7066115702479339, 0.7107438016528925], "f1_valid": [0.6830193066927556, 0.6861840594495913, 0.6866339581644894, 0.7108059535446117, 0.67693219666931, 0.6539681616946617, 0.6839250010186161, 0.6705079452292477, 0.6967493888225597, 0.6716773550826175, 0.6976145019154225, 0.6990520093416842], "f1_train": [0.7940121270125698, 0.7937774619716573, 0.7973205498580594, 0.7921823799854167, 0.7941796624188862, 0.7912792467156149, 0.7919731495677469, 0.7919986551522253, 0.7937240347113577, 0.7954536087176644, 0.7929830726397533, 0.7914863574162943], "time": [2.7497506141662598, 2.7099709510803223, 2.3146815299987793, 2.3749313354492188, 2.8900842666625977, 2.4097063541412354, 2.379999876022339, 2.3001794815063477, 2.4602456092834473, 2.2533376216888428, 2.755005359649658, 2.8248119354248047]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.7030927835051546, 0.6989690721649484, 0.6969072164948453, 0.6639175257731958, 0.709278350515464, 0.7257731958762886, 0.6845360824742268, 0.6989690721649484, 0.6983471074380165, 0.6942148760330579, 0.6735537190082644, 0.7169421487603306], "precision": [0.6991577386968533, 0.694688931643259, 0.6920103383973154, 0.6862947614411181, 0.7020482029205265, 0.7210790153919191, 0.6812456801300585, 0.6915720007879702, 0.6895426371149512, 0.6943632033435847, 0.6636981081107156, 0.7120361619315352], "recall": [0.7030927835051546, 0.6989690721649484, 0.6969072164948453, 0.6639175257731958, 0.709278350515464, 0.7257731958762886, 0.6845360824742268, 0.6989690721649484, 0.6983471074380165, 0.6942148760330579, 0.6735537190082644, 0.7169421487603306], "f1_valid": [0.6987022192571899, 0.6895080362429464, 0.6931172552033347, 0.6694852924399093, 0.7017514515013591, 0.7178451439530976, 0.6693678938834926, 0.6942363897968349, 0.6858612312679662, 0.681849906612063, 0.6603334073383877, 0.7133451281678421], "f1_train": [0.7585937770875999, 0.7496647952180797, 0.7573235578533573, 0.7632767305566382, 0.7501781245811541, 0.7433105559767317, 0.7500385729875182, 0.761920987876366, 0.7511748017893219, 0.7599964276439385, 0.7447641455659085, 0.7632342030159341], "time": [8.250166654586792, 8.979825735092163, 8.665829420089722, 7.995290994644165, 7.745167970657349, 7.720118522644043, 7.8702311515808105, 7.73266339302063, 8.625635623931885, 8.314906597137451, 8.163125991821289, 7.870356321334839]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.7051546391752578, 0.6783505154639176, 0.668041237113402, 0.6701030927835051, 0.6845360824742268, 0.7134020618556701, 0.6597938144329897, 0.668041237113402, 0.6983471074380165, 0.6859504132231405, 0.6611570247933884, 0.7148760330578512], "precision": [0.7047448771004966, 0.6736412916509322, 0.6677063248815825, 0.6677879637969462, 0.6782705140093457, 0.7087049786829763, 0.6554258937534666, 0.6631345062476878, 0.6959036184884183, 0.6839486021480726, 0.6565832529846027, 0.7117182279719221], "recall": [0.7051546391752578, 0.6783505154639176, 0.668041237113402, 0.6701030927835051, 0.6845360824742268, 0.7134020618556701, 0.6597938144329897, 0.668041237113402, 0.6983471074380165, 0.6859504132231405, 0.6611570247933884, 0.7148760330578512], "f1_valid": [0.7049467331739709, 0.6748217384330479, 0.6678709474618756, 0.6687624824869803, 0.6798251546391753, 0.7075210127127577, 0.6544808694351844, 0.6645490118245493, 0.6968232781474529, 0.6848433867000232, 0.6581419568375722, 0.7129849019877605], "f1_train": [0.909984840743879, 0.9103970391286064, 0.9151932368525686, 0.9104778473296066, 0.9093235080654936, 0.909230172112763, 0.9092287835259977, 0.9092141155811629, 0.9114027390401257, 0.9105473803192743, 0.909904197137834, 0.911864197904481], "time": [2.1054582595825195, 2.2792723178863525, 2.09753680229187, 2.1321377754211426, 2.060232162475586, 2.3209967613220215, 2.232544422149658, 2.4303221702575684, 3.046424627304077, 3.5846359729766846, 2.3327510356903076, 2.3411147594451904]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001DE893825E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.6597938144329897, 0.7051546391752578, 0.6824742268041237, 0.7257731958762886, 0.6845360824742268, 0.6577319587628866, 0.7072164948453609, 0.7175257731958763, 0.7107438016528925, 0.6818181818181818, 0.7107438016528925, 0.71900826446281], "precision": [0.6573948885508797, 0.6996754559873116, 0.6766272980927264, 0.722219201491378, 0.677510213985192, 0.6528825949069972, 0.6987267709667053, 0.7114421274601687, 0.7084348332909898, 0.6733281086729362, 0.7019261873404692, 0.7148484448655975], "recall": [0.6597938144329897, 0.7051546391752578, 0.6824742268041237, 0.7257731958762886, 0.6845360824742268, 0.6577319587628866, 0.7072164948453609, 0.7175257731958763, 0.7107438016528925, 0.6818181818181818, 0.7107438016528925, 0.71900826446281], "f1_valid": [0.655075489790013, 0.6997890900189715, 0.6764776927862972, 0.7195154393861988, 0.6791532173354572, 0.6549055748555842, 0.7014581184548628, 0.7130359203625144, 0.7054834594443719, 0.6691309227541111, 0.703451343556665, 0.7131922829236879], "f1_train": [0.8615167433378905, 0.8659634024880628, 0.8599414531193087, 0.8551508677422477, 0.8605500517725495, 0.8588295227983879, 0.861515872852755, 0.8605239338013192, 0.8572905311301079, 0.8582016235157568, 0.8598985523604703, 0.8598881145396902], "time": [13.87399172782898, 13.089954137802124, 13.769886255264282, 13.421333074569702, 13.792786121368408, 13.478953838348389, 14.128182411193848, 12.90666651725769, 13.288979053497314, 13.438765287399292, 14.62053108215332, 14.490142107009888]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001DE893825E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.6577319587628866, 0.5917525773195876, 0.6103092783505155, 0.6329896907216495, 0.6350515463917525, 0.6123711340206186, 0.6268041237113402, 0.6412371134020619, 0.6384297520661157, 0.6487603305785123, 0.6508264462809917, 0.6466942148760331], "precision": [0.6964776800986192, 0.6011653382437936, 0.6314585802565765, 0.5982159830372157, 0.6662239293975094, 0.6984848819326402, 0.6211922755223932, 0.7136530987217612, 0.7021136368659742, 0.714521147301896, 0.7766480466616152, 0.6633675328127155], "recall": [0.6577319587628866, 0.5917525773195876, 0.6103092783505155, 0.6329896907216495, 0.6350515463917525, 0.6123711340206186, 0.6268041237113402, 0.6412371134020619, 0.6384297520661157, 0.6487603305785123, 0.6508264462809917, 0.6466942148760331], "f1_valid": [0.5589187307537823, 0.46318986598396944, 0.5027876075908585, 0.5166436390689766, 0.5246676537113792, 0.497469970680034, 0.5139256797839528, 0.5343905850343843, 0.5420476103312662, 0.5521445917288227, 0.540927509367059, 0.5463252955784764], "f1_train": [0.6946219892482837, 0.6765041192176271, 0.6797395230771796, 0.6861246550713493, 0.6849410812622366, 0.6809822862761478, 0.6848542175612164, 0.6927581182967576, 0.6797503150537544, 0.6844959322045238, 0.6841731762058274, 0.6958284352311718], "time": [2.5100297927856445, 2.387554407119751, 2.5399389266967773, 2.444997787475586, 2.510047435760498, 2.6300675868988037, 2.344884157180786, 2.4096755981445312, 2.860177755355835, 2.429703950881958, 2.350163459777832, 2.4401984214782715]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001DE893825E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [5331, 5331, 5331, 5331, 5331, 5331, 5331, 5331, 5332, 5332, 5332, 5332], "accuracy": [0.5814432989690722, 0.6144329896907217, 0.6123711340206186, 0.6288659793814433, 0.6, 0.6206185567010309, 0.5731958762886598, 0.5752577319587628, 0.6301652892561983, 0.6570247933884298, 0.640495867768595, 0.5971074380165289], "precision": [0.5853792204805887, 0.6132924615542693, 0.6123711340206186, 0.6387855599248946, 0.6088165619528272, 0.6248471568662667, 0.5786049021280402, 0.5918573494138784, 0.6492496737712048, 0.6570247933884298, 0.6440840799129569, 0.6205391421214018], "recall": [0.5814432989690722, 0.6144329896907217, 0.6123711340206186, 0.6288659793814433, 0.6, 0.6206185567010309, 0.5731958762886598, 0.5752577319587628, 0.6301652892561983, 0.6570247933884298, 0.640495867768595, 0.5971074380165289], "f1_valid": [0.5829988202797919, 0.6138396201596265, 0.6123711340206186, 0.6327861260314118, 0.6029611093733709, 0.6223583493127087, 0.5754996747911281, 0.5800900451478255, 0.635700891975162, 0.6570247933884298, 0.641795232176414, 0.6042489156983912], "f1_train": [0.6261364724676284, 0.6334279466263613, 0.6269086504356953, 0.6365815670232637, 0.6322965437059508, 0.6319151953298577, 0.6357715952383097, 0.6260017783348205, 0.6299163041021104, 0.6245050615770434, 0.6284926836673885, 0.6252736021948476], "time": [7.580122709274292, 7.670047998428345, 7.44046950340271, 7.500008821487427, 7.355135679244995, 7.439867734909058, 7.750176906585693, 7.754817485809326, 7.49017333984375, 7.535158634185791, 7.8100745677948, 7.183191299438477]}
