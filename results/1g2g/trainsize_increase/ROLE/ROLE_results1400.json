{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140]], "accuracy": [0.6071428571428571, 0.5428571428571428, 0.6071428571428571, 0.5714285714285714, 0.5071428571428571, 0.6071428571428571, 0.7285714285714285, 0.5642857142857143, 0.6214285714285714, 0.5714285714285714], "precision": [0.5443154761904762, 0.5203469884606308, 0.5399970050913446, 0.5167427161239958, 0.4503662418767805, 0.6014578969417679, 0.7385944700460829, 0.4774453511448121, 0.5659137527255108, 0.5217363195566843], "recall": [0.6071428571428571, 0.5428571428571428, 0.6071428571428571, 0.5714285714285714, 0.5071428571428571, 0.6071428571428571, 0.7285714285714285, 0.5642857142857143, 0.6214285714285714, 0.5714285714285714], "f1_valid": [0.5288666961934289, 0.4868599176157076, 0.548417435055366, 0.49271096504741363, 0.44934769700666716, 0.5519267222586842, 0.6643912737508797, 0.4883772959334128, 0.576646392721159, 0.48369823930896416], "f1_train": [0.8532574424831866, 0.8404764595593138, 0.8484895761966376, 0.8497532901665533, 0.8477960502168781, 0.8425035074341768, 0.8438255444178914, 0.8431042969335659, 0.850840348269966, 0.8550622331028661], "time": [1.843315839767456, 2.015176773071289, 1.827699899673462, 1.6090152263641357, 1.952666997909546, 1.7496209144592285, 1.7183446884155273, 1.8589379787445068, 1.8589410781860352, 1.687107801437378]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140]], "accuracy": [0.6785714285714286, 0.6, 0.5928571428571429, 0.5928571428571429, 0.6142857142857143, 0.5928571428571429, 0.6428571428571429, 0.5428571428571428, 0.7, 0.6357142857142857], "precision": [0.6420499661853045, 0.5354591836734695, 0.5279905808477238, 0.5502269998908655, 0.568342505854801, 0.5141040507111936, 0.5910661662004304, 0.45398064898064894, 0.660060690943044, 0.6046527777777779], "recall": [0.6785714285714286, 0.6, 0.5928571428571429, 0.5928571428571429, 0.6142857142857143, 0.5928571428571429, 0.6428571428571429, 0.5428571428571428, 0.7, 0.6357142857142857], "f1_valid": [0.6543386243386243, 0.5493272600415458, 0.5341441755636551, 0.5584562916075522, 0.5717154653910833, 0.5324080086580086, 0.6119064376483732, 0.47994899387526574, 0.6791109570041609, 0.6026148213573423], "f1_train": [0.9736229300779153, 0.9723796301713536, 0.9695279221490393, 0.9749743273429835, 0.9710710989569771, 0.9711569197602241, 0.9705434412705854, 0.9750147682950681, 0.9740522691747874, 0.9684664709830568], "time": [2.421306848526001, 2.4369301795959473, 2.51503586769104, 2.4369266033172607, 2.4056880474090576, 2.4994189739227295, 2.5931341648101807, 2.4213082790374756, 2.452516794204712, 2.6712472438812256]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140], [1260, 140]], "accuracy": [0.5785714285714286, 0.5, 0.4642857142857143, 0.4357142857142857, 0.5285714285714286, 0.5571428571428572, 0.39285714285714285, 0.45714285714285713, 0.4928571428571429, 0.5142857142857142], "precision": [0.5975123436471362, 0.54511137011137, 0.44702008517798, 0.42286399237618744, 0.5386632767328656, 0.561659826974268, 0.36633091946456003, 0.46718791673906984, 0.5150251129776992, 0.5242567649856652], "recall": [0.5785714285714286, 0.5, 0.4642857142857143, 0.4357142857142857, 0.5285714285714286, 0.5571428571428572, 0.39285714285714285, 0.45714285714285713, 0.4928571428571429, 0.5142857142857142], "f1_valid": [0.5776015409581468, 0.4963445111148205, 0.44998853976700853, 0.42685367374434346, 0.5210525437437237, 0.5494720961948141, 0.36875769175594764, 0.4545827756203696, 0.4846088612710988, 0.5137258104277086], "f1_train": [0.5978669078909944, 0.6361833551707109, 0.6087389473866924, 0.6372012415409786, 0.6108912010810925, 0.5980408056471541, 0.6196926739443102, 0.6106090107308959, 0.5882622725509653, 0.6009427599860341], "time": [0.6873323917388916, 0.6717169284820557, 0.6873438358306885, 0.687305212020874, 0.6873393058776855, 0.7186121940612793, 0.7029600143432617, 0.702958345413208, 0.6717169284820557, 0.6717185974121094]}
