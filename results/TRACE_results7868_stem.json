{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wor...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6509146341463414, 0.6615853658536586, 0.6814024390243902, 0.6692073170731707, 0.6798780487804879, 0.6692073170731707, 0.6935975609756098, 0.649390243902439, 0.7038167938931298, 0.6473282442748092, 0.6732824427480916, 0.7053435114503817], "precision": [0.6403178967470884, 0.6547621676739352, 0.6716801785465625, 0.6584160745712241, 0.678394131301633, 0.6610805489060382, 0.6844415305911533, 0.6422463154365672, 0.699080543841051, 0.6386986445080055, 0.6617586993159512, 0.6975816452968285], "recall": [0.6509146341463414, 0.6615853658536586, 0.6814024390243902, 0.6692073170731707, 0.6798780487804879, 0.6692073170731707, 0.6935975609756098, 0.649390243902439, 0.7038167938931298, 0.6473282442748092, 0.6732824427480916, 0.7053435114503817], "f1_valid": [0.6345090875291723, 0.6380520679777539, 0.662642478296588, 0.6468302841793578, 0.6637078675474254, 0.655796976317515, 0.6821592445371323, 0.6300467615289165, 0.688518178824085, 0.6269169116893674, 0.6554520688129217, 0.692910946549664], "f1_train": [0.6755088393923305, 0.6818606569845548, 0.6717069485155451, 0.67449419695261, 0.6756101216743611, 0.6738140008232605, 0.674367762607127, 0.6773613959458741, 0.672018291719753, 0.6772644781963026, 0.6786178966207036, 0.6738297565605703], "time": [43.83366584777832, 41.15584707260132, 42.18641209602356, 43.94176006317139, 42.25240707397461, 41.75765085220337, 43.47124910354614, 42.10067367553711, 43.82557415962219, 42.946903228759766, 41.52854299545288, 42.3865749835968]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wor...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a228e7f80>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.676829268292683, 0.6432926829268293, 0.6737804878048781, 0.6539634146341463, 0.6783536585365854, 0.6859756097560976, 0.7012195121951219, 0.6326219512195121, 0.6931297709923664, 0.7022900763358778, 0.6564885496183206, 0.7312977099236642], "precision": [0.6692864825927196, 0.6379227300676368, 0.6709832960902709, 0.6445540315658407, 0.678355514971319, 0.6782835716288198, 0.6899143235974309, 0.6266172479351159, 0.6861237228846868, 0.6941460584704534, 0.6468557653676176, 0.7226637655816625], "recall": [0.676829268292683, 0.6432926829268293, 0.6737804878048781, 0.6539634146341463, 0.6783536585365854, 0.6859756097560976, 0.7012195121951219, 0.6326219512195121, 0.6931297709923664, 0.7022900763358778, 0.6564885496183206, 0.7312977099236642], "f1_valid": [0.661564930812058, 0.6257468237495951, 0.6533357245337159, 0.6386345360770975, 0.6603791886627975, 0.6751758291392438, 0.6900897734032773, 0.609796577475604, 0.6752888349900168, 0.6874777201689182, 0.6409510316306004, 0.7163009965288175], "f1_train": [0.8560405234694807, 0.8550923098104062, 0.853235968950324, 0.8594905005921438, 0.8522031953899876, 0.8591624670704426, 0.8595316213357751, 0.8559210875194562, 0.8607903161678447, 0.8588634251033537, 0.8561970484275176, 0.8586988496375836], "time": [6.917999744415283, 8.57801604270935, 6.921601057052612, 7.573520183563232, 6.974333047866821, 7.630686044692993, 7.573739767074585, 7.38326621055603, 7.158337116241455, 7.1780359745025635, 8.530379056930542, 6.566293001174927]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wor...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6829268292682927, 0.6478658536585366, 0.6890243902439024, 0.663109756097561, 0.6692073170731707, 0.649390243902439, 0.663109756097561, 0.6798780487804879, 0.6366412213740458, 0.6412213740458015, 0.6641221374045801, 0.6610687022900763], "precision": [0.6757960130180182, 0.647660221143854, 0.6826371123227132, 0.6428541620682043, 0.6664120893981117, 0.641013289550926, 0.6572494012532811, 0.6721547889606955, 0.6638963371608185, 0.6321235903382969, 0.653475420415875, 0.6587563116321203], "recall": [0.6829268292682927, 0.6478658536585366, 0.6890243902439024, 0.663109756097561, 0.6692073170731707, 0.649390243902439, 0.663109756097561, 0.6798780487804879, 0.6366412213740458, 0.6412213740458015, 0.6641221374045801, 0.6610687022900763], "f1_valid": [0.6613769285834965, 0.6477615382206947, 0.6831779796293945, 0.6356167758822746, 0.6477584672345751, 0.6399529272229769, 0.6557005430862067, 0.6715531790957836, 0.6426888880738943, 0.6082774980729172, 0.655725542603197, 0.6596092068061572], "f1_train": [0.6705102478589194, 0.7232713289240128, 0.7046605881864249, 0.6583095939260736, 0.6717550490825737, 0.712100030369989, 0.7084486844127681, 0.6992795066765071, 0.7256513649850458, 0.6497792411213745, 0.7079164207173037, 0.7226960501919794], "time": [40.52688694000244, 39.30780506134033, 40.127816915512085, 37.697266817092896, 37.58968901634216, 40.81541299819946, 38.859454870224, 39.83446407318115, 39.433045864105225, 39.517730951309204, 38.643919229507446, 40.36972117424011]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wor...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6692073170731707, 0.649390243902439, 0.6448170731707317, 0.635670731707317, 0.6295731707317073, 0.6966463414634146, 0.649390243902439, 0.6585365853658537, 0.6763358778625954, 0.6732824427480916, 0.650381679389313, 0.6946564885496184], "precision": [0.6777704972303461, 0.6479738678528882, 0.6490856303516765, 0.6389215738590872, 0.6322117662121504, 0.6995834127286585, 0.6498505740942238, 0.6644287697232374, 0.6789363943579582, 0.6714849504486408, 0.6561453151486278, 0.693995280249061], "recall": [0.6692073170731707, 0.649390243902439, 0.6448170731707317, 0.635670731707317, 0.6295731707317073, 0.6966463414634146, 0.649390243902439, 0.6585365853658537, 0.6763358778625954, 0.6732824427480916, 0.650381679389313, 0.6946564885496184], "f1_valid": [0.6723713008255218, 0.6486279592053178, 0.6466188478077621, 0.6370493625280494, 0.6307173991295316, 0.6979193022934314, 0.6496144102517089, 0.6606093196221869, 0.6774100877917671, 0.6722260451773829, 0.6527245192042256, 0.6942998477055172], "f1_train": [0.9929234033117944, 0.9922282455371363, 0.9929237898552826, 0.9902846244895136, 0.991952708613616, 0.9918122842493844, 0.9916725352767259, 0.9916714185859773, 0.9913977959653089, 0.9915356923387766, 0.9920914221632066, 0.9918137193370025], "time": [7.002267837524414, 6.806189775466919, 6.960208177566528, 6.798625946044922, 7.203835725784302, 7.112651109695435, 7.3198161125183105, 6.99190092086792, 6.985636234283447, 7.146912097930908, 7.069974899291992, 7.273205995559692]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wor...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a228e7f80>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6692073170731707, 0.6661585365853658, 0.6280487804878049, 0.6432926829268293, 0.649390243902439, 0.6722560975609756, 0.7179878048780488, 0.6646341463414634, 0.667175572519084, 0.6564885496183206, 0.6610687022900763, 0.6900763358778625], "precision": [0.6680594437102005, 0.6612270246497172, 0.6318413595914433, 0.6386046666170806, 0.6457081581160639, 0.6725128859786135, 0.7161849198230021, 0.6628772440130574, 0.676708663623655, 0.6505769572164033, 0.6558923992067093, 0.6864216009314383], "recall": [0.6692073170731707, 0.6661585365853658, 0.6280487804878049, 0.6432926829268293, 0.649390243902439, 0.6722560975609756, 0.7179878048780488, 0.6646341463414634, 0.667175572519084, 0.6564885496183206, 0.6610687022900763, 0.6900763358778625], "f1_valid": [0.6685944900138757, 0.6601728875111945, 0.6297392183749194, 0.639932001696912, 0.646354661598564, 0.6723829337491771, 0.716948914573006, 0.6637003189039155, 0.6705630860961919, 0.6503578377319772, 0.6562105444144365, 0.6877772600219401], "f1_train": [0.9756320510687324, 0.9754910918939105, 0.9752112175926552, 0.9747967412719462, 0.9756288815823058, 0.9752285197961071, 0.9739637285917397, 0.9757863739292988, 0.9756457574769609, 0.9753603436175478, 0.9760537155604767, 0.976059937895881], "time": [29.46613883972168, 29.41656804084778, 28.29786491394043, 28.084097862243652, 27.632320165634155, 27.552888870239258, 27.98707103729248, 29.98386812210083, 28.67763614654541, 27.679879903793335, 27.874915838241577, 27.90834403038025]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a228e7f80>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6173780487804879, 0.5792682926829268, 0.6021341463414634, 0.6204268292682927, 0.6021341463414634, 0.6295731707317073, 0.6707317073170732, 0.6539634146341463, 0.5862595419847328, 0.6442748091603053, 0.6519083969465649, 0.583206106870229], "precision": [0.7263143736313524, 0.7584688346883469, 0.7169246847027976, 0.6748419337963056, 0.611132283197832, 0.7051922372872798, 0.6910454954783557, 0.750204541014035, 0.7603646104973685, 0.7116473130649794, 0.7301126521640445, 0.6315533463518552], "recall": [0.6173780487804879, 0.5792682926829268, 0.6021341463414634, 0.6204268292682927, 0.6021341463414634, 0.6295731707317073, 0.6707317073170732, 0.6539634146341463, 0.5862595419847328, 0.6442748091603053, 0.6519083969465649, 0.583206106870229], "f1_valid": [0.4859634523191582, 0.43735186022287303, 0.4659095520479612, 0.498211389241095, 0.46367630463048154, 0.5034596503676346, 0.5587342172187039, 0.5368536519821971, 0.4500469431538887, 0.5293208019418562, 0.5366071816562132, 0.4437225875247268], "f1_train": [0.850737695387926, 0.835743871104644, 0.8421977958314123, 0.8490231192044158, 0.8413333898595635, 0.8490569014702904, 0.865938783420959, 0.8584834717562999, 0.8383777221794918, 0.8584367085356645, 0.8589646291423563, 0.8381032358368141], "time": [6.907486200332642, 6.877372980117798, 7.0000691413879395, 6.981861114501953, 7.1343700885772705, 6.902994871139526, 6.780400037765503, 6.412646055221558, 6.77936577796936, 7.029481887817383, 6.726834058761597, 6.5363450050354]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a228e7f80>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.600609756097561, 0.6036585365853658, 0.5990853658536586, 0.5762195121951219, 0.5762195121951219, 0.6067073170731707, 0.586890243902439, 0.5975609756097561, 0.6045801526717557, 0.5954198473282443, 0.6, 0.5847328244274809], "precision": [0.6030995162995921, 0.6042183882163702, 0.5948438616185325, 0.577170486565186, 0.5878375092047702, 0.6099180462782657, 0.5851721254957859, 0.603293903785218, 0.6036772204941563, 0.5991968684013693, 0.6022785398744479, 0.5817905313969632], "recall": [0.600609756097561, 0.6036585365853658, 0.5990853658536586, 0.5762195121951219, 0.5762195121951219, 0.6067073170731707, 0.586890243902439, 0.5975609756097561, 0.6045801526717557, 0.5954198473282443, 0.6, 0.5847328244274809], "f1_valid": [0.6017675450288409, 0.6039329604261199, 0.5966576382768931, 0.5766739336740795, 0.5805251284872595, 0.6081752883476501, 0.585921774961378, 0.5998830964613693, 0.6041162895232866, 0.5970789081908837, 0.6010516066212268, 0.583068743374087], "f1_train": [0.6243349673326991, 0.6249137113685239, 0.6239565667043231, 0.6204888543496954, 0.6264505281592467, 0.6234090208692363, 0.6249139690538931, 0.6265172900072395, 0.625943108568993, 0.6229604446208103, 0.6224376335395373, 0.6276226754992237], "time": [39.064172983169556, 40.51990604400635, 39.033880949020386, 41.62608504295349, 38.96626830101013, 38.94906497001648, 40.8374547958374, 40.39813590049744, 41.24863886833191, 40.426714181900024, 40.57503414154053, 37.455005168914795]}
