{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200]], "accuracy": [0.605, 0.58, 0.585, 0.59, 0.61, 0.565, 0.56, 0.52, 0.65, 0.615], "precision": [0.520389548873937, 0.5132892628205128, 0.5389093330726556, 0.565836870174499, 0.5479486853624784, 0.4928711828711828, 0.5295382853685939, 0.45284834197705487, 0.5782454114313871, 0.5629906706330524], "recall": [0.605, 0.58, 0.585, 0.59, 0.61, 0.565, 0.56, 0.52, 0.65, 0.615], "f1_valid": [0.5433292052775578, 0.5290423421141507, 0.5153908556698409, 0.5280285397287686, 0.5540498585414434, 0.5055815440432998, 0.5042387177932026, 0.4472442993647835, 0.6016753452443803, 0.5757909966647413], "f1_train": [0.8353400205944733, 0.8316340216367438, 0.83024161737159, 0.8327822094794497, 0.8278222960288149, 0.8282690983451295, 0.8268310768772512, 0.8315203355910795, 0.8357872211874878, 0.8236112966998075], "time": [2.624417781829834, 2.452562093734741, 2.7961881160736084, 2.4213380813598633, 2.4525516033172607, 2.640001058578491, 2.390096664428711, 2.3432042598724365, 2.296334743499756, 2.3900697231292725]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200]], "accuracy": [0.605, 0.63, 0.54, 0.655, 0.67, 0.555, 0.555, 0.525, 0.605, 0.59], "precision": [0.56481768707483, 0.5942666666666666, 0.4787701067048893, 0.6943988095238095, 0.6305695197868788, 0.5139532520325203, 0.4906192810457516, 0.4735759552656104, 0.5676937947992837, 0.47002280317348805], "recall": [0.605, 0.63, 0.54, 0.655, 0.67, 0.555, 0.555, 0.525, 0.605, 0.59], "f1_valid": [0.5661956636471456, 0.5969332627520463, 0.4901257994903335, 0.6171212425717583, 0.6492080307762896, 0.511703248031496, 0.5186302377862241, 0.4791273847132561, 0.5724428749774703, 0.5153915563839702], "f1_train": [0.9621122064754837, 0.960863759965044, 0.9640408138575544, 0.9606071787293654, 0.966502269281663, 0.9627686272764137, 0.9640471365319551, 0.9648573893544634, 0.9619542633165904, 0.9655672288139222], "time": [4.670748472213745, 4.59264063835144, 4.592674970626831, 4.65515661239624, 4.436456680297852, 4.420830965042114, 4.608262300491333, 4.514570713043213, 4.530198812484741, 4.467668056488037]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200], [1800, 200]], "accuracy": [0.48, 0.42, 0.41, 0.395, 0.475, 0.46, 0.49, 0.435, 0.495, 0.51], "precision": [0.4828744504328698, 0.45479901215805474, 0.40162293956043954, 0.4131944764096663, 0.48916437728937723, 0.48528722250033723, 0.4815450328407225, 0.46565344129554653, 0.5231791889143147, 0.48259301643192487], "recall": [0.48, 0.42, 0.41, 0.395, 0.475, 0.46, 0.49, 0.435, 0.495, 0.51], "f1_valid": [0.4552732202193308, 0.4138838038495376, 0.3966967552689393, 0.38185507246376815, 0.46588172043010745, 0.457049235290189, 0.48116974424726877, 0.43652934874770893, 0.48953986912545894, 0.4940517786122712], "f1_train": [0.5747078394727374, 0.5902550683108023, 0.5728398729688764, 0.5934316662832483, 0.5842076812465818, 0.5780300632505316, 0.5836682955189352, 0.5700173473824417, 0.578325895467424, 0.5743124345037208], "time": [1.077873706817627, 0.9997661113739014, 1.0153872966766357, 1.0934934616088867, 1.0153868198394775, 1.0934932231903076, 1.0153694152832031, 0.9997649192810059, 1.0309901237487793, 1.0778403282165527]}
