{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170]], "accuracy": [0.6176470588235294, 0.5352941176470588, 0.6529411764705882, 0.6352941176470588, 0.6, 0.5176470588235295, 0.5352941176470588, 0.6, 0.6411764705882353, 0.5470588235294118], "precision": [0.5769593442415717, 0.4834126426739085, 0.6022340731164262, 0.5853800536009579, 0.5425881471469707, 0.38388868205510057, 0.5147399510767389, 0.5088363998567463, 0.5787971789506317, 0.4801470588235294], "recall": [0.6176470588235294, 0.5352941176470588, 0.6529411764705882, 0.6352941176470588, 0.6, 0.5176470588235295, 0.5352941176470588, 0.6, 0.6411764705882353, 0.5470588235294118], "f1_valid": [0.5735505954213449, 0.47095311808934104, 0.5921227453887439, 0.5834660481719306, 0.5458559525653983, 0.4269234571656717, 0.4932307066833336, 0.5319798319327731, 0.588882451041705, 0.4752695158752462], "f1_train": [0.8450294077091014, 0.8396026608292152, 0.836864972689325, 0.8420707837491364, 0.8513227468910982, 0.8446216788420584, 0.8365907956809141, 0.8436341727269348, 0.8439724751180729, 0.8386949599423777], "time": [2.233858108520508, 2.2026400566101074, 2.1245033740997314, 1.999528408050537, 2.1245040893554688, 2.030775785446167, 2.249438762664795, 1.8901822566986084, 2.3587775230407715, 1.8120834827423096]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170]], "accuracy": [0.6, 0.6705882352941176, 0.6352941176470588, 0.6529411764705882, 0.6352941176470588, 0.5588235294117647, 0.6235294117647059, 0.6411764705882353, 0.5647058823529412, 0.5235294117647059], "precision": [0.5694570135746607, 0.640499688419344, 0.5732353886588386, 0.601361979393747, 0.5890419043871729, 0.47059636403581034, 0.550268669559327, 0.6199274694203699, 0.5050742314360364, 0.4570265787343586], "recall": [0.6, 0.6705882352941176, 0.6352941176470588, 0.6529411764705882, 0.6352941176470588, 0.5588235294117647, 0.6235294117647059, 0.6411764705882353, 0.5647058823529412, 0.5235294117647059], "f1_valid": [0.5771003664349654, 0.6441334234843362, 0.5890474142873523, 0.6083041987913175, 0.6064985994397759, 0.5014571062069687, 0.5734434735225248, 0.6091771713684602, 0.5255485063801285, 0.477249548202093], "f1_train": [0.9660143340999728, 0.9647819400028632, 0.9669021435628312, 0.9658559011657077, 0.9661833052258965, 0.9666010555217448, 0.9713338797951399, 0.9664647171936998, 0.9650949828961208, 0.9645856771038994], "time": [3.530421495437622, 3.608530282974243, 3.671044111251831, 3.3273425102233887, 3.342933177947998, 3.4366791248321533, 3.3429622650146484, 3.311721086502075, 3.436694383621216, 3.436692237854004]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170], [1530, 170]], "accuracy": [0.48823529411764705, 0.49411764705882355, 0.5647058823529412, 0.5117647058823529, 0.4470588235294118, 0.4176470588235294, 0.49411764705882355, 0.4411764705882353, 0.43529411764705883, 0.43529411764705883], "precision": [0.5143425190156082, 0.5041876750700279, 0.561187996218534, 0.5219823919949076, 0.469063623514234, 0.4253693448256034, 0.47867805175880945, 0.43067947063055856, 0.4679729068167202, 0.4050456694992475], "recall": [0.48823529411764705, 0.49411764705882355, 0.5647058823529412, 0.5117647058823529, 0.4470588235294118, 0.4176470588235294, 0.49411764705882355, 0.4411764705882353, 0.43529411764705883, 0.43529411764705883], "f1_valid": [0.4717360502161521, 0.4918380364930541, 0.5548862325678935, 0.5053085516707808, 0.45002059294318797, 0.4091851971066584, 0.4822776481931958, 0.42640456289341866, 0.4253260003188267, 0.41739973145405473], "f1_train": [0.6041932083339727, 0.5773645821068912, 0.5733283563103538, 0.5885562291295029, 0.5990012341560307, 0.6021970177513283, 0.598561526582335, 0.596816015798971, 0.6087601708089356, 0.5967386916642915], "time": [0.8435516357421875, 0.8435473442077637, 0.8123059272766113, 0.8435525894165039, 0.8435523509979248, 0.8123083114624023, 0.8279283046722412, 0.8279285430908203, 0.8747944831848145, 0.8279330730438232]}
