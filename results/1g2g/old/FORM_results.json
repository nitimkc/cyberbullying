{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7109826589595376, 0.7167630057803468, 0.6936416184971098, 0.6127167630057804, 0.6416184971098265, 0.7572254335260116, 0.7109826589595376, 0.630057803468208, 0.6936416184971098, 0.6878612716763006, 0.7398843930635838, 0.6936416184971098], "precision": [0.6537044380772796, 0.6913249726757484, 0.6755678580151429, 0.540900866025579, 0.6182859556048248, 0.6980010820353655, 0.6944223406651152, 0.5931424067262218, 0.6391145106637433, 0.6190655340374761, 0.7081975827640566, 0.6257236920701729], "recall": [0.7109826589595376, 0.7167630057803468, 0.6936416184971098, 0.6127167630057804, 0.6416184971098265, 0.7572254335260116, 0.7109826589595376, 0.630057803468208, 0.6936416184971098, 0.6878612716763006, 0.7398843930635838, 0.6936416184971098], "f1": [0.6528485425782333, 0.6708817213320611, 0.6417642201389226, 0.5351881920887244, 0.5749097137529012, 0.7055344452822947, 0.663693131132917, 0.5849532439127815, 0.639539406745833, 0.6279638431435365, 0.694457103110834, 0.637391217310795], "time": [1.2749669551849365, 1.1621313095092773, 1.131904125213623, 1.5365409851074219, 1.2523069381713867, 1.1048188209533691, 1.2783629894256592, 1.2380568981170654, 1.20058012008667, 1.2628369331359863, 1.140456199645996, 1.1050670146942139]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7398843930635838, 0.6589595375722543, 0.7341040462427746, 0.7283236994219653, 0.6589595375722543, 0.6936416184971098, 0.7745664739884393, 0.7283236994219653, 0.7283236994219653, 0.7225433526011561, 0.6936416184971098, 0.7398843930635838], "precision": [0.6858010244842332, 0.6051188970618813, 0.6813822016134155, 0.647005044198578, 0.5857585268065724, 0.6544777201354001, 0.7292314092825941, 0.7352528381918743, 0.6576004511490202, 0.6723977573992785, 0.6042809335477423, 0.6778660886319846], "recall": [0.7398843930635838, 0.6589595375722543, 0.7341040462427746, 0.7283236994219653, 0.6589595375722543, 0.6936416184971098, 0.7745664739884393, 0.7283236994219653, 0.7283236994219653, 0.7225433526011561, 0.6936416184971098, 0.7398843930635838], "f1": [0.7013801783206703, 0.6000376435159044, 0.6941330585904044, 0.67189411371772, 0.5907650043469392, 0.641385490881391, 0.7435061794084215, 0.6886215428283509, 0.6885341633318511, 0.6803807930674316, 0.6338934427578851, 0.6923562666507244], "time": [3.304900884628296, 3.3931968212127686, 3.415539026260376, 3.365732192993164, 3.3358898162841797, 3.39785099029541, 3.1846797466278076, 3.187879800796509, 3.497642993927002, 3.4580841064453125, 3.2912960052490234, 3.2246901988983154]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.5953757225433526, 0.6184971098265896, 0.6763005780346821, 0.6473988439306358, 0.6473988439306358, 0.6069364161849711, 0.6473988439306358, 0.630057803468208, 0.5780346820809249, 0.6011560693641619, 0.5664739884393064, 0.5780346820809249], "precision": [0.5243362931224204, 0.5355933968650732, 0.6795761078998073, 0.6149662121673074, 0.5885808886238092, 0.5364890082381321, 0.5629707479418462, 0.5688666729839856, 0.5214212852771166, 0.608986024687205, 0.5403952144105391, 0.527794679376669], "recall": [0.5953757225433526, 0.6184971098265896, 0.6763005780346821, 0.6473988439306358, 0.6473988439306358, 0.6069364161849711, 0.6473988439306358, 0.630057803468208, 0.5780346820809249, 0.6011560693641619, 0.5664739884393064, 0.5780346820809249], "f1": [0.5506521524296385, 0.5631391729657627, 0.6501704461242034, 0.6083966014428007, 0.6093323930747996, 0.5649712826469747, 0.5969701559095001, 0.5919547417954996, 0.5360070922155288, 0.5796298408376623, 0.5276503727904833, 0.5290058009541179], "time": [0.8055810928344727, 0.7315409183502197, 0.7174339294433594, 0.706787109375, 0.7717111110687256, 0.7384309768676758, 0.7952408790588379, 0.7145528793334961, 0.7182478904724121, 0.7439002990722656, 0.7131509780883789, 0.7732739448547363]}
