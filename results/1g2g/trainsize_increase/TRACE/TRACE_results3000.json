{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500]], "accuracy": [0.718, 0.714, 0.694, 0.75, 0.744, 0.754], "precision": [0.7105099557522123, 0.6994494623655915, 0.6987760683760683, 0.7294539718080505, 0.7344177777777777, 0.7313489010989012], "recall": [0.718, 0.714, 0.694, 0.75, 0.744, 0.754], "f1_valid": [0.662127748231811, 0.6449457003242451, 0.6292723131521798, 0.6988916546927596, 0.6962873624020758, 0.7115740316677431], "f1_train": [0.7227752000000001, 0.7381774554174945, 0.7329172590665629, 0.7304638639347256, 0.722974605486552, 0.7248280031080032], "time": [4.992143154144287, 5.023808002471924, 4.963899850845337, 4.952639102935791, 5.009019136428833, 5.01344108581543]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500]], "accuracy": [0.702, 0.756, 0.7, 0.754, 0.756, 0.744], "precision": [0.6659951630519582, 0.7686235279219956, 0.690948275862069, 0.7624316319780164, 0.7751916950791309, 0.718841514041514], "recall": [0.702, 0.756, 0.7, 0.754, 0.756, 0.744], "f1_valid": [0.6528551228683499, 0.711885198135198, 0.6281115393334844, 0.7023463522285285, 0.704429868767747, 0.6933537936913896], "f1_train": [0.7842271807957615, 0.7894769707459364, 0.7799627221312234, 0.7857732181120612, 0.7758765817012082, 0.7891691054370145], "time": [1.6326768398284912, 1.6154427528381348, 1.62972092628479, 1.6382880210876465, 1.6341493129730225, 1.6388049125671387]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500]], "accuracy": [0.752, 0.772, 0.744, 0.724, 0.73, 0.774], "precision": [0.7357185563251915, 0.7615231704169502, 0.7266842723967303, 0.7069, 0.7107588466899806, 0.7646755964175319], "recall": [0.752, 0.772, 0.744, 0.724, 0.73, 0.774], "f1_valid": [0.72480739063822, 0.7596517097995409, 0.7213754385964912, 0.7033430353430353, 0.7149416907269707, 0.7556182186781998], "f1_train": [0.8274279327315135, 0.841676298140458, 0.8264904462892844, 0.8408544300963211, 0.8333405222015908, 0.837624379943047], "time": [5.143985986709595, 5.0175909996032715, 4.972158908843994, 4.951563835144043, 4.9147961139678955, 5.023025035858154]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500]], "accuracy": [0.686, 0.716, 0.714, 0.698, 0.7, 0.71], "precision": [0.6755694478427534, 0.7212561437591064, 0.7057225274725275, 0.694493557752519, 0.6826248019459507, 0.699430865993321], "recall": [0.686, 0.716, 0.714, 0.698, 0.7, 0.71], "f1_valid": [0.6797900024922909, 0.7184461960923764, 0.7090073632828657, 0.6961089551749492, 0.6879373623684856, 0.7031507842650587], "f1_train": [0.99879977425621, 0.9991997117333534, 0.9991996985910957, 0.998398774204576, 0.9987997755045604, 0.9991996960789463], "time": [1.6180989742279053, 1.597856044769287, 1.6283128261566162, 1.6169919967651367, 1.6146750450134277, 1.6327629089355469]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500], [2500, 500]], "accuracy": [0.7, 0.736, 0.768, 0.742, 0.742, 0.754], "precision": [0.6883468834688347, 0.7133494753833737, 0.7559406216681702, 0.724532871551829, 0.7197480139026812, 0.7342566236167692], "recall": [0.7, 0.736, 0.768, 0.742, 0.742, 0.754], "f1_valid": [0.6690786912786052, 0.71296875, 0.7478901607887248, 0.7130917370397432, 0.7131501693202215, 0.7311105376344085], "f1_train": [0.9295421984165487, 0.9274451386827768, 0.9245916946184789, 0.9250525038267169, 0.9264694242933783, 0.9243645465465863], "time": [4.890875339508057, 4.840703964233398, 4.941451072692871, 4.9265289306640625, 4.925074100494385, 4.94097113609314]}
