{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30]], "accuracy": [0.43333333333333335, 0.7, 0.6333333333333333, 0.6666666666666666, 0.6, 0.5666666666666667, 0.5666666666666667, 0.5333333333333333, 0.5666666666666667, 0.5], "precision": [0.41833333333333333, 0.6577777777777778, 0.5680555555555554, 0.6629629629629631, 0.5072222222222222, 0.4033333333333333, 0.5019047619047619, 0.44, 0.5159523809523809, 0.36666666666666664], "recall": [0.43333333333333335, 0.7, 0.6333333333333333, 0.6666666666666666, 0.6, 0.5666666666666667, 0.5666666666666667, 0.5333333333333333, 0.5666666666666667, 0.5], "f1_valid": [0.37179487179487175, 0.6574358974358975, 0.5596153846153846, 0.627180527383367, 0.5247653183137054, 0.45925925925925914, 0.49646464646464644, 0.4555555555555555, 0.5109328579916815, 0.3865079365079365], "f1_train": [0.7773402596508129, 0.7715128531500192, 0.7603004829662159, 0.7520523072471953, 0.7724639080599315, 0.779050237835982, 0.7574680681519583, 0.7531660171089047, 0.7713380687591483, 0.7820058282108301], "time": [0.23431825637817383, 0.21869850158691406, 0.21866488456726074, 0.23431849479675293, 0.21869897842407227, 0.21870040893554688, 0.23428940773010254, 0.21869707107543945, 0.21874117851257324, 0.29680395126342773]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30]], "accuracy": [0.6333333333333333, 0.6666666666666666, 0.7666666666666667, 0.43333333333333335, 0.6333333333333333, 0.7666666666666667, 0.6, 0.7, 0.7, 0.7333333333333333], "precision": [0.5074074074074074, 0.6583333333333333, 0.705, 0.33966329966329967, 0.49269841269841275, 0.716031746031746, 0.5111111111111112, 0.6064682539682539, 0.5988888888888889, 0.6966666666666667], "recall": [0.6333333333333333, 0.6666666666666666, 0.7666666666666667, 0.43333333333333335, 0.6333333333333333, 0.7666666666666667, 0.6, 0.7, 0.7, 0.7333333333333333], "f1_valid": [0.5416666666666666, 0.5885964912280702, 0.7317460317460317, 0.3790849673202615, 0.5496503496503495, 0.729945054945055, 0.5515151515151515, 0.6498778998778998, 0.6229260935143288, 0.6939684569479966], "f1_train": [0.9846936783639754, 0.9925487789217835, 0.9962138861154692, 0.9887461688006153, 0.9805352640861412, 0.9925599861658283, 0.983346055872567, 0.9888155725600604, 0.9846245751106789, 0.9882244008714597], "time": [0.2030785083770752, 0.21869874000549316, 0.23431968688964844, 0.23431992530822754, 0.21869206428527832, 0.21870160102844238, 0.21869897842407227, 0.21869874000549316, 0.21869683265686035, 0.23432016372680664]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30], [270, 30]], "accuracy": [0.6, 0.5, 0.6666666666666666, 0.6333333333333333, 0.6666666666666666, 0.5333333333333333, 0.7, 0.7666666666666667, 0.5666666666666667, 0.7], "precision": [0.5736507936507936, 0.47126984126984134, 0.5288888888888889, 0.5592592592592592, 0.7336842105263158, 0.4857142857142857, 0.6490079365079365, 0.7569444444444445, 0.47222222222222215, 0.612063492063492], "recall": [0.6, 0.5, 0.6666666666666666, 0.6333333333333333, 0.6666666666666666, 0.5333333333333333, 0.7, 0.7666666666666667, 0.5666666666666667, 0.7], "f1_valid": [0.5671568627450979, 0.4640000000000001, 0.5880341880341879, 0.588471177944862, 0.65506993006993, 0.4749650349650349, 0.6711111111111111, 0.7421052631578947, 0.46585858585858586, 0.6529244176303], "f1_train": [0.6654421379956549, 0.6962605618379039, 0.6651537928078473, 0.6436696787244517, 0.6591634001347576, 0.6605198987807682, 0.6766717619797603, 0.6493413285903563, 0.6986977516266842, 0.6610321488393601], "time": [0.12497091293334961, 0.14058732986450195, 0.12496542930603027, 0.1405928134918213, 0.15621423721313477, 0.12497067451477051, 0.15621447563171387, 0.14059233665466309, 0.14059185981750488, 0.15621423721313477]}
