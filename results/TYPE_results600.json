{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50]], "accuracy": [0.64, 0.5, 0.66, 0.54, 0.62, 0.66, 0.66, 0.64, 0.56, 0.66, 0.64, 0.56], "precision": [0.6175, 0.45949321266968324, 0.6732205128205129, 0.48200000000000004, 0.577047619047619, 0.6914285714285714, 0.6801515151515152, 0.6293535353535353, 0.5486192068800765, 0.6695454545454546, 0.578483660130719, 0.53125], "recall": [0.64, 0.5, 0.66, 0.54, 0.62, 0.66, 0.66, 0.64, 0.56, 0.66, 0.64, 0.56], "f1_valid": [0.6149242838898011, 0.4595483870967742, 0.6446279569892472, 0.508695652173913, 0.593121473121473, 0.6315151515151515, 0.6500317460317462, 0.6087464840406017, 0.549226750261233, 0.6443909774436091, 0.6074993633817164, 0.5425557809330628], "f1_train": [0.8871689422112364, 0.9006280886834661, 0.8891495355550016, 0.8987203021187266, 0.8953007479483979, 0.8892831906783943, 0.8901598554721928, 0.8933923647526391, 0.8934203359755326, 0.8961067216723624, 0.8973083964384341, 0.8953098386561578], "time": [0.4741065502166748, 0.4526638984680176, 0.45179271697998047, 0.44405293464660645, 0.5863409042358398, 0.4614274501800537, 0.40383076667785645, 0.5548379421234131, 0.4649012088775635, 0.43395161628723145, 0.4038395881652832, 0.38332509994506836]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50]], "accuracy": [0.6, 0.68, 0.72, 0.5, 0.62, 0.58, 0.6, 0.5, 0.56, 0.62, 0.68, 0.62], "precision": [0.5844444444444444, 0.6716339869281046, 0.6659109311740891, 0.46090909090909093, 0.6177620521892968, 0.5302805429864254, 0.6223921568627451, 0.5336296296296297, 0.5047058823529411, 0.5921794871794872, 0.6442857142857144, 0.608857142857143], "recall": [0.6, 0.68, 0.72, 0.5, 0.62, 0.58, 0.6, 0.5, 0.56, 0.62, 0.68, 0.62], "f1_valid": [0.5742857142857143, 0.6708061509785648, 0.69003367003367, 0.47021346469622327, 0.6088676236044657, 0.5520761904761905, 0.594390243902439, 0.4573588516746412, 0.5300806451612903, 0.596741737532318, 0.6609427609427609, 0.6133497536945813], "f1_train": [0.9782816047002093, 0.9824263055972824, 0.9799626554862524, 0.9806673248313105, 0.9757608126291949, 0.9761014182610589, 0.9845706804847439, 0.9782370838370422, 0.9764063023725109, 0.9803470640679943, 0.9866560882754958, 0.982612810220914], "time": [0.6212799549102783, 0.6707484722137451, 0.6854104995727539, 0.6897623538970947, 0.571962833404541, 0.6196761131286621, 0.6250696182250977, 0.639923095703125, 0.5719733238220215, 0.5658736228942871, 0.5685272216796875, 0.7274351119995117]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50], [550, 50]], "accuracy": [0.52, 0.5, 0.56, 0.54, 0.52, 0.56, 0.72, 0.54, 0.62, 0.54, 0.4, 0.6], "precision": [0.6186666666666667, 0.5386666666666666, 0.5527058823529412, 0.49422924901185766, 0.4959316239316239, 0.527578947368421, 0.7540932400932401, 0.5686285714285714, 0.6390793650793651, 0.5038095238095238, 0.37456043956043955, 0.6415734989648033], "recall": [0.52, 0.5, 0.56, 0.54, 0.52, 0.56, 0.72, 0.54, 0.62, 0.54, 0.4, 0.6], "f1_valid": [0.5184045584045585, 0.4731428571428571, 0.5153311516284869, 0.5089230769230769, 0.5016524216524216, 0.5356825396825396, 0.7165837837837838, 0.5312299741602068, 0.597974807386572, 0.5101923076923077, 0.3688964346349745, 0.5737472283813747], "f1_train": [0.6332064345501046, 0.6340196020323788, 0.6232575241495297, 0.6171030759626505, 0.6489892236486249, 0.6377801567571388, 0.6148446751198263, 0.6188024941543786, 0.6175094978303535, 0.6185345964923235, 0.6444104342057978, 0.6355781673361567], "time": [0.33515357971191406, 0.26270413398742676, 0.3131418228149414, 0.265627384185791, 0.26569652557373047, 0.2631955146789551, 0.27573633193969727, 0.2636427879333496, 0.26555871963500977, 0.25731492042541504, 0.2728393077850342, 0.3030986785888672]}
