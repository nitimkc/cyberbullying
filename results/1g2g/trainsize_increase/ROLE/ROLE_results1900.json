{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190]], "accuracy": [0.5947368421052631, 0.6473684210526316, 0.5631578947368421, 0.5631578947368421, 0.5789473684210527, 0.5894736842105263, 0.6526315789473685, 0.5210526315789473, 0.5631578947368421, 0.6105263157894737], "precision": [0.527728926610604, 0.5992868575732713, 0.5244299179081787, 0.45840794293425874, 0.5152646321686569, 0.5233068426050883, 0.6259594133011854, 0.46077141868543753, 0.511777038750723, 0.571072945521699], "recall": [0.5947368421052631, 0.6473684210526316, 0.5631578947368421, 0.5631578947368421, 0.5789473684210527, 0.5894736842105263, 0.6526315789473685, 0.5210526315789473, 0.5631578947368421, 0.6105263157894737], "f1_valid": [0.5352565365832809, 0.5953681375242934, 0.5222412553927316, 0.49919801440802136, 0.5203651737942904, 0.5092273187338977, 0.6064797272666596, 0.46056763467771905, 0.5053684631747436, 0.5518648132746548], "f1_train": [0.8357651855642549, 0.8413445118995351, 0.8404420683096188, 0.8349398500571241, 0.8413134787041561, 0.8414140763565395, 0.8357442993887558, 0.8409733775280737, 0.8395929694430964, 0.8355042387077682], "time": [2.1557796001434326, 2.6243865489959717, 2.4213132858276367, 2.390064239501953, 2.3276116847991943, 2.3119266033172607, 2.2650938034057617, 2.874361038208008, 2.4056894779205322, 2.3275487422943115]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190]], "accuracy": [0.5526315789473685, 0.5789473684210527, 0.5210526315789473, 0.6842105263157895, 0.5789473684210527, 0.5736842105263158, 0.6473684210526316, 0.6105263157894737, 0.6, 0.6368421052631579], "precision": [0.5095842543843023, 0.5265155535898569, 0.4347657647103308, 0.6135298879004693, 0.5607236842105263, 0.5251750085999313, 0.584800917844361, 0.5659058317889523, 0.5462107974414591, 0.5549623287671233], "recall": [0.5526315789473685, 0.5789473684210527, 0.5210526315789473, 0.6842105263157895, 0.5789473684210527, 0.5736842105263158, 0.6473684210526316, 0.6105263157894737, 0.6, 0.6368421052631579], "f1_valid": [0.5196444805084259, 0.549494136721533, 0.4670260922170494, 0.6391785270576136, 0.5315556245554788, 0.5388954528873738, 0.6009753417221239, 0.5681117980784753, 0.560359630694383, 0.5891428549579152], "f1_train": [0.9623456202709937, 0.9645301156922234, 0.9623690356658557, 0.9659579057933331, 0.9635957805158996, 0.967916292560839, 0.9619265907357866, 0.961393363674498, 0.9626834127890885, 0.9644636828256552], "time": [4.186504125595093, 4.233396291732788, 4.280209302902222, 4.18651556968689, 4.233379125595093, 4.202133655548096, 4.155266046524048, 4.202134609222412, 4.1708972454071045, 4.233381032943726]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190], [1710, 190]], "accuracy": [0.49473684210526314, 0.45789473684210524, 0.41578947368421054, 0.4473684210526316, 0.4631578947368421, 0.49473684210526314, 0.4473684210526316, 0.47368421052631576, 0.4789473684210526, 0.4842105263157895], "precision": [0.5129651050703683, 0.47671245296704295, 0.4716081514762516, 0.44187024510073514, 0.47723160652654206, 0.48089224047407175, 0.43232680681423824, 0.4571434100463955, 0.4698685570668209, 0.47869528472343653], "recall": [0.49473684210526314, 0.45789473684210524, 0.41578947368421054, 0.4473684210526316, 0.4631578947368421, 0.49473684210526314, 0.4473684210526316, 0.47368421052631576, 0.4789473684210526, 0.4842105263157895], "f1_valid": [0.49014803743639024, 0.4528123338649655, 0.4253261622361793, 0.4331924079688747, 0.4600225334386874, 0.4771810009438924, 0.4224343473661238, 0.4518706867606783, 0.46211391586151246, 0.471196881780149], "f1_train": [0.576609514870117, 0.5876294542162257, 0.5847897213629666, 0.5765299597837178, 0.5753278137577124, 0.584438967135168, 0.5819902409721142, 0.5820561330463685, 0.5834336139204049, 0.5817234518703455], "time": [0.9685192108154297, 0.9684860706329346, 1.1247339248657227, 0.9685285091400146, 1.062279224395752, 1.062284231185913, 1.0613107681274414, 1.1871898174285889, 1.0153565406799316, 0.9997646808624268]}
