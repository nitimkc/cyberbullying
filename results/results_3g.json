{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6609756097560976, 0.6609756097560976, 0.6707317073170732, 0.6097560975609756, 0.6707317073170732, 0.651219512195122, 0.6902439024390243, 0.6650366748166259, 0.6674816625916871, 0.6821515892420538, 0.6943765281173594, 0.6992665036674817], "precision": [0.6588997378899139, 0.6586862234961175, 0.6671078093211421, 0.6454371701611753, 0.6672552698377664, 0.6441344165160522, 0.7149382331081794, 0.6663183120949416, 0.6762645572079488, 0.6927150663729806, 0.692348791204885, 0.7047064698902766], "recall": [0.6609756097560976, 0.6609756097560976, 0.6707317073170732, 0.6097560975609756, 0.6707317073170732, 0.651219512195122, 0.6902439024390243, 0.6650366748166259, 0.6674816625916871, 0.6821515892420538, 0.6943765281173594, 0.6992665036674817], "f1": [0.6345511249384993, 0.6401366354101896, 0.6547464587937266, 0.579352579166394, 0.6513512676110368, 0.629652957497467, 0.6691668766171183, 0.6383147540820823, 0.6382896457610734, 0.6648600710758202, 0.6716432631289909, 0.6845807901711096], "time": [22.141072988510132, 20.00481653213501, 20.902353763580322, 20.851475477218628, 20.561689376831055, 21.25288414955139, 21.701143741607666, 21.66695237159729, 20.792309284210205, 21.429757356643677, 21.84921360015869, 21.96626305580139]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000022CCE3AAC18>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6926829268292682, 0.675609756097561, 0.6780487804878049, 0.7341463414634146, 0.6902439024390243, 0.6926829268292682, 0.7024390243902439, 0.6577017114914425, 0.684596577017115, 0.6552567237163814, 0.6430317848410758, 0.6968215158924206], "precision": [0.6930957545006144, 0.6829086606768454, 0.6904936913420476, 0.7324533434193391, 0.6813296616837136, 0.7073891286831738, 0.7113589444895028, 0.6670900411731708, 0.6864396708619517, 0.6558548024787096, 0.6519554723829509, 0.6970524422537633], "recall": [0.6926829268292682, 0.675609756097561, 0.6780487804878049, 0.7341463414634146, 0.6902439024390243, 0.6926829268292682, 0.7024390243902439, 0.6577017114914425, 0.684596577017115, 0.6552567237163814, 0.6430317848410758, 0.6968215158924206], "f1": [0.6797264230287067, 0.659141401369263, 0.6606608664053584, 0.7244097774830474, 0.6719154573117829, 0.672221772577438, 0.6861832626629819, 0.6396776170825575, 0.6645636404336719, 0.6397183062209911, 0.6179653364877095, 0.6759087475326744], "time": [1.4431982040405273, 1.3274564743041992, 1.3882920742034912, 1.415255069732666, 1.1958050727844238, 1.53291654586792, 1.7980728149414062, 1.2177462577819824, 1.3361010551452637, 1.260669469833374, 1.336435079574585, 1.3005609512329102]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6658536585365854, 0.697560975609756, 0.6463414634146342, 0.6829268292682927, 0.7121951219512195, 0.6536585365853659, 0.6926829268292682, 0.6723716381418093, 0.6992665036674817, 0.6919315403422983, 0.6430317848410758, 0.6723716381418093], "precision": [0.6643766209167469, 0.6947342334780279, 0.6456769219363926, 0.6801415482748268, 0.7112234780884394, 0.6857772666283305, 0.6935729182293988, 0.6717301523017366, 0.6953402459265076, 0.6907501794779427, 0.6557998433308805, 0.6707320547191544], "recall": [0.6658536585365854, 0.697560975609756, 0.6463414634146342, 0.6829268292682927, 0.7121951219512195, 0.6536585365853659, 0.6926829268292682, 0.6723716381418093, 0.6992665036674817, 0.6919315403422983, 0.6430317848410758, 0.6723716381418093], "f1": [0.6650159087158444, 0.6837834565396013, 0.645974613290849, 0.6809422178819785, 0.7116408858143489, 0.656536376877343, 0.6799816997235975, 0.6704243079949084, 0.6925250779878945, 0.6863690006786134, 0.6183168625223497, 0.6578275489125699], "time": [21.173570156097412, 21.37788224220276, 21.951847314834595, 21.733044147491455, 21.90552043914795, 22.0960476398468, 21.5431227684021, 22.280353546142578, 21.506744384765625, 21.771453857421875, 22.07905912399292, 21.759357690811157]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6463414634146342, 0.6926829268292682, 0.6878048780487804, 0.6634146341463415, 0.6780487804878049, 0.697560975609756, 0.6585365853658537, 0.684596577017115, 0.6992665036674817, 0.6454767726161369, 0.6308068459657702, 0.6821515892420538], "precision": [0.6514751286346309, 0.6912327708329201, 0.6863474122546103, 0.6627784128639294, 0.6780487804878049, 0.7007491424197465, 0.6653816743572841, 0.6898054027430041, 0.6979193827987905, 0.6468093566790231, 0.6355875115797437, 0.681366615700477], "recall": [0.6463414634146342, 0.6926829268292682, 0.6878048780487804, 0.6634146341463415, 0.6780487804878049, 0.697560975609756, 0.6585365853658537, 0.684596577017115, 0.6992665036674817, 0.6454767726161369, 0.6308068459657702, 0.6821515892420538], "f1": [0.6484497865392157, 0.6908894654903996, 0.6869278606292384, 0.6629541558952456, 0.6780487804878049, 0.6987411966751554, 0.660700579907897, 0.6860234224638546, 0.6983864361373152, 0.645954425905526, 0.6325661867575951, 0.6816936391508517], "time": [1.2092938423156738, 1.1957919597625732, 1.2287781238555908, 1.2885432243347168, 1.2895750999450684, 1.222717046737671, 1.4261770248413086, 1.2018065452575684, 1.3220176696777344, 1.5299246311187744, 1.4571185111999512, 1.4431581497192383]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000022CCE3AAC18>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6585365853658537, 0.6853658536585366, 0.6341463414634146, 0.6634146341463415, 0.6195121951219512, 0.6658536585365854, 0.6463414634146342, 0.6308068459657702, 0.6748166259168704, 0.6625916870415648, 0.6968215158924206, 0.6650366748166259], "precision": [0.7092946605141728, 0.7230740104215714, 0.6503141167775314, 0.7008767446778514, 0.6314716534426025, 0.7024894033206557, 0.6873045653533459, 0.645419256356312, 0.6652927092096415, 0.6775356831180446, 0.7252425055809504, 0.6814966098265454], "recall": [0.6585365853658537, 0.6853658536585366, 0.6341463414634146, 0.6634146341463415, 0.6195121951219512, 0.6658536585365854, 0.6463414634146342, 0.6308068459657702, 0.6748166259168704, 0.6625916870415648, 0.6968215158924206, 0.6650366748166259], "f1": [0.6191369606003753, 0.649381173405767, 0.5964553738943984, 0.6174401970997099, 0.5749922298015736, 0.6239771463198216, 0.5987058237929318, 0.5813697907034561, 0.6433253340501207, 0.6216304547280509, 0.6561822928149171, 0.6257825391227143], "time": [1.148935079574585, 1.4591894149780273, 1.2057902812957764, 1.4761085510253906, 1.237663984298706, 1.407252311706543, 1.3015682697296143, 1.317509412765503, 1.291562795639038, 1.2197449207305908, 1.1878302097320557, 1.389272928237915]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(2, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000022CCE3AAC18>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5634146341463414, 0.5365853658536586, 0.5975609756097561, 0.6, 0.6195121951219512, 0.6292682926829268, 0.6195121951219512, 0.5965770171149144, 0.5843520782396088, 0.5721271393643031, 0.5256723716381418, 0.5525672371638142], "precision": [0.5171311921996504, 0.5115676232596502, 0.5629011553273428, 0.5764386746754505, 0.6028969115420398, 0.6278229448961156, 0.6423149015932521, 0.5847968467974737, 0.6705087903131911, 0.5162588086175917, 0.5143365192264948, 0.5046600178127741], "recall": [0.5634146341463414, 0.5365853658536586, 0.5975609756097561, 0.6, 0.6195121951219512, 0.6292682926829268, 0.6195121951219512, 0.5965770171149144, 0.5843520782396088, 0.5721271393643031, 0.5256723716381418, 0.5525672371638142], "f1": [0.455905692588794, 0.41841635987114584, 0.501718574108818, 0.47656550806818576, 0.5256711751065801, 0.5434726321785511, 0.516882059631627, 0.4900930496725306, 0.4789986958009724, 0.45643386498093413, 0.40792452689112696, 0.4366565445798267], "time": [20.670536279678345, 22.97279167175293, 22.487077474594116, 22.11011505126953, 20.834479570388794, 22.502119064331055, 21.25638246536255, 21.859405040740967, 21.26329231262207, 20.879275798797607, 21.425577640533447, 22.00333285331726]}
