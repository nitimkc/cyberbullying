{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110]], "accuracy": [0.5818181818181818, 0.6090909090909091, 0.6727272727272727, 0.5909090909090909, 0.6272727272727273, 0.6454545454545455, 0.5272727272727272, 0.6454545454545455, 0.6363636363636364, 0.6636363636363637], "precision": [0.4771920625957893, 0.5025234025234026, 0.5804675324675325, 0.48465116279069764, 0.4969216391331354, 0.5664639986673885, 0.4134199134199134, 0.5152066115702479, 0.5829782019223014, 0.6785167464114832], "recall": [0.5818181818181818, 0.6090909090909091, 0.6727272727272727, 0.5909090909090909, 0.6272727272727273, 0.6454545454545455, 0.5272727272727272, 0.6454545454545455, 0.6363636363636364, 0.6636363636363637], "f1_valid": [0.5227279032522478, 0.5469593450028233, 0.6180567995223167, 0.5025252525252525, 0.5501075717115425, 0.5713543198980092, 0.45453103406837714, 0.5593543588443035, 0.5683714460200053, 0.6187897663833493], "f1_train": [0.8497178535388705, 0.8615422434095609, 0.8659436612524883, 0.8587767087944849, 0.8528945013173774, 0.857766282437912, 0.853440957853898, 0.8497783277348043, 0.8449734988859217, 0.8485252472334973], "time": [1.1872262954711914, 1.093494176864624, 1.2809195518493652, 1.2653253078460693, 1.234055519104004, 1.24967360496521, 1.343433141708374, 1.4059174060821533, 1.3902933597564697, 1.1403601169586182]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110]], "accuracy": [0.5727272727272728, 0.6545454545454545, 0.6545454545454545, 0.6181818181818182, 0.6636363636363637, 0.6363636363636364, 0.6818181818181818, 0.6545454545454545, 0.5909090909090909, 0.6727272727272727], "precision": [0.5143315508021391, 0.6197885835095137, 0.6211477483555405, 0.4980985100171147, 0.649519026882777, 0.5820667504538471, 0.634736842105263, 0.5469934640522875, 0.4900603176219078, 0.5997916976640382], "recall": [0.5727272727272728, 0.6545454545454545, 0.6545454545454545, 0.6181818181818182, 0.6636363636363637, 0.6363636363636364, 0.6818181818181818, 0.6545454545454545, 0.5909090909090909, 0.6727272727272727], "f1_valid": [0.5186561365665844, 0.623358331809036, 0.6175508764111705, 0.5502005975245412, 0.6205962667221151, 0.5831393685753137, 0.6409433645029036, 0.5912599681020734, 0.5354138018843901, 0.6229133408849432], "f1_train": [0.971936775673634, 0.9666270813679532, 0.9654478530884605, 0.9729544217581146, 0.9699785279258455, 0.9730672951197042, 0.9736865848417503, 0.9665968183098789, 0.9736677858518432, 0.9734963268156583], "time": [1.6558752059936523, 1.64024019241333, 1.6402339935302734, 1.7026934623718262, 1.6558740139007568, 1.6246209144592285, 1.6402404308319092, 1.5933759212493896, 1.734001874923706, 1.6246182918548584]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110], [990, 110]], "accuracy": [0.4818181818181818, 0.4909090909090909, 0.5363636363636364, 0.4636363636363636, 0.5636363636363636, 0.4909090909090909, 0.5636363636363636, 0.5363636363636364, 0.5, 0.5727272727272728], "precision": [0.4327555273009819, 0.4823809523809524, 0.517743376741166, 0.39378220463746777, 0.5459506879674947, 0.45556821099311473, 0.5488742685885543, 0.5394462935996117, 0.45540106951871656, 0.5645853994490357], "recall": [0.4818181818181818, 0.4909090909090909, 0.5363636363636364, 0.4636363636363636, 0.5636363636363636, 0.4909090909090909, 0.5636363636363636, 0.5363636363636364, 0.5, 0.5727272727272728], "f1_valid": [0.43977250768555115, 0.47030303030303033, 0.5106908845042719, 0.4174059561128527, 0.5521134494853659, 0.469805278896188, 0.5465939011717491, 0.5235901981664693, 0.47296779268090744, 0.5517860994193097], "f1_train": [0.6215117909641062, 0.6153825740319924, 0.6418847588628653, 0.6288056019183954, 0.6463078490658739, 0.6372630019242002, 0.6395731444744969, 0.6355589888017991, 0.6301081878290157, 0.6327078519902921], "time": [0.5155034065246582, 0.5311245918273926, 0.6092314720153809, 0.5155045986175537, 0.5311546325683594, 0.5623354911804199, 0.5311253070831299, 0.6248574256896973, 0.6717171669006348, 0.6092321872711182]}
