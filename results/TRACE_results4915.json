{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7146341463414634, 0.6829268292682927, 0.6634146341463415, 0.7219512195121951, 0.6780487804878049, 0.6609756097560976, 0.7365853658536585, 0.687041564792176, 0.6919315403422983, 0.726161369193154, 0.7114914425427873, 0.6821515892420538], "precision": [0.7118771636331608, 0.6802224467511975, 0.6615926829268293, 0.7214253017683174, 0.6742719690870232, 0.6565669581224115, 0.7374254919789225, 0.6839488090295398, 0.6929930228397638, 0.7225390633836177, 0.7104032265983248, 0.6794787841862423], "recall": [0.7146341463414634, 0.6829268292682927, 0.6634146341463415, 0.7219512195121951, 0.6780487804878049, 0.6609756097560976, 0.7365853658536585, 0.687041564792176, 0.6919315403422983, 0.726161369193154, 0.7114914425427873, 0.6821515892420538], "f1": [0.7125218330065156, 0.6782960693668934, 0.6623612634874517, 0.7182030802136976, 0.6745151793357342, 0.655377493350328, 0.7294653252530078, 0.6816681275084191, 0.6891518964264217, 0.7212726488346615, 0.7108785763613541, 0.677986651754148], "time": [26.9643611907959, 27.046275854110718, 28.190338611602783, 26.75010919570923, 25.589963912963867, 25.400264024734497, 27.289934635162354, 26.409897565841675, 26.215068817138672, 26.890498399734497, 25.69008493423462, 26.298461437225342]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7146341463414634, 0.7365853658536585, 0.7073170731707317, 0.7097560975609756, 0.7317073170731707, 0.7, 0.7219512195121951, 0.6650366748166259, 0.6943765281173594, 0.6723716381418093, 0.7041564792176039, 0.7114914425427873], "precision": [0.713095506216954, 0.7339989030177078, 0.703306726756272, 0.7073430382533402, 0.7296514061445885, 0.6973561037318153, 0.7225315822388994, 0.6618647903704714, 0.6966302994673285, 0.6703376407404202, 0.7016272359268044, 0.7077473154953974], "recall": [0.7146341463414634, 0.7365853658536585, 0.7073170731707317, 0.7097560975609756, 0.7317073170731707, 0.7, 0.7219512195121951, 0.6650366748166259, 0.6943765281173594, 0.6723716381418093, 0.7041564792176039, 0.7114914425427873], "f1": [0.7101491680582628, 0.7334759405460238, 0.7039094670573329, 0.7058236453380101, 0.7302349832598081, 0.6981664263105649, 0.7176676317424838, 0.6613087877878414, 0.6889253409485248, 0.6659467481564615, 0.7001782225350999, 0.7078640171950271], "time": [1.5302956104278564, 1.6200852394104004, 1.515005111694336, 1.5099318027496338, 1.7450952529907227, 2.480085611343384, 1.7599594593048096, 2.1049673557281494, 1.4840006828308105, 1.6227607727050781, 1.5096375942230225, 1.539963722229004]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7, 0.7121951219512195, 0.6707317073170732, 0.7317073170731707, 0.6487804878048781, 0.6682926829268293, 0.724390243902439, 0.726161369193154, 0.6797066014669927, 0.7212713936430318, 0.684596577017115, 0.6454767726161369], "precision": [0.6965897435897435, 0.714618311899843, 0.6712418827753545, 0.7314581011628508, 0.668487243021927, 0.6681024623643367, 0.7213043599232075, 0.7427250500111137, 0.696858428001174, 0.7267887348060258, 0.7125252592915118, 0.6579142224009213], "recall": [0.7, 0.7121951219512195, 0.6707317073170732, 0.7317073170731707, 0.6487804878048781, 0.6682926829268293, 0.724390243902439, 0.726161369193154, 0.6797066014669927, 0.7212713936430318, 0.684596577017115, 0.6454767726161369], "f1": [0.6967544534754941, 0.6972972937849583, 0.6709509153856856, 0.7315652300019787, 0.6525005856688162, 0.6681816618065772, 0.7165071879120951, 0.7288458017215167, 0.6814386748584611, 0.7226375780547879, 0.6870031824220771, 0.6469638577796288], "time": [26.140296936035156, 25.968114614486694, 26.849926233291626, 26.57012629508972, 25.340085983276367, 25.479918718338013, 25.914727926254272, 26.174750804901123, 26.104886770248413, 26.219835996627808, 25.999703645706177, 26.45992660522461]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6878048780487804, 0.6804878048780488, 0.7024390243902439, 0.7341463414634146, 0.675609756097561, 0.7414634146341463, 0.6731707317073171, 0.6650366748166259, 0.7041564792176039, 0.6968215158924206, 0.7163814180929096, 0.6772616136919315], "precision": [0.69514018429431, 0.681714407773526, 0.7047497524717958, 0.7454422919827284, 0.6793359787498314, 0.7462152344352656, 0.6858326525848991, 0.6731373004105423, 0.7110526570644987, 0.7029946266515971, 0.7147604695324852, 0.6833232781888039], "recall": [0.6878048780487804, 0.6804878048780488, 0.7024390243902439, 0.7341463414634146, 0.675609756097561, 0.7414634146341463, 0.6731707317073171, 0.6650366748166259, 0.7041564792176039, 0.6968215158924206, 0.7163814180929096, 0.6772616136919315], "f1": [0.6898460608007861, 0.681000057400614, 0.7029935388230029, 0.7366282723008848, 0.6765712087607493, 0.7427123836455756, 0.6763926791365815, 0.6673476802664354, 0.7053833742918002, 0.6982481520990079, 0.7144547942040009, 0.67898805265799], "time": [1.5396370887756348, 1.6448965072631836, 1.6599509716033936, 1.5802206993103027, 1.6067352294921875, 1.4398865699768066, 1.438180685043335, 1.4114758968353271, 1.497971534729004, 1.5513908863067627, 1.567697525024414, 1.6686787605285645]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7170731707317073, 0.7560975609756098, 0.7024390243902439, 0.7341463414634146, 0.7, 0.7268292682926829, 0.7073170731707317, 0.7188264058679706, 0.7090464547677262, 0.706601466992665, 0.7041564792176039, 0.7017114914425427], "precision": [0.7310980427192766, 0.7569539192882291, 0.7092959042797976, 0.7362940125767697, 0.7009297520661157, 0.725695504930558, 0.7076264053048082, 0.7210577229722546, 0.7154919218125888, 0.7061675955072975, 0.7033171901448185, 0.7004728099667402], "recall": [0.7170731707317073, 0.7560975609756098, 0.7024390243902439, 0.7341463414634146, 0.7, 0.7268292682926829, 0.7073170731707317, 0.7188264058679706, 0.7090464547677262, 0.706601466992665, 0.7041564792176039, 0.7017114914425427], "f1": [0.7202769739561802, 0.7564534064793536, 0.7038518699615907, 0.7350053854439146, 0.7004273267106942, 0.7261049087693853, 0.7057670323940908, 0.7195942620975082, 0.7110501366934627, 0.7063675485272852, 0.7025041545007954, 0.7008215135006497], "time": [10.230214357376099, 10.660477638244629, 10.090212345123291, 10.239873170852661, 9.920640468597412, 10.140134811401367, 10.850277423858643, 10.225107431411743, 10.79388165473938, 9.830251455307007, 9.820209741592407, 9.78504204750061]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.651219512195122, 0.651219512195122, 0.6121951219512195, 0.6268292682926829, 0.5853658536585366, 0.6170731707317073, 0.6487804878048781, 0.5965770171149144, 0.6528117359413202, 0.60880195599022, 0.6503667481662592, 0.6577017114914425], "precision": [0.6858076251995396, 0.7131744483159118, 0.6614238346405857, 0.6627970851688261, 0.6980831688212444, 0.6957125526752651, 0.6951086966126526, 0.7062671115864022, 0.720199062008353, 0.6482878548174104, 0.7255115210125469, 0.7576444990278889], "recall": [0.651219512195122, 0.651219512195122, 0.6121951219512195, 0.6268292682926829, 0.5853658536585366, 0.6170731707317073, 0.6487804878048781, 0.5965770171149144, 0.6528117359413202, 0.60880195599022, 0.6503667481662592, 0.6577017114914425], "f1": [0.5858626972327202, 0.5744686865526577, 0.5326875793003064, 0.5422038317024358, 0.4854829591342272, 0.5240433307479556, 0.5738752658121602, 0.5017992994033641, 0.5879183209588539, 0.5234418564749281, 0.5748783513274173, 0.5776871164520032], "time": [1.6602602005004883, 1.3726632595062256, 1.6246337890625, 1.3183307647705078, 1.5308663845062256, 1.2622034549713135, 1.4376366138458252, 1.4238648414611816, 1.32781982421875, 1.388070821762085, 1.2653212547302246, 1.2653017044067383]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5853658536585366, 0.624390243902439, 0.5707317073170731, 0.6585365853658537, 0.624390243902439, 0.6097560975609756, 0.6073170731707317, 0.5794621026894865, 0.5990220048899756, 0.6332518337408313, 0.6112469437652812, 0.5623471882640587], "precision": [0.5539152759948652, 0.6299527818490488, 0.5544229961016188, 0.6722389695807071, 0.6385216386982812, 0.6294249061470786, 0.6139175968030867, 0.5742822076403452, 0.6102717599958533, 0.6335435210416737, 0.6371529893410979, 0.5596705930679462], "recall": [0.5853658536585366, 0.624390243902439, 0.5707317073170731, 0.6585365853658537, 0.624390243902439, 0.6097560975609756, 0.6073170731707317, 0.5794621026894865, 0.5990220048899756, 0.6332518337408313, 0.6112469437652812, 0.5623471882640587], "f1": [0.48705743509047994, 0.5626883429640588, 0.4786047284919967, 0.60369891778388, 0.5605606912075629, 0.5496497477164232, 0.5488323819408406, 0.502280048123569, 0.5253493594676051, 0.560038958838252, 0.5338771542865645, 0.4691656830782098], "time": [24.40327215194702, 24.457950353622437, 24.643107891082764, 24.146758794784546, 23.933847427368164, 22.89214515686035, 23.51949119567871, 26.148841619491577, 25.260536909103394, 25.9217586517334, 25.24578022956848, 25.83477783203125]}
