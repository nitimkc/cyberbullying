{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 tok...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6722560975609756, 0.6707317073170732, 0.6875, 0.6402439024390244, 0.6554878048780488, 0.6935975609756098, 0.6875, 0.6524390243902439, 0.665648854961832, 0.6458015267175573, 0.665648854961832, 0.6259541984732825], "precision": [0.664542090930618, 0.6624627278209596, 0.6742277992277992, 0.6312214951417806, 0.6541797098111201, 0.6894942186243633, 0.6785586825736719, 0.6436921212922792, 0.6537247732777691, 0.639082540957993, 0.6617788652267973, 0.6147005937234944], "recall": [0.6722560975609756, 0.6707317073170732, 0.6875, 0.6402439024390244, 0.6554878048780488, 0.6935975609756098, 0.6875, 0.6524390243902439, 0.665648854961832, 0.6458015267175573, 0.665648854961832, 0.6259541984732825], "f1_valid": [0.6672456173109274, 0.6634863262288472, 0.6757557815073434, 0.6181982425884865, 0.6366846284741918, 0.6851977361429923, 0.6767065365156153, 0.6435060864462091, 0.6508893448342458, 0.6287444411743541, 0.6601911088280978, 0.6178787043986237], "f1_train": [0.7074321504160369, 0.7126890993437693, 0.7133883839860246, 0.7124874805040591, 0.7128049972664057, 0.7129763389924061, 0.7099570933784071, 0.7132830347147338, 0.7133858356118986, 0.7169736934088482, 0.7137434286538304, 0.718661538896364], "time": [11.3022940158844, 12.760093212127686, 10.962320804595947, 10.805745840072632, 10.860821962356567, 10.83455514907837, 10.957765817642212, 11.247280836105347, 10.963112831115723, 11.221878051757812, 11.390651226043701, 11.165710926055908]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6326219512195121, 0.6310975609756098, 0.6341463414634146, 0.6341463414634146, 0.6692073170731707, 0.6554878048780488, 0.6722560975609756, 0.6661585365853658, 0.6809160305343511, 0.6580152671755726, 0.6442748091603053, 0.6381679389312978], "precision": [0.6318193259641957, 0.6257592390682021, 0.6300019829466588, 0.6293888041080442, 0.6626670277940672, 0.6505607578397212, 0.6661893897864404, 0.6600990349561233, 0.6740563290398679, 0.6551790558111714, 0.6368251056356397, 0.6304029824249955], "recall": [0.6326219512195121, 0.6310975609756098, 0.6341463414634146, 0.6341463414634146, 0.6692073170731707, 0.6554878048780488, 0.6722560975609756, 0.6661585365853658, 0.6809160305343511, 0.6580152671755726, 0.6442748091603053, 0.6381679389312978], "f1_valid": [0.6322075611092707, 0.6278460170419013, 0.6313598840034508, 0.6255345584870438, 0.662979780330313, 0.6484420043403621, 0.6682331603250244, 0.661807562968862, 0.6738439540362523, 0.6563762000767177, 0.6394554176206233, 0.6332057788668751], "f1_train": [0.9129217496170094, 0.9088822467126785, 0.9112748474473045, 0.9124699755894168, 0.9044549407531174, 0.9097391882721145, 0.9069834167536308, 0.9103911680561285, 0.907723693549795, 0.9088545578215563, 0.9048833795314946, 0.9101267393375527], "time": [6.2264392375946045, 6.486033916473389, 5.898186922073364, 6.064702749252319, 6.285989761352539, 6.054095029830933, 6.390530109405518, 6.164612054824829, 6.358236312866211, 6.086646795272827, 6.112898111343384, 5.946452856063843]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 tok...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6554878048780488, 0.6676829268292683, 0.6661585365853658, 0.6600609756097561, 0.6676829268292683, 0.6798780487804879, 0.6432926829268293, 0.6646341463414634, 0.6274809160305344, 0.6519083969465649, 0.6534351145038167, 0.6427480916030535], "precision": [0.6397412837656741, 0.6580928382383239, 0.6585897967689893, 0.6544481439880188, 0.6547058947908047, 0.6821740465025947, 0.6411734086853064, 0.6638399463435926, 0.624919161483249, 0.6355900472278403, 0.6351567594188624, 0.6344729939210804], "recall": [0.6554878048780488, 0.6676829268292683, 0.6661585365853658, 0.6600609756097561, 0.6676829268292683, 0.6798780487804879, 0.6432926829268293, 0.6646341463414634, 0.6274809160305344, 0.6519083969465649, 0.6534351145038167, 0.6427480916030535], "f1_valid": [0.6411354467713362, 0.6477879396179402, 0.6502752300830724, 0.6486176601082422, 0.6536038244588207, 0.6543250770645179, 0.6206624449611636, 0.6642123392324977, 0.6260552257091697, 0.6292290773994306, 0.6319735078210362, 0.6356603600306618], "f1_train": [0.7105031930503241, 0.7035913130834455, 0.710962030753706, 0.7128334754272146, 0.7195900379663415, 0.6977830814592089, 0.7072090120264525, 0.721079284136444, 0.7155665557750696, 0.7014831854399962, 0.7114694827839575, 0.719281820044526], "time": [10.876425981521606, 10.93471884727478, 10.643610000610352, 10.868205785751343, 11.029269933700562, 11.184136867523193, 11.3258957862854, 11.198744773864746, 11.115578174591064, 11.142784118652344, 11.276742219924927, 11.143895149230957]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 tok...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6219512195121951, 0.635670731707317, 0.6554878048780488, 0.6234756097560976, 0.6463414634146342, 0.614329268292683, 0.6265243902439024, 0.6310975609756098, 0.6030534351145038, 0.6198473282442748, 0.6229007633587786, 0.648854961832061], "precision": [0.624302338561143, 0.6342933381292699, 0.6532841082317074, 0.6279496331548682, 0.6492009830770045, 0.6243517605212817, 0.6148787406445732, 0.6286468265071795, 0.6030534351145038, 0.6157905726017823, 0.6347850768734545, 0.6510735063541684], "recall": [0.6219512195121951, 0.635670731707317, 0.6554878048780488, 0.6234756097560976, 0.6463414634146342, 0.614329268292683, 0.6265243902439024, 0.6310975609756098, 0.6030534351145038, 0.6198473282442748, 0.6229007633587786, 0.648854961832061], "f1_valid": [0.6229838921765841, 0.6349455235400246, 0.6541661980536573, 0.6251954148303667, 0.6476229855141423, 0.6181713645564664, 0.6135580968250205, 0.6297798275309745, 0.6030534351145038, 0.6172998735983656, 0.6264363297602132, 0.6498685365686583], "f1_train": [0.9585840655808475, 0.9580273962759431, 0.9567785595170006, 0.956401799798261, 0.9545514438843891, 0.9559972444767741, 0.9523345537753924, 0.9547612637211829, 0.9595862521558622, 0.9577384530586458, 0.9579964497455283, 0.9581259617802885], "time": [6.093955039978027, 5.929711103439331, 5.938856840133667, 6.075307130813599, 6.1782920360565186, 6.057743072509766, 6.136750936508179, 6.016550779342651, 5.898504018783569, 6.091055870056152, 6.057707786560059, 6.053868055343628]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.625, 0.6219512195121951, 0.6128048780487805, 0.614329268292683, 0.6021341463414634, 0.6219512195121951, 0.6432926829268293, 0.6265243902439024, 0.6030534351145038, 0.6045801526717557, 0.6473282442748092, 0.650381679389313], "precision": [0.629891304347826, 0.6192718574993388, 0.613471844272, 0.6106144314119013, 0.6030173399390244, 0.6173325872987614, 0.6486824806254878, 0.6362826796461378, 0.6068903700178894, 0.6038191638782078, 0.6422919476008746, 0.652382189201889], "recall": [0.625, 0.6219512195121951, 0.6128048780487805, 0.614329268292683, 0.6021341463414634, 0.6219512195121951, 0.6432926829268293, 0.6265243902439024, 0.6030534351145038, 0.6045801526717557, 0.6473282442748092, 0.650381679389313], "f1_valid": [0.6269821784466076, 0.6204049173832863, 0.6131328166030293, 0.6121541500791745, 0.6025634276012368, 0.6188101220118721, 0.6456127305175491, 0.6303711859910746, 0.6046168742251766, 0.6041871687468444, 0.6424772573999419, 0.6512599968930669], "f1_train": [0.9468779506743048, 0.947558990452686, 0.9490102230829719, 0.9486801488187843, 0.9467306255831144, 0.9465975028742714, 0.9484239823108088, 0.944329204839443, 0.9496497557019666, 0.9492029822617554, 0.9465489952881251, 0.9491228937942715], "time": [22.18595790863037, 22.33533000946045, 22.597076177597046, 22.416406869888306, 22.628926038742065, 22.69020915031433, 22.287326097488403, 23.27734112739563, 22.76050090789795, 22.4242901802063, 22.95827579498291, 24.418368101119995]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.6753048780487805, 0.6722560975609756, 0.6554878048780488, 0.6448170731707317, 0.6463414634146342, 0.6432926829268293, 0.6661585365853658, 0.6295731707317073, 0.6564885496183206, 0.6778625954198473, 0.6702290076335878, 0.6641221374045801], "precision": [0.6746639195884147, 0.6714307477885905, 0.6519176310058501, 0.6424528024607381, 0.6493941852099212, 0.6443895342943454, 0.662815486971582, 0.6257924466993721, 0.6592898122025952, 0.6746837790300656, 0.6676226026732043, 0.6718352828618883], "recall": [0.6753048780487805, 0.6722560975609756, 0.6554878048780488, 0.6448170731707317, 0.6463414634146342, 0.6432926829268293, 0.6661585365853658, 0.6295731707317073, 0.6564885496183206, 0.6778625954198473, 0.6702290076335878, 0.6641221374045801], "f1_valid": [0.6749702590611073, 0.6718042927802279, 0.6532070279804005, 0.6434540078773644, 0.6477199022012422, 0.6438174634519518, 0.6641369687552837, 0.627155430722182, 0.6577668702529654, 0.6758119425749956, 0.6685229749212657, 0.6669457795094651], "f1_train": [0.8493027466043137, 0.8464346566512521, 0.8441588485575241, 0.8503366385967573, 0.846217812556754, 0.8482553384971834, 0.8457130259950801, 0.8473783165549432, 0.8482370403449322, 0.845520177947941, 0.8466914165274728, 0.8452818443977343], "time": [7.15778923034668, 7.391100883483887, 6.69370174407959, 7.427952766418457, 6.370194911956787, 7.015238046646118, 7.056560277938843, 6.458595037460327, 6.406102895736694, 6.141718864440918, 6.045704126358032, 6.269599914550781]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 CountVectorizer(analyzer='word', binary=True,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=False, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [[7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7212, 656], [7213, 655], [7213, 655], [7213, 655], [7213, 655]], "accuracy": [0.5335365853658537, 0.4954268292682927, 0.5198170731707317, 0.5167682926829268, 0.5060975609756098, 0.49847560975609756, 0.5640243902439024, 0.5487804878048781, 0.49923664122137407, 0.48091603053435117, 0.549618320610687, 0.46106870229007635], "precision": [0.5613300982532673, 0.5605373588093113, 0.5795446522383663, 0.5623754104127581, 0.5661031493385299, 0.5395915872229788, 0.6106316447779864, 0.6206712523719166, 0.5633904964790816, 0.5537970445487572, 0.6013037037037036, 0.5252350464722028], "recall": [0.5335365853658537, 0.4954268292682927, 0.5198170731707317, 0.5167682926829268, 0.5060975609756098, 0.49847560975609756, 0.5640243902439024, 0.5487804878048781, 0.49923664122137407, 0.48091603053435117, 0.549618320610687, 0.46106870229007635], "f1_valid": [0.529433821283651, 0.4885608246363584, 0.5206595757090263, 0.5119719230667668, 0.5057394710334823, 0.4983229387088391, 0.5634894508454287, 0.5491202141602296, 0.49292656245452626, 0.47985124290467807, 0.5478162940302789, 0.46021949503043397], "f1_train": [0.5629747949671496, 0.5622597498560454, 0.5637291629748256, 0.5611672543370313, 0.5619433988008692, 0.560770906307386, 0.5625488077370243, 0.5659260394005233, 0.5617007970551291, 0.5614903940710806, 0.5670673871453149, 0.5588660927092199], "time": [13.057651281356812, 11.65850019454956, 11.372889041900635, 13.232503890991211, 12.014699935913086, 12.744099855422974, 11.984843969345093, 12.724819898605347, 12.721034049987793, 12.68341326713562, 12.124797821044922, 12.640228033065796]}
