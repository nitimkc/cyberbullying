{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [898, 898, 898, 899, 899], "accuracy": [0.7244444444444444, 0.7244444444444444, 0.6355555555555555, 0.7008928571428571, 0.6383928571428571], "precision": [0.726320987654321, 0.7286464646464647, 0.6515555555555557, 0.6900441297208538, 0.6319286616161618], "recall": [0.7244444444444444, 0.7244444444444444, 0.6355555555555555, 0.7008928571428571, 0.6383928571428571], "f1_valid": [0.6998011559696342, 0.7072222222222222, 0.5993790849673203, 0.6801274300025113, 0.6025958466453674], "f1_train": [0.8209982257374039, 0.8057793919458842, 0.8077865977926761, 0.8136759555930768, 0.8029360510780321], "time": [1.6987838745117188, 1.7653357982635498, 1.6824383735656738, 1.7930784225463867, 1.6678040027618408]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [898, 898, 898, 899, 899], "accuracy": [0.6311111111111111, 0.6888888888888889, 0.7466666666666667, 0.6294642857142857, 0.6875], "precision": [0.6752420033670034, 0.6892705351721745, 0.7378953100537744, 0.6282721185064936, 0.6808474955277282], "recall": [0.6311111111111111, 0.6888888888888889, 0.7466666666666667, 0.6294642857142857, 0.6875], "f1_valid": [0.5771790148346683, 0.6566727053140097, 0.7375520282186949, 0.5971215260149687, 0.6661407199625993], "f1_train": [0.8284766525831138, 0.845846166827106, 0.8549478553485631, 0.8337729752635203, 0.8495627206795884], "time": [0.3783280849456787, 0.3749115467071533, 0.37491798400878906, 0.4218122959136963, 0.39057493209838867]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [898, 898, 898, 899, 899], "accuracy": [0.6311111111111111, 0.6177777777777778, 0.6711111111111111, 0.6785714285714286, 0.6607142857142857], "precision": [0.6265417088821345, 0.6187022222222223, 0.6552220056313625, 0.6785714285714286, 0.6592687074829932], "recall": [0.6311111111111111, 0.6177777777777778, 0.6711111111111111, 0.6785714285714286, 0.6607142857142857], "f1_valid": [0.6278131661182508, 0.6181914381914382, 0.6519187377711169, 0.6785714285714286, 0.6599389396046188], "f1_train": [0.9732162897528281, 0.960033687887175, 0.9606969444985235, 0.9777317886096107, 0.9766462625404078], "time": [1.7006475925445557, 1.8277127742767334, 1.812107801437378, 1.6246461868286133, 1.7027089595794678]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [898, 898, 898, 899, 899], "accuracy": [0.64, 0.6222222222222222, 0.64, 0.7008928571428571, 0.6473214285714286], "precision": [0.639103326369918, 0.6212068965517241, 0.6347222222222222, 0.7197375706708632, 0.647881838905775], "recall": [0.64, 0.6222222222222222, 0.64, 0.7008928571428571, 0.6473214285714286], "f1_valid": [0.6394378265831147, 0.6217025940811107, 0.6363327674023769, 0.70634374296224, 0.647588789956944], "f1_train": [1.0, 1.0, 1.0, 1.0, 1.0], "time": [0.35925936698913574, 0.37494802474975586, 0.3905048370361328, 0.35929346084594727, 0.3905673027038574]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [898, 898, 898, 899, 899], "accuracy": [0.6711111111111111, 0.7333333333333333, 0.6666666666666666, 0.6517857142857143, 0.6696428571428571], "precision": [0.6718970626255394, 0.7317948717948718, 0.657469342251951, 0.6451494917215712, 0.6591737060828704], "recall": [0.6711111111111111, 0.7333333333333333, 0.6666666666666666, 0.6517857142857143, 0.6696428571428571], "f1_valid": [0.6620300960630796, 0.7233649233649233, 0.6604532245237723, 0.6419170510870099, 0.6617261904761904], "f1_train": [0.9403406327262501, 0.9315471201793987, 0.9360808464968225, 0.9293094083559772, 0.9245543307908365], "time": [0.7498271465301514, 0.7029340267181396, 0.781069278717041, 0.7029643058776855, 0.7185850143432617]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [898, 898, 898, 899, 899], "accuracy": [0.5955555555555555, 0.5733333333333334, 0.6266666666666667, 0.5758928571428571, 0.7008928571428571], "precision": [0.5574190333831589, 0.7570104633781765, 0.6721351766513056, 0.7582013574660633, 0.6586438923395445], "recall": [0.5955555555555555, 0.5733333333333334, 0.6266666666666667, 0.5758928571428571, 0.7008928571428571], "f1_valid": [0.45244500667650567, 0.427047619047619, 0.5132956685499059, 0.4344915399320911, 0.632172373081464], "f1_train": [0.7072225691248357, 0.7100024187381441, 0.7190921146271334, 0.6702435595560695, 0.8242522175797052], "time": [0.3592965602874756, 0.3437008857727051, 0.3280513286590576, 0.34366822242736816, 0.34369444847106934]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [898, 898, 898, 899, 899], "accuracy": [0.3688888888888889, 0.4222222222222222, 0.3466666666666667, 0.41517857142857145, 0.41964285714285715], "precision": [0.136079012345679, 0.17827160493827163, 0.12017777777777777, 0.1723732461734694, 0.1761001275510204], "recall": [0.3688888888888889, 0.4222222222222222, 0.3466666666666667, 0.41517857142857145, 0.41964285714285715], "f1_valid": [0.1988167388167388, 0.25069444444444444, 0.1784818481848185, 0.24360635421360974, 0.24809074573225517], "f1_train": [0.6735184189322864, 0.6568364539579753, 0.6574332901866348, 0.6603991696237383, 0.6571895506275158], "time": [1.5777649879455566, 1.5777969360351562, 1.546515703201294, 1.6245877742767334, 1.499652624130249]}
