{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125]], "accuracy": [0.688, 0.672, 0.664, 0.68, 0.688, 0.632, 0.632, 0.672, 0.656, 0.64, 0.648, 0.696], "precision": [0.6711757575757576, 0.6396923076923078, 0.6415092924126172, 0.6579811912225705, 0.668739393939394, 0.6162181818181819, 0.5963341485827384, 0.6590454999286836, 0.6559015384615385, 0.5913072840203275, 0.5907959183673469, 0.6806141535383846], "recall": [0.688, 0.672, 0.664, 0.68, 0.688, 0.632, 0.632, 0.672, 0.656, 0.64, 0.648, 0.696], "f1_valid": [0.6787552106299791, 0.6548571428571429, 0.6519647384163892, 0.6651682539682541, 0.6763965103760626, 0.6157967846618674, 0.6135492063492063, 0.664091030789826, 0.644511105304416, 0.6130881424059719, 0.6179327827125863, 0.6873908430788046], "f1_train": [0.907047118829767, 0.9102377455986475, 0.9073707158041879, 0.9106205241934698, 0.9084260832228478, 0.9073331799401271, 0.9097932929614336, 0.9088248798901001, 0.904929945940486, 0.9148151935772788, 0.9134368758223437, 0.9077725304524359], "time": [1.6100013256072998, 1.8303723335266113, 2.8078014850616455, 2.5000040531158447, 1.9698665142059326, 1.8549325466156006, 2.004950761795044, 2.189580202102661, 2.214745044708252, 1.6749534606933594, 2.2248642444610596, 2.233187437057495]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125]], "accuracy": [0.68, 0.68, 0.672, 0.632, 0.656, 0.744, 0.648, 0.608, 0.656, 0.656, 0.552, 0.584], "precision": [0.656, 0.6608331053351573, 0.6139822514929953, 0.5974030197444831, 0.6237825059101655, 0.72653562294483, 0.6092682830282039, 0.5962481389578165, 0.6494538324420678, 0.6405882352941177, 0.5430869409660107, 0.5555518484484343], "recall": [0.68, 0.68, 0.672, 0.632, 0.656, 0.744, 0.648, 0.608, 0.656, 0.656, 0.552, 0.584], "f1_valid": [0.6668160272804774, 0.6691296774554196, 0.6413067143709004, 0.6132881048889292, 0.6392634920634921, 0.7345210848265035, 0.6263618796662276, 0.6000740312319259, 0.649756421285486, 0.6464171122994652, 0.5389819991597453, 0.5684776027185666], "f1_train": [0.9726339740880354, 0.9747764780532452, 0.9726958505911293, 0.9746203331796971, 0.9748601452758195, 0.9770342614969361, 0.9714700664171518, 0.9712059016263395, 0.9745783325319015, 0.9735647932313393, 0.9759523053036713, 0.971827696634018], "time": [3.244947910308838, 3.0800185203552246, 3.119878053665161, 3.1065797805786133, 3.424597978591919, 3.325052499771118, 3.659731864929199, 3.354900598526001, 2.9404046535491943, 3.0250961780548096, 3.2200639247894287, 3.245180130004883]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125], [1375, 125]], "accuracy": [0.544, 0.512, 0.56, 0.536, 0.608, 0.536, 0.584, 0.552, 0.568, 0.608, 0.544, 0.544], "precision": [0.5309742424242424, 0.49981726242371405, 0.5615387488328666, 0.5224827586206896, 0.5787415165479681, 0.546974358974359, 0.6052799999999999, 0.5400342405618964, 0.619978835978836, 0.6341375661375662, 0.5861543859649123, 0.5669942857142858], "recall": [0.544, 0.512, 0.56, 0.536, 0.608, 0.536, 0.584, 0.552, 0.568, 0.608, 0.544, 0.544], "f1_valid": [0.5200835532891627, 0.4900753085582531, 0.5440122788010112, 0.5111272568306011, 0.5907281474506971, 0.5301913429522752, 0.5693155085093763, 0.5314981366459628, 0.552103547716451, 0.5862375437184142, 0.522679439282179, 0.5294173080456485], "f1_train": [0.6473169944237164, 0.6509200539753165, 0.6575265972529003, 0.6642168145221733, 0.6548543503036867, 0.66532346173281, 0.6427301412483158, 0.6759132982143287, 0.6309316130039964, 0.6490630610905542, 0.6574721062347568, 0.6501190404778077], "time": [1.1699638366699219, 0.980266809463501, 0.901756763458252, 0.8449876308441162, 1.0700082778930664, 1.510132074356079, 1.2000608444213867, 1.5402019023895264, 1.4047715663909912, 1.1250617504119873, 0.8900229930877686, 1.5999791622161865]}
