{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220]], "accuracy": [0.6181818181818182, 0.6090909090909091, 0.5727272727272728, 0.5545454545454546, 0.5636363636363636, 0.5818181818181818, 0.6045454545454545, 0.5863636363636363, 0.6318181818181818, 0.6409090909090909], "precision": [0.5830736600860441, 0.5641369013854413, 0.49524667405764966, 0.4855259916235526, 0.49714766751498524, 0.5422307282724447, 0.4841444270015699, 0.5216175653299874, 0.5909610920234005, 0.5690508263465168], "recall": [0.6181818181818182, 0.6090909090909091, 0.5727272727272728, 0.5545454545454546, 0.5636363636363636, 0.5818181818181818, 0.6045454545454545, 0.5863636363636363, 0.6318181818181818, 0.6409090909090909], "f1_valid": [0.5677540303699243, 0.5381017426925196, 0.5060551702730741, 0.49096180987961807, 0.5132043578589846, 0.523260863608965, 0.5335624985530927, 0.5211705455556231, 0.5911095682626784, 0.5763306895125077], "f1_train": [0.8156233604855734, 0.8224993499586818, 0.8114149392471344, 0.8102388942470026, 0.8141561463823451, 0.8237477693654367, 0.8162480367940629, 0.8214299569798614, 0.817737230313177, 0.8235481697505364], "time": [2.4213082790374756, 2.249476194381714, 2.3119595050811768, 2.4056830406188965, 2.1713669300079346, 2.5150351524353027, 2.421309471130371, 2.327547073364258, 2.5619022846221924, 2.593141555786133]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220]], "accuracy": [0.5818181818181818, 0.6136363636363636, 0.6045454545454545, 0.5772727272727273, 0.6272727272727273, 0.5545454545454546, 0.5954545454545455, 0.5681818181818182, 0.5954545454545455, 0.5772727272727273], "precision": [0.5744776355014785, 0.5533247890652582, 0.5427785155933934, 0.5198772506563205, 0.5507211617375855, 0.493932733932734, 0.5837615908278394, 0.49232915394114957, 0.5471103662044308, 0.537999881859531], "recall": [0.5818181818181818, 0.6136363636363636, 0.6045454545454545, 0.5772727272727273, 0.6272727272727273, 0.5545454545454546, 0.5954545454545455, 0.5681818181818182, 0.5954545454545455, 0.5772727272727273], "f1_valid": [0.5516160481927083, 0.5642711274233013, 0.5615017821128776, 0.5155043001392333, 0.5851683684084711, 0.5181140360905895, 0.5411654344807555, 0.5215397083397028, 0.5623581378592172, 0.5277895955209417], "f1_train": [0.9638653780607154, 0.9575754334691748, 0.9608790037500767, 0.9599119116070055, 0.964452257451991, 0.9634005917338873, 0.9595574141641746, 0.9621260545683076, 0.9641627082812445, 0.9605257650796479], "time": [5.420614242553711, 5.373739719390869, 5.311251163482666, 5.311253309249878, 5.326860427856445, 5.592467546463013, 5.717409133911133, 5.29567813873291, 5.561191082000732, 5.358121395111084]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220], [1980, 220]], "accuracy": [0.4590909090909091, 0.4590909090909091, 0.4772727272727273, 0.4863636363636364, 0.39090909090909093, 0.42272727272727273, 0.45, 0.42272727272727273, 0.4090909090909091, 0.45], "precision": [0.45752268963362736, 0.46548254281949936, 0.5152695789059425, 0.47059066040375536, 0.4001193224298477, 0.42558657712556475, 0.45695437008422574, 0.42564075188282374, 0.5121170432982577, 0.4538441558441558], "recall": [0.4590909090909091, 0.4590909090909091, 0.4772727272727273, 0.4863636363636364, 0.39090909090909093, 0.42272727272727273, 0.45, 0.42272727272727273, 0.4090909090909091, 0.45], "f1_valid": [0.44881236342086744, 0.45008618490144664, 0.4637035652553717, 0.4773190235690236, 0.3806886134334041, 0.4113082203197626, 0.435487012987013, 0.41240712200442, 0.41770599072485864, 0.44450613145664536], "f1_train": [0.5754838793955676, 0.5761029567547553, 0.5771953111705619, 0.5756302687938007, 0.5846813071927429, 0.5879830774504533, 0.5663074373990051, 0.5730942389576036, 0.568589310279486, 0.57167450113713], "time": [1.2341108322143555, 1.1715998649597168, 1.1872365474700928, 1.1716043949127197, 1.1403536796569824, 1.2184312343597412, 1.140354871749878, 1.3278110027313232, 1.140357494354248, 1.1872222423553467]}
