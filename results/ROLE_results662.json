{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.5294117647058824, 0.5588235294117647, 0.5176470588235295, 0.5882352941176471, 0.5176470588235295, 0.5764705882352941, 0.5411764705882353, 0.5882352941176471, 0.4647058823529412, 0.5764705882352941, 0.6035502958579881, 0.6035502958579881], "precision": [0.47084537036078294, 0.492638820233976, 0.4813377037562013, 0.5219716593437822, 0.47287865681031765, 0.49859909171100214, 0.5509803921568628, 0.5741141218916479, 0.43875151208259483, 0.5331148121899363, 0.52097506701057, 0.5505020095685046], "recall": [0.5294117647058824, 0.5588235294117647, 0.5176470588235295, 0.5882352941176471, 0.5176470588235295, 0.5764705882352941, 0.5411764705882353, 0.5882352941176471, 0.4647058823529412, 0.5764705882352941, 0.6035502958579881, 0.6035502958579881], "f1": [0.47140939099667734, 0.48720609807698906, 0.44435998670870896, 0.5322719678801434, 0.4472090775021548, 0.5111137151048749, 0.4685208629082043, 0.5436262465674231, 0.3914349276974416, 0.5228282851726391, 0.5346011028527445, 0.5501369820466595], "time": [0.9840676784515381, 0.9314687252044678, 1.0503814220428467, 0.900775671005249, 0.8839643001556396, 0.9780681133270264, 0.5011184215545654, 0.93324875831604, 1.0012500286102295, 0.9495751857757568, 0.8501110076904297, 2.100985288619995]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.5294117647058824, 0.5882352941176471, 0.5764705882352941, 0.6235294117647059, 0.49411764705882355, 0.5, 0.6058823529411764, 0.6352941176470588, 0.611764705882353, 0.5882352941176471, 0.5680473372781065, 0.5739644970414202], "precision": [0.522644823207483, 0.5128043827305439, 0.5462582211568012, 0.5877878534781052, 0.4389456770455481, 0.4636967955227566, 0.5241455239665477, 0.5863390573620753, 0.5729727226987767, 0.533112951062778, 0.5418683880222341, 0.5189052768628365], "recall": [0.5294117647058824, 0.5882352941176471, 0.5764705882352941, 0.6235294117647059, 0.49411764705882355, 0.5, 0.6058823529411764, 0.6352941176470588, 0.611764705882353, 0.5882352941176471, 0.5680473372781065, 0.5739644970414202], "f1": [0.5049469032649038, 0.5437033342933801, 0.535562101927427, 0.5968169671548895, 0.44910622055357907, 0.47027330775151893, 0.5600282738841488, 0.6073415435813635, 0.5790721376954916, 0.5429799804728248, 0.5230070272459147, 0.5403956040385763], "time": [0.7224118709564209, 1.7164366245269775, 1.8426010608673096, 1.2788331508636475, 1.4711716175079346, 1.725651502609253, 1.567913293838501, 1.8157446384429932, 1.6167676448822021, 1.7836613655090332, 1.675112247467041, 0.7095785140991211]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.43529411764705883, 0.37058823529411766, 0.4411764705882353, 0.4411764705882353, 0.47058823529411764, 0.4764705882352941, 0.4235294117647059, 0.5294117647058824, 0.5235294117647059, 0.4470588235294118, 0.46153846153846156, 0.4319526627218935], "precision": [0.48080755776489004, 0.375645278159669, 0.46499458157692747, 0.46862596421262526, 0.5062856593617839, 0.48067745643175874, 0.4080121728589003, 0.5745855839076177, 0.6116746209725337, 0.4549276861908626, 0.45659431458248023, 0.4287180376021113], "recall": [0.43529411764705883, 0.37058823529411766, 0.4411764705882353, 0.4411764705882353, 0.47058823529411764, 0.4764705882352941, 0.4235294117647059, 0.5294117647058824, 0.5235294117647059, 0.4470588235294118, 0.46153846153846156, 0.4319526627218935], "f1": [0.4048930307321367, 0.3477988800354088, 0.4319102310867018, 0.42729620322409695, 0.44857592836331317, 0.46485926336200567, 0.4043947826439508, 0.5208392962860775, 0.5095543116708593, 0.42413116028772824, 0.45459217500232335, 0.40054590407368196], "time": [0.3162398338317871, 0.21648168563842773, 0.23514223098754883, 0.24892020225524902, 0.23302173614501953, 0.26720595359802246, 0.23197460174560547, 0.3000810146331787, 0.28212547302246094, 0.36702990531921387, 0.3151390552520752, 0.230576753616333]}
