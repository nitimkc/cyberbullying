{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.5975609756097561, 0.6073170731707317, 0.6, 0.6073170731707317, 0.5926829268292683, 0.6121951219512195, 0.6268292682926829, 0.5672371638141809, 0.6210268948655256, 0.5916870415647921, 0.6014669926650367, 0.5916870415647921], "precision": [0.6199637630662022, 0.6045569620253164, 0.7294236602628918, 0.6585266792615706, 0.6053964962238455, 0.6648436522826767, 0.6409684485445891, 0.5992081422956358, 0.6959151990709391, 0.6869080150397567, 0.6845824736266598, 0.6273202830603183], "recall": [0.5975609756097561, 0.6073170731707317, 0.6, 0.6073170731707317, 0.5926829268292683, 0.6121951219512195, 0.6268292682926829, 0.5672371638141809, 0.6210268948655256, 0.5916870415647921, 0.6014669926650367, 0.5916870415647921], "f1": [0.5096845206539521, 0.4895214356060275, 0.4967774580184055, 0.5103511952825023, 0.47374404286151994, 0.5051170244552796, 0.5289106659953597, 0.4399286216928589, 0.5196167716398323, 0.4758866162453976, 0.4944201088847667, 0.4834283862127368], "time": [3.879927158355713, 2.700082540512085, 2.6896350383758545, 3.061880588531494, 3.450056552886963, 3.3450686931610107, 2.9599578380584717, 2.802227020263672, 3.0200867652893066, 3.1057016849517822, 3.2901320457458496, 2.988936424255371]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.5926829268292683, 0.6121951219512195, 0.5902439024390244, 0.5756097560975609, 0.6048780487804878, 0.5951219512195122, 0.6463414634146342, 0.6063569682151589, 0.589242053789731, 0.6063569682151589, 0.6161369193154034, 0.60880195599022], "precision": [0.6415082884595079, 0.6434515302471567, 0.6726915624078521, 0.7082976478619748, 0.7193518057629557, 0.6345643142951805, 0.663033202627274, 0.6072233718579099, 0.6199724413749065, 0.6781950644902647, 0.7069166755708639, 0.6886225635665715], "recall": [0.5926829268292683, 0.6121951219512195, 0.5902439024390244, 0.5756097560975609, 0.6048780487804878, 0.5951219512195122, 0.6463414634146342, 0.6063569682151589, 0.589242053789731, 0.6063569682151589, 0.6161369193154034, 0.60880195599022], "f1": [0.4706746523075135, 0.5245449137418203, 0.48582548954422855, 0.4573408299018056, 0.4938415055900502, 0.5043183992540025, 0.5514936937597228, 0.5021854876593838, 0.4787223169993474, 0.49672187646977795, 0.5106232133109482, 0.4945161656388533], "time": [0.25113797187805176, 0.27144360542297363, 0.2899625301361084, 0.333111047744751, 0.26479244232177734, 0.26476120948791504, 0.2599780559539795, 0.2548987865447998, 0.2597646713256836, 0.2552614212036133, 0.27525949478149414, 0.2700018882751465]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6390243902439025, 0.6341463414634146, 0.6195121951219512, 0.6439024390243903, 0.6121951219512195, 0.624390243902439, 0.6536585365853659, 0.6381418092909535, 0.6234718826405868, 0.6381418092909535, 0.589242053789731, 0.6430317848410758], "precision": [0.6555602475427741, 0.6152520325203252, 0.6615744030378177, 0.6452302476692722, 0.6965916078182935, 0.6474429583005509, 0.699320435439244, 0.632091558343045, 0.6603345591630508, 0.645101292350681, 0.598680993437138, 0.6328670938310219], "recall": [0.6390243902439025, 0.6341463414634146, 0.6195121951219512, 0.6439024390243903, 0.6121951219512195, 0.624390243902439, 0.6536585365853659, 0.6381418092909535, 0.6234718826405868, 0.6381418092909535, 0.589242053789731, 0.6430317848410758], "f1": [0.5983813449804518, 0.5816919658422102, 0.5779019462921902, 0.6036990254085675, 0.5876366240819675, 0.5711788617886179, 0.6085689094540392, 0.6006269514548355, 0.6294186463666123, 0.6013386468500728, 0.5907931057323855, 0.6286351626355556], "time": [3.0800557136535645, 2.9162070751190186, 2.980043888092041, 2.985609292984009, 2.9748148918151855, 2.955118179321289, 3.295097589492798, 3.417736053466797, 3.1000168323516846, 3.095346689224243, 3.304898977279663, 2.9849612712860107]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6463414634146342, 0.6268292682926829, 0.6902439024390243, 0.6390243902439025, 0.6390243902439025, 0.6317073170731707, 0.6682926829268293, 0.6577017114914425, 0.6308068459657702, 0.6503667481662592, 0.6797066014669927, 0.6552567237163814], "precision": [0.6411106694107997, 0.6227691758039925, 0.6862846533475094, 0.6353603805315898, 0.6334272393159797, 0.6199485241010954, 0.6662828786561247, 0.6549802495773064, 0.620110128226948, 0.6539951546817608, 0.6855529047646041, 0.6455633125800477], "recall": [0.6463414634146342, 0.6268292682926829, 0.6902439024390243, 0.6390243902439025, 0.6390243902439025, 0.6317073170731707, 0.6682926829268293, 0.6577017114914425, 0.6308068459657702, 0.6503667481662592, 0.6797066014669927, 0.6552567237163814], "f1": [0.6420097557304477, 0.6206543794679802, 0.6856192239433002, 0.6341583848388356, 0.6336154996519521, 0.6037670680425342, 0.6606460184725128, 0.6385848126714604, 0.6189255113470247, 0.634257316273107, 0.6647968747387685, 0.641249758620439], "time": [0.3198690414428711, 0.44005751609802246, 0.3099238872528076, 0.280592679977417, 0.32965803146362305, 0.2552800178527832, 0.2500448226928711, 0.25972437858581543, 0.259890079498291, 0.2519955635070801, 0.2997934818267822, 0.26982927322387695]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6585365853658537, 0.6439024390243903, 0.6463414634146342, 0.6317073170731707, 0.6414634146341464, 0.6634146341463415, 0.6439024390243903, 0.6577017114914425, 0.6185819070904646, 0.6601466992665037, 0.6601466992665037, 0.6381418092909535], "precision": [0.6648898939320519, 0.6539338409257109, 0.6397936481777946, 0.6326641875097094, 0.6473919068736143, 0.6647072990995201, 0.6350441458236238, 0.6572717503909417, 0.6316218418907905, 0.6618363249647167, 0.6531173594132029, 0.6327105838784275], "recall": [0.6585365853658537, 0.6439024390243903, 0.6463414634146342, 0.6317073170731707, 0.6414634146341464, 0.6634146341463415, 0.6439024390243903, 0.6577017114914425, 0.6185819070904646, 0.6601466992665037, 0.6601466992665037, 0.6381418092909535], "f1": [0.6389602053915276, 0.6081999491869918, 0.6069317429392459, 0.6043389615244968, 0.6049701682787578, 0.6474428970628437, 0.6186680226812118, 0.6314498821816397, 0.5989622343714103, 0.6269654776643692, 0.6404938855107459, 0.6173009758452407], "time": [0.38009071350097656, 0.44492387771606445, 0.39995598793029785, 0.36982226371765137, 0.5188665390014648, 0.39463114738464355, 0.3947458267211914, 0.4098024368286133, 0.419691801071167, 0.4303417205810547, 0.38520336151123047, 0.4601120948791504]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5609756097560976, 0.6121951219512195, 0.6292682926829268, 0.5560975609756098, 0.5951219512195122, 0.5634146341463414, 0.5975609756097561, 0.5819070904645477, 0.5672371638141809, 0.5501222493887531, 0.5745721271393643, 0.5574572127139364], "precision": [0.31469363474122547, 0.7633249641319944, 0.7670463355000299, 0.309244497323022, 0.5568866571018651, 0.31743604997025576, 0.7599141272586319, 0.757137206961024, 0.3217580000119559, 0.30263448927254144, 0.3301331292854538, 0.7542637102539302], "recall": [0.5609756097560976, 0.6121951219512195, 0.6292682926829268, 0.5560975609756098, 0.5951219512195122, 0.5634146341463414, 0.5975609756097561, 0.5819070904645477, 0.5672371638141809, 0.5501222493887531, 0.5745721271393643, 0.5574572127139364], "f1": [0.4032012195121951, 0.46997718603120986, 0.48861550812770316, 0.39746157963147033, 0.4484231489992376, 0.4060804383394847, 0.4495954119475538, 0.43069897595800366, 0.41060537287017146, 0.3904653189667806, 0.41933058968245535, 0.40424040072703443], "time": [0.21752572059631348, 0.2461841106414795, 0.27001023292541504, 0.24487543106079102, 0.2557082176208496, 0.23224139213562012, 0.3000633716583252, 0.2786567211151123, 0.24489235877990723, 0.23998332023620605, 0.3301379680633545, 0.26500654220581055]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5829268292682926, 0.5756097560975609, 0.5536585365853659, 0.5878048780487805, 0.5317073170731708, 0.5560975609756098, 0.573170731707317, 0.5501222493887531, 0.5599022004889975, 0.5745721271393643, 0.6039119804400978, 0.46943765281173594], "precision": [0.5772265748732974, 0.5942425466516229, 0.5475080178967374, 0.5828850132110495, 0.5276270214805644, 0.5503887174014359, 0.5694527067221892, 0.5501222493887531, 0.5529591904164032, 0.5679309082277937, 0.599826544263505, 0.5851633540287354], "recall": [0.5829268292682926, 0.5756097560975609, 0.5536585365853659, 0.5878048780487805, 0.5317073170731708, 0.5560975609756098, 0.573170731707317, 0.5501222493887531, 0.5599022004889975, 0.5745721271393643, 0.6039119804400978, 0.46943765281173594], "f1": [0.5782448409252317, 0.579480442845903, 0.5496494399909033, 0.5840863250188636, 0.5284098015805333, 0.5516537324464154, 0.5705488822312174, 0.5501222493887531, 0.5553683627808276, 0.569601678532318, 0.6006983143062945, 0.43328333386920825], "time": [3.154996633529663, 3.4166829586029053, 3.232489824295044, 3.114755392074585, 3.016509532928467, 3.149998903274536, 3.079998254776001, 3.039742946624756, 3.0298855304718018, 3.271533966064453, 3.030820846557617, 3.420823574066162]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.5723014256619144, 0.6374745417515275, 0.5824847250509165, 0.6537678207739308, 0.594704684317719], "precision": [0.65998314488377, 0.6958539746478747, 0.6301536109847096, 0.697870151838947, 0.6122287630722646], "recall": [0.5723014256619144, 0.6374745417515275, 0.5824847250509165, 0.6537678207739308, 0.594704684317719], "f1": [0.4622799430851212, 0.5558152166743168, 0.4775499322151084, 0.572196460292904, 0.5028122537300797], "time": [8.565296411514282, 2.774380922317505, 2.7008655071258545, 5.316205739974976, 5.299806356430054]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6109979633401222, 0.5906313645621182, 0.5885947046843177, 0.6537678207739308, 0.5987780040733197], "precision": [0.6585020434037846, 0.6469878828002472, 0.6900085973168909, 0.7088316440107991, 0.5992308929145674], "recall": [0.6109979633401222, 0.5906313645621182, 0.5885947046843177, 0.6537678207739308, 0.5987780040733197], "f1": [0.5215946695143668, 0.4819963072271325, 0.4685683919750919, 0.5786147510535551, 0.5025535923391117], "time": [0.6663329601287842, 0.6664774417877197, 0.6148059368133545, 0.6500415802001953, 0.6509218215942383]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6456211812627292, 0.6191446028513238, 0.6354378818737271, 0.6537678207739308, 0.6537678207739308], "precision": [0.6598402582897689, 0.6374912661483343, 0.6395685434861187, 0.6469289589878799, 0.6640721098586437], "recall": [0.6456211812627292, 0.6191446028513238, 0.6354378818737271, 0.6537678207739308, 0.6537678207739308], "f1": [0.6206159468565011, 0.5694200139555896, 0.6369146826560602, 0.6240317902100965, 0.6135408450372603], "time": [5.182286500930786, 5.357431650161743, 5.242946147918701, 5.1589930057525635, 5.112420082092285]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.659877800407332, 0.6456211812627292, 0.6313645621181263, 0.6802443991853361, 0.6741344195519349], "precision": [0.6504264933646869, 0.6402471345428936, 0.6269573623786185, 0.6828384234942414, 0.6715517565018255], "recall": [0.659877800407332, 0.6456211812627292, 0.6313645621181263, 0.6802443991853361, 0.6741344195519349], "f1": [0.6493744865454204, 0.624078797636946, 0.6174696760657764, 0.673227745111004, 0.6641999172993973], "time": [0.6162164211273193, 0.618412971496582, 0.2755725383758545, 0.6341667175292969, 0.2694132328033447]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.659877800407332, 0.6171079429735234, 0.659877800407332, 0.7107942973523421, 0.6211812627291242], "precision": [0.6639422081757488, 0.616053899433004, 0.6582061882365332, 0.7120364910859296, 0.6470520248997964], "recall": [0.659877800407332, 0.6171079429735234, 0.659877800407332, 0.7107942973523421, 0.6211812627291242], "f1": [0.6318327477171454, 0.5927395333335572, 0.6367147189621788, 0.6903279762316954, 0.5865714841191494], "time": [0.43219923973083496, 0.4600706100463867, 0.4175548553466797, 1.0677669048309326, 0.4311513900756836]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5723014256619144, 0.5865580448065173, 0.5641547861507128, 0.594704684317719, 0.570264765784114], "precision": [0.7571094516104699, 0.3440503399272444, 0.754503512199177, 0.7599787987045508, 0.3270293860925225], "recall": [0.5723014256619144, 0.5865580448065173, 0.5641547861507128, 0.594704684317719, 0.570264765784114], "f1": [0.42710740355752147, 0.4337065902548832, 0.40913175383377925, 0.449888928931202, 0.41567937531864074], "time": [0.24966835975646973, 0.23380827903747559, 0.23438596725463867, 0.6235930919647217, 0.2490239143371582]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002293B8025E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5315682281059063, 0.5254582484725051, 0.6558044806517311, 0.6089613034623218, 0.570264765784114], "precision": [0.5379398697130428, 0.5438581605834482, 0.6489724379383461, 0.6248342505524982, 0.5647274685731616], "recall": [0.5315682281059063, 0.5254582484725051, 0.6558044806517311, 0.6089613034623218, 0.570264765784114], "f1": [0.5334936118299823, 0.5303000029545315, 0.6466631553861292, 0.6110698199328018, 0.5649164878849257], "time": [2.7675647735595703, 2.8014392852783203, 2.785398483276367, 2.7334442138671875, 2.802917957305908]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.5784114052953157, 0.6089613034623218, 0.6069246435845214, 0.5926680244399185, 0.6415478615071283], "precision": [0.660032356007673, 0.6851730434814176, 0.6537204142962417, 0.6306008146639511, 0.6334426189725914], "recall": [0.5784114052953157, 0.6089613034623218, 0.6069246435845214, 0.5926680244399185, 0.6415478615071283], "f1": [0.4621435028353356, 0.5337255808058602, 0.5155285878131689, 0.46927579310471357, 0.5728364834770706], "time": [3.88970685005188, 2.6868960857391357, 2.421271800994873, 2.784644365310669, 2.561863660812378]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6089613034623218, 0.5987780040733197, 0.5926680244399185, 0.604887983706721, 0.6130346232179226], "precision": [0.6547149803742032, 0.6311796086091614, 0.7165189893172897, 0.6111065446091429, 0.6507817405019678], "recall": [0.6089613034623218, 0.5987780040733197, 0.5926680244399185, 0.604887983706721, 0.6130346232179226], "f1": [0.51794874010017, 0.48567687464419945, 0.48717863800317474, 0.5216337796178201, 0.5203133034330436], "time": [0.26459312438964844, 0.3123908042907715, 0.23428869247436523, 0.2655611038208008, 0.26555824279785156]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6496945010183299, 0.639511201629328, 0.6334012219959266, 0.639511201629328, 0.6334012219959266], "precision": [0.6409454801817327, 0.6449913994980816, 0.6290614262719757, 0.6412948006648407, 0.6454160667259606], "recall": [0.6496945010183299, 0.639511201629328, 0.6334012219959266, 0.639511201629328, 0.6334012219959266], "f1": [0.6123829150710611, 0.6413012082660694, 0.6226352504790083, 0.613288938265343, 0.6037748065286705], "time": [2.4212684631347656, 2.62434720993042, 2.686832904815674, 2.483754873275757, 2.46813702583313]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6680244399185336, 0.6680244399185336, 0.6435845213849287, 0.6476578411405295, 0.6272912423625254], "precision": [0.6682650458708297, 0.6624092849143766, 0.6392103245159081, 0.6549678917296026, 0.6182602691949779], "recall": [0.6680244399185336, 0.6680244399185336, 0.6435845213849287, 0.6476578411405295, 0.6272912423625254], "f1": [0.6509382107818908, 0.6541276402207309, 0.6241408448711501, 0.6309985929275004, 0.6104291626701618], "time": [0.2659289836883545, 0.25025439262390137, 0.23461484909057617, 0.28157758712768555, 0.2659339904785156]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6619144602851323, 0.6802443991853361, 0.659877800407332, 0.6496945010183299, 0.659877800407332], "precision": [0.6655389410072144, 0.6879464844454775, 0.6582829080338709, 0.6610336439543496, 0.6688295216446728], "recall": [0.6619144602851323, 0.6802443991853361, 0.659877800407332, 0.6496945010183299, 0.659877800407332], "f1": [0.632707043480793, 0.6633180821465913, 0.6288596391121712, 0.6162143623627757, 0.6435074633421709], "time": [0.40675878524780273, 0.39107656478881836, 0.39107704162597656, 0.3910799026489258, 0.3749208450317383]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5743380855397149, 0.560081466395112, 0.5987780040733197, 0.5967413441955194, 0.5580448065173116], "precision": [0.6490034167374131, 0.31369124899929896, 0.7624234144202267, 0.6584224459155977, 0.3114140060809437], "recall": [0.5743380855397149, 0.560081466395112, 0.5987780040733197, 0.5967413441955194, 0.5580448065173116], "f1": [0.42686580454796197, 0.40214726699387937, 0.4646282709956653, 0.45381512622640774, 0.39974974375357747], "time": [0.21873021125793457, 0.21869611740112305, 0.23977875709533691, 0.23432278633117676, 0.30077338218688965]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5824847250509165, 0.6211812627291242, 0.5641547861507128, 0.5723014256619144, 0.6069246435845214], "precision": [0.598020657004314, 0.629744446650592, 0.5519957328172864, 0.5841830313214098, 0.6033895444080405], "recall": [0.5824847250509165, 0.6211812627291242, 0.5641547861507128, 0.5723014256619144, 0.6069246435845214], "f1": [0.5883012501210646, 0.6204203552389221, 0.5501392895553358, 0.5727272506064963, 0.6045927246282398], "time": [2.771981954574585, 2.847550630569458, 3.067310094833374, 3.1375365257263184, 3.204725980758667]}
