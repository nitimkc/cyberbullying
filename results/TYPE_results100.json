{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[91, 9], [91, 9], [91, 9], [91, 9], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8]], "accuracy": [0.4444444444444444, 0.5555555555555556, 0.3333333333333333, 0.1111111111111111, 0.5, 0.5, 0.375, 0.375, 0.375, 0.375, 0.625, 0.125], "precision": [0.19753086419753085, 0.30864197530864196, 0.1111111111111111, 0.013888888888888888, 0.2857142857142857, 0.25, 0.140625, 0.25, 0.140625, 0.140625, 0.5357142857142857, 0.015625], "recall": [0.4444444444444444, 0.5555555555555556, 0.3333333333333333, 0.1111111111111111, 0.5, 0.5, 0.375, 0.375, 0.375, 0.375, 0.625, 0.125], "f1_valid": [0.2735042735042735, 0.39682539682539686, 0.16666666666666666, 0.024691358024691357, 0.36363636363636365, 0.3333333333333333, 0.20454545454545453, 0.3, 0.20454545454545453, 0.20454545454545453, 0.5769230769230769, 0.027777777777777776], "f1_train": [0.8417059131344846, 0.8571428571428571, 0.8415537950421672, 0.8412698412698413, 0.8283887468030692, 0.8434265010351968, 0.8432760364004046, 0.8283887468030692, 0.8432760364004046, 0.8584398976982096, 0.8287707997852926, 0.8429951690821256], "time": [1.4657609462738037, 0.07533073425292969, 0.09877181053161621, 0.08977389335632324, 0.08509349822998047, 0.0851590633392334, 0.07529139518737793, 0.08490896224975586, 0.08518695831298828, 0.07474875450134277, 0.07999992370605469, 0.09492325782775879]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[91, 9], [91, 9], [91, 9], [91, 9], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8]], "accuracy": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.5, 0.625, 0.375, 0.25, 0.5, 0.625, 0.25, 0.75], "precision": [0.2074074074074074, 0.18518518518518517, 0.2222222222222222, 0.7407407407407407, 0.5357142857142857, 0.5357142857142857, 0.1607142857142857, 0.08333333333333333, 0.3125, 0.625, 0.15, 0.6964285714285714], "recall": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.5, 0.625, 0.375, 0.25, 0.5, 0.625, 0.25, 0.75], "f1_valid": [0.25555555555555554, 0.23703703703703705, 0.26666666666666666, 0.6333333333333333, 0.4125, 0.5769230769230769, 0.22499999999999998, 0.125, 0.375, 0.625, 0.16666666666666669, 0.6874999999999999], "f1_train": [0.9673147365455057, 0.9673147365455057, 0.9872527472527471, 0.9673221515326779, 0.9837662337662337, 0.9676932367149758, 0.967670011148272, 0.9873876352962759, 0.967670011148272, 0.983768115942029, 0.9676564156945918, 0.9676850763807286], "time": [0.05504107475280762, 0.05546212196350098, 0.06538152694702148, 0.05467581748962402, 0.05481672286987305, 0.06506848335266113, 0.07020926475524902, 0.05983567237854004, 0.06508755683898926, 0.05857586860656738, 0.05987906455993652, 0.059922218322753906]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[91, 9], [91, 9], [91, 9], [91, 9], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8], [92, 8]], "accuracy": [0.3333333333333333, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.25, 0.375, 0.375, 0.625, 0.625, 0.5, 0.25, 0.5], "precision": [0.2222222222222222, 0.5555555555555556, 0.5185185185185185, 0.5333333333333333, 0.25, 0.5, 0.3333333333333333, 0.7083333333333333, 0.9, 0.425, 0.125, 0.7875], "recall": [0.3333333333333333, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.25, 0.375, 0.375, 0.625, 0.625, 0.5, 0.25, 0.5], "f1_valid": [0.25925925925925924, 0.4074074074074074, 0.4761904761904763, 0.5166666666666667, 0.25, 0.42857142857142855, 0.35, 0.5875, 0.6444444444444445, 0.45833333333333326, 0.16666666666666666, 0.43452380952380953], "f1_train": [0.5274236138693971, 0.5060177917320775, 0.5918175941705353, 0.550221695761301, 0.5592622571692877, 0.5436755243522083, 0.5401918359002317, 0.5619805799204897, 0.5580434782608696, 0.541970365935793, 0.5440623738975039, 0.5587980065180489], "time": [0.053861379623413086, 0.060211181640625, 0.04792428016662598, 0.05007147789001465, 0.04985189437866211, 0.04868817329406738, 0.06048297882080078, 0.05993151664733887, 0.05529499053955078, 0.07023382186889648, 0.06003212928771973, 0.04969048500061035]}
