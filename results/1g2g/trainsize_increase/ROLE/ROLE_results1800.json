{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180]], "accuracy": [0.5611111111111111, 0.5833333333333334, 0.5277777777777778, 0.5666666666666667, 0.6333333333333333, 0.5722222222222222, 0.5833333333333334, 0.5777777777777777, 0.6111111111111112, 0.6111111111111112], "precision": [0.5249395424836601, 0.5125611039080992, 0.4510407352210631, 0.5024499992241929, 0.6174425874759455, 0.4599794916868087, 0.5648029591691563, 0.4971189547943934, 0.5684050741002911, 0.5444924223704802], "recall": [0.5611111111111111, 0.5833333333333334, 0.5277777777777778, 0.5666666666666667, 0.6333333333333333, 0.5722222222222222, 0.5833333333333334, 0.5777777777777777, 0.6111111111111112, 0.6111111111111112], "f1_valid": [0.49081942996176575, 0.5169659352992686, 0.4688664183846842, 0.5034538057028877, 0.597593071107283, 0.4907137044503434, 0.5284097761327554, 0.516524480333678, 0.5700492978407592, 0.5529922412531109], "f1_train": [0.8390570108901136, 0.8394593154561193, 0.8382511109146542, 0.8395732279977857, 0.8371778416129771, 0.8511014020342913, 0.838058430408055, 0.8406927067694511, 0.8385244757916129, 0.841916335245507], "time": [2.1869914531707764, 2.233821392059326, 2.4369616508483887, 1.999532699584961, 2.31198787689209, 2.124500274658203, 2.436940908432007, 2.0620200634002686, 2.093294620513916, 2.343214273452759]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180]], "accuracy": [0.65, 0.5944444444444444, 0.6388888888888888, 0.5777777777777777, 0.5555555555555556, 0.6222222222222222, 0.5611111111111111, 0.6166666666666667, 0.5555555555555556, 0.5777777777777777], "precision": [0.5821148071148071, 0.5736530881333372, 0.5851746844704592, 0.5156533308580092, 0.49866978700312037, 0.5773077839744507, 0.5041931422622903, 0.540878306878307, 0.5030881028412347, 0.5363105094662473], "recall": [0.65, 0.5944444444444444, 0.6388888888888888, 0.5777777777777777, 0.5555555555555556, 0.6222222222222222, 0.5611111111111111, 0.6166666666666667, 0.5555555555555556, 0.5777777777777777], "f1_valid": [0.6089060793008162, 0.560676626206038, 0.6090086229286791, 0.5331694616896053, 0.523727477073806, 0.5781642856435693, 0.5117879812709245, 0.567167827782402, 0.5241581347108146, 0.5468866749537079], "f1_train": [0.9625685265658902, 0.9652332618663907, 0.9647382083347606, 0.9653754252107297, 0.9654646122078137, 0.9636601994326335, 0.9642771614869421, 0.9640392838654397, 0.964011244776087, 0.9707138528682286], "time": [3.7491183280944824, 3.7803919315338135, 3.733474016189575, 3.7178795337677, 3.7804081439971924, 3.936572551727295, 3.717890977859497, 3.7178657054901123, 3.7178797721862793, 3.920919895172119]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180], [1620, 180]], "accuracy": [0.45, 0.46111111111111114, 0.42777777777777776, 0.48333333333333334, 0.4722222222222222, 0.49444444444444446, 0.4777777777777778, 0.5277777777777778, 0.43333333333333335, 0.46111111111111114], "precision": [0.4491013872655303, 0.48023505090409635, 0.5523708870771645, 0.5087471336251824, 0.45028759820426484, 0.49798739272423487, 0.4942600194611689, 0.5385044461711128, 0.4643600286577579, 0.49624690181965725], "recall": [0.45, 0.46111111111111114, 0.42777777777777776, 0.48333333333333334, 0.4722222222222222, 0.49444444444444446, 0.4777777777777778, 0.5277777777777778, 0.43333333333333335, 0.46111111111111114], "f1_valid": [0.43243755520960203, 0.4516421410390555, 0.4435620835549821, 0.4803950061708553, 0.4551867366866001, 0.4893140297488123, 0.4794711737340181, 0.523650337328225, 0.4350529100529101, 0.4575185716856708], "f1_train": [0.5974804761031791, 0.6003416675136298, 0.5877829775527381, 0.5999463995157216, 0.59863330062035, 0.5825425734089386, 0.5852483910654082, 0.5956359914824912, 0.5742230911824663, 0.5861119443299673], "time": [0.8904151916503906, 0.9529001712799072, 0.8904151916503906, 0.9685225486755371, 0.8904156684875488, 0.9060380458831787, 0.9216547012329102, 0.9060275554656982, 0.8904142379760742, 0.9216563701629639]}
