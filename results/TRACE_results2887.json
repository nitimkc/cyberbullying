{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7039674465920651, 0.6673448626653102, 0.6795523906408952, 0.7009155645981688, 0.6948118006103764], "precision": [0.7009170407710514, 0.6628796779555887, 0.6793482741978715, 0.6985538075162107, 0.6914819742792976], "recall": [0.7039674465920651, 0.6673448626653102, 0.6795523906408952, 0.7009155645981688, 0.6948118006103764], "f1": [0.6981583880270957, 0.6624812768253759, 0.6698986295067035, 0.6951265171833007, 0.6883330771414883], "time": [16.261781692504883, 17.060266494750977, 16.10556674003601, 16.967118740081787, 16.558900356292725]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7049847405900305, 0.7019328585961343, 0.7293997965412004, 0.688708036622584, 0.6836215666327569], "precision": [0.7026230988824463, 0.7025492190772545, 0.7256155085936069, 0.6892549610522593, 0.6800134552760307], "recall": [0.7049847405900305, 0.7019328585961343, 0.7293997965412004, 0.688708036622584, 0.6836215666327569], "f1": [0.6998680660670805, 0.6940516143163156, 0.726126261371112, 0.6817973204853988, 0.6785409761606702], "time": [1.0778417587280273, 1.1403255462646484, 1.1246905326843262, 1.2809474468231201, 1.093491554260254]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7090539165818922, 0.6663275686673449, 0.7090539165818922, 0.659206510681587, 0.6754832146490336], "precision": [0.7157753320812174, 0.6857601013214178, 0.7099765089894123, 0.6561597792396122, 0.688429518011217], "recall": [0.7090539165818922, 0.6663275686673449, 0.7090539165818922, 0.659206510681587, 0.6754832146490336], "f1": [0.7102804969865324, 0.6682731946473619, 0.709468136496051, 0.6561559402998344, 0.677830247773482], "time": [16.527709245681763, 16.02872109413147, 16.230900287628174, 16.766875743865967, 16.68392777442932]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6876907426246185, 0.659206510681587, 0.6958290946083419, 0.7009155645981688, 0.6703967446592065], "precision": [0.6926975268381221, 0.6710768347537112, 0.6992788899917978, 0.7075820148492039, 0.6853823789338542], "recall": [0.6876907426246185, 0.659206510681587, 0.6958290946083419, 0.7009155645981688, 0.6703967446592065], "f1": [0.6890024163689902, 0.6619173180624378, 0.696771901913595, 0.7022939431052093, 0.6732518941441206], "time": [1.2028687000274658, 1.0935084819793701, 1.1715924739837646, 1.155973196029663, 1.1403779983520508]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7090539165818922, 0.711088504577823, 0.7080366225839267, 0.6856561546286877, 0.6846388606307223], "precision": [0.7088915393712785, 0.7135264225783584, 0.7126043955396792, 0.6849966938009522, 0.6846388606307223], "recall": [0.7090539165818922, 0.711088504577823, 0.7080366225839267, 0.6856561546286877, 0.6846388606307223], "f1": [0.708969781970637, 0.7120132780950007, 0.7095252149060254, 0.6852687662145767, 0.6846388606307223], "time": [6.365218639373779, 5.217601299285889, 5.358192443847656, 5.1394853591918945, 5.030035972595215]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6052899287894201, 0.6286876907426246, 0.641912512716175, 0.6337741607324516, 0.6195320447609359], "precision": [0.6985564515861803, 0.6847797887792717, 0.6746171593099521, 0.6875431737121426, 0.6898853559426082], "recall": [0.6052899287894201, 0.6286876907426246, 0.641912512716175, 0.6337741607324516, 0.6195320447609359], "f1": [0.5069465937091435, 0.5440523237572749, 0.577963296460974, 0.5528909328165674, 0.5355165872240234], "time": [1.0153999328613281, 1.1090991497039795, 1.0935084819793701, 1.062265396118164, 1.0310559272766113]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.6154628687690743, 0.6510681586978637, 0.6093591047812817, 0.6439471007121058, 0.6632756866734486], "precision": [0.6357499185838915, 0.6511574804888728, 0.6330304661213991, 0.6373299713275905, 0.6698117210781613], "recall": [0.6154628687690743, 0.6510681586978637, 0.6093591047812817, 0.6439471007121058, 0.6632756866734486], "f1": [0.6159969879182408, 0.6511121468319384, 0.6081530235453706, 0.6383642774962648, 0.6655093257562462], "time": [16.087470054626465, 21.332282304763794, 16.695021629333496, 18.757993459701538, 16.854933500289917]}
