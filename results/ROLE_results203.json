{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.4117647058823529, 0.4764705882352941, 0.4117647058823529, 0.3941176470588235, 0.4588235294117647, 0.4764705882352941, 0.49411764705882355, 0.4647058823529412, 0.40588235294117647, 0.38823529411764707, 0.46745562130177515, 0.378698224852071], "precision": [0.3271920356747238, 0.4734102041700984, 0.37830366078854255, 0.36260053588992774, 0.4748794599807136, 0.46006592292089254, 0.4817170111287759, 0.4053215561554017, 0.4197669885744829, 0.34283351536901235, 0.6014172743144707, 0.35533544206000783], "recall": [0.4117647058823529, 0.4764705882352941, 0.4117647058823529, 0.3941176470588235, 0.4588235294117647, 0.4764705882352941, 0.49411764705882355, 0.4647058823529412, 0.40588235294117647, 0.38823529411764707, 0.46745562130177515, 0.378698224852071], "f1": [0.31585472940971665, 0.387039600393256, 0.33640380397522085, 0.31167901234567896, 0.3779646612278747, 0.3900914675035541, 0.39972952185149946, 0.36292309408837065, 0.3349501529512061, 0.2895791318544128, 0.3791677609002752, 0.29893300693071645], "time": [0.21669244766235352, 0.22412824630737305, 0.19001126289367676, 0.18484735488891602, 0.20015168190002441, 0.2302086353302002, 0.21016812324523926, 0.1953878402709961, 0.19976520538330078, 0.20975756645202637, 0.18265771865844727, 0.24003911018371582]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.5058823529411764, 0.40588235294117647, 0.48823529411764705, 0.4764705882352941, 0.5117647058823529, 0.5470588235294118, 0.4823529411764706, 0.49411764705882355, 0.45294117647058824, 0.5176470588235295, 0.4970414201183432, 0.5384615384615384], "precision": [0.42383262214330725, 0.36508538899430737, 0.4454350490196079, 0.46480019856043686, 0.43605962589639435, 0.49714519140989727, 0.4139046360117081, 0.5160942863365009, 0.4284501912219996, 0.4689942503317117, 0.42194020554344447, 0.4670408825496493], "recall": [0.5058823529411764, 0.40588235294117647, 0.48823529411764705, 0.4764705882352941, 0.5117647058823529, 0.5470588235294118, 0.4823529411764706, 0.49411764705882355, 0.45294117647058824, 0.5176470588235295, 0.4970414201183432, 0.5384615384615384], "f1": [0.4454834002918191, 0.3325477626948215, 0.42867984258247954, 0.4349687555929906, 0.44809294163009333, 0.4962987006698241, 0.4251910562344225, 0.4595061510621282, 0.39919461014579916, 0.45769134375982495, 0.4368974554737267, 0.49276140313275485], "time": [0.18509387969970703, 0.16974234580993652, 0.14059209823608398, 0.12497138977050781, 0.1405928134918213, 0.1405942440032959, 0.1405956745147705, 0.15621376037597656, 0.1405932903289795, 0.1562058925628662, 0.140594482421875, 0.14060425758361816]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.4235294117647059, 0.47058823529411764, 0.4235294117647059, 0.36470588235294116, 0.38823529411764707, 0.4294117647058823, 0.45294117647058824, 0.40588235294117647, 0.5, 0.45294117647058824, 0.38461538461538464, 0.3668639053254438], "precision": [0.44300309911500735, 0.47107096098455087, 0.4577573529411764, 0.43042274381016776, 0.44631713554987207, 0.46525589993082256, 0.4616899206981411, 0.40608683512494764, 0.4997433658128432, 0.5043164493492303, 0.4233845253250057, 0.43707249091864475], "recall": [0.4235294117647059, 0.47058823529411764, 0.4235294117647059, 0.36470588235294116, 0.38823529411764707, 0.4294117647058823, 0.45294117647058824, 0.40588235294117647, 0.5, 0.45294117647058824, 0.38461538461538464, 0.3668639053254438], "f1": [0.4169855684690803, 0.4597672319701875, 0.42046807276298537, 0.3730017954728157, 0.38217580107775423, 0.42044843108689306, 0.4401142099845668, 0.3930912518853695, 0.4810209487076411, 0.43761832938303524, 0.38431997919819233, 0.37966458614407433], "time": [0.11381912231445312, 0.15238714218139648, 0.10301637649536133, 0.09486222267150879, 0.09376239776611328, 0.2626028060913086, 0.31807470321655273, 0.1333146095275879, 0.1385207176208496, 0.13222622871398926, 0.11573958396911621, 0.1165933609008789]}
