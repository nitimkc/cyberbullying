{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.6989498249708285, 0.7012835472578763, 0.6942823803967327, 0.6810747663551402, 0.6915887850467289], "precision": [0.6965534982243441, 0.6981109407994703, 0.6938899310489862, 0.6762726235054985, 0.692618378226973], "recall": [0.6989498249708285, 0.7012835472578763, 0.6942823803967327, 0.6810747663551402, 0.6915887850467289], "f1_valid": [0.6963503158888606, 0.6968182247242011, 0.6877439009702091, 0.676915183920115, 0.6847717634921087], "f1_train": [0.7626875142762067, 0.754550086743291, 0.7498073579587831, 0.7740387968369032, 0.7562362004220792], "time": [5.342477560043335, 5.030078411102295, 5.389405012130737, 5.108252286911011, 6.523132085800171]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.6977829638273045, 0.7164527421236873, 0.705950991831972, 0.7184579439252337, 0.6869158878504673], "precision": [0.6939231864860038, 0.7198740247905931, 0.7042883675171852, 0.7161397497814733, 0.6838952681169223], "recall": [0.6977829638273045, 0.7164527421236873, 0.705950991831972, 0.7184579439252337, 0.6869158878504673], "f1_valid": [0.6924170958355059, 0.7099824999166925, 0.7004792388030561, 0.7163364584288872, 0.6831934289137692], "f1_train": [0.8229816868129365, 0.8214218321510299, 0.8325900374533717, 0.8265451695807491, 0.8244080364697582], "time": [1.8120667934417725, 1.6246068477630615, 1.7027451992034912, 2.12387752532959, 1.8902406692504883]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.7001166861143524, 0.6837806301050176, 0.6931155192532089, 0.6915887850467289, 0.7009345794392523], "precision": [0.6996560950008727, 0.6845206860310319, 0.6912601122533457, 0.6879345967952603, 0.6991835426692486], "recall": [0.7001166861143524, 0.6837806301050176, 0.6931155192532089, 0.6915887850467289, 0.7009345794392523], "f1_valid": [0.6998624647146591, 0.6841050639474153, 0.6892545740135483, 0.6873340547751395, 0.6901060045481441], "f1_train": [0.7966513350777252, 0.8014572519541322, 0.7962409228257596, 0.7841674896129104, 0.7747917242945354], "time": [5.349065780639648, 5.733077526092529, 5.51439905166626, 6.357970237731934, 5.701786041259766]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.691948658109685, 0.6686114352392065, 0.6487747957992999, 0.6577102803738317, 0.6799065420560748], "precision": [0.6912654705907075, 0.6707363250453103, 0.6495946279791045, 0.6602887854046683, 0.6832286467189003], "recall": [0.691948658109685, 0.6686114352392065, 0.6487747957992999, 0.6577102803738317, 0.6799065420560748], "f1_valid": [0.6915729983793638, 0.6694416515748228, 0.649113307525356, 0.6587415339126234, 0.6810351875959352], "f1_train": [0.9579426501314767, 0.9602965920225237, 0.9582543967617272, 0.9585712665473269, 0.9588285103892745], "time": [1.9214675426483154, 2.1088902950286865, 1.6090421676635742, 1.6714563369750977, 1.499626636505127]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.7071178529754959, 0.7001166861143524, 0.6989498249708285, 0.6869158878504673, 0.6845794392523364], "precision": [0.7061169537177462, 0.6984182779536918, 0.69837993256703, 0.6850019728047394, 0.6848609593936696], "recall": [0.7071178529754959, 0.7001166861143524, 0.6989498249708285, 0.6869158878504673, 0.6845794392523364], "f1_valid": [0.7065387810046655, 0.6977916284800766, 0.6986304683701878, 0.6835628270353739, 0.6847164587055399], "f1_train": [0.8848833903803802, 0.8845424119291971, 0.8947365693852385, 0.8880428165699192, 0.891305161370268], "time": [7.265747308731079, 6.81096887588501, 6.889923334121704, 6.529752969741821, 6.725736141204834]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.6674445740956826, 0.661610268378063, 0.632438739789965, 0.6542056074766355, 0.6518691588785047], "precision": [0.6861202026240042, 0.6964698042467234, 0.6604325705670321, 0.6832177135080705, 0.6656208277703604], "recall": [0.6674445740956826, 0.661610268378063, 0.632438739789965, 0.6542056074766355, 0.6518691588785047], "f1_valid": [0.6304225815168918, 0.6194585824831592, 0.5792779359477269, 0.608660622821143, 0.6172292741897987], "f1_train": [0.8368433727315401, 0.8232630453184331, 0.8217551573666506, 0.824706983879236, 0.8371924844594876], "time": [1.4372107982635498, 1.640242099761963, 1.5152294635772705, 1.3121891021728516, 1.6246578693389893]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [3426, 3426, 3426, 3427, 3427], "accuracy": [0.5344224037339557, 0.5344224037339557, 0.5274212368728122, 0.5245327102803738, 0.5689252336448598], "precision": [0.598390491247911, 0.6074244927766287, 0.6072393096909262, 0.6029616175285143, 0.6226531158593719], "recall": [0.5344224037339557, 0.5344224037339557, 0.5274212368728122, 0.5245327102803738, 0.5689252336448598], "f1_valid": [0.5139312955800034, 0.5113479836860176, 0.510264996101902, 0.4873842225164753, 0.5524483183327807], "f1_train": [0.6185348655304072, 0.6225636501878267, 0.6313130064355019, 0.6228285501573351, 0.624367603566078], "time": [4.87383508682251, 5.260667085647583, 4.81890869140625, 4.83165717124939, 4.987105131149292]}
