{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70]], "accuracy": [0.5857142857142857, 0.5857142857142857, 0.6571428571428571, 0.6285714285714286, 0.5714285714285714, 0.6714285714285714, 0.6, 0.44285714285714284, 0.5714285714285714, 0.6142857142857143], "precision": [0.4465158497416562, 0.4789704343275772, 0.6094415584415583, 0.5107142857142857, 0.4631996658312447, 0.6642029539088363, 0.5135531135531135, 0.3005874060150376, 0.45209728160038726, 0.6269459706959707], "recall": [0.5857142857142857, 0.5857142857142857, 0.6571428571428571, 0.6285714285714286, 0.5714285714285714, 0.6714285714285714, 0.6, 0.44285714285714284, 0.5714285714285714, 0.6142857142857143], "f1_valid": [0.48225864814010705, 0.5074654377880184, 0.6078481680920705, 0.5615054226818933, 0.5035283831046543, 0.6314441047674129, 0.5332282430213464, 0.3503943152879323, 0.4992180780358121, 0.5577681577681578], "f1_train": [0.8620936663718004, 0.8621313819614125, 0.8527781561745531, 0.8446392799845263, 0.8558704569709897, 0.8614234660453148, 0.8640804069531366, 0.853294368975745, 0.8620845693124574, 0.8554392561722429], "time": [0.5779581069946289, 0.531092643737793, 0.5779571533203125, 0.6560666561126709, 0.56233811378479, 0.593611478805542, 0.577958345413208, 0.5467169284820557, 0.4998483657836914, 0.5467162132263184]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70]], "accuracy": [0.6714285714285714, 0.5571428571428572, 0.5857142857142857, 0.5714285714285714, 0.6285714285714286, 0.5714285714285714, 0.6571428571428571, 0.6571428571428571, 0.5285714285714286, 0.7], "precision": [0.6517190152801358, 0.6255913978494624, 0.4627825323677858, 0.5116326530612245, 0.6091364775575302, 0.4319670143099684, 0.6241234955520669, 0.6117316017316017, 0.37529000476720165, 0.6777855969974196], "recall": [0.6714285714285714, 0.5571428571428572, 0.5857142857142857, 0.5714285714285714, 0.6285714285714286, 0.5714285714285714, 0.6571428571428571, 0.6571428571428571, 0.5285714285714286, 0.7], "f1_valid": [0.6340598212768025, 0.5128732705203293, 0.5089255945293096, 0.5323384526224283, 0.5932044396706051, 0.487431162941367, 0.6167008471404756, 0.6192842168313867, 0.4373184734886862, 0.6746185998852119], "f1_train": [0.9834993863063675, 0.9835072192291536, 0.9813503042123126, 0.9849876943745973, 0.9835073686271172, 0.9812827297586781, 0.9817334572131656, 0.9903993840610789, 0.9851280560612726, 0.9850841421998329], "time": [0.781097412109375, 0.7498228549957275, 0.7967212200164795, 0.749823808670044, 0.7654454708099365, 0.7498252391815186, 0.7654464244842529, 0.7498228549957275, 0.7654454708099365, 0.827927827835083]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70], [630, 70]], "accuracy": [0.5, 0.5857142857142857, 0.4857142857142857, 0.37142857142857144, 0.5, 0.5285714285714286, 0.5142857142857142, 0.4857142857142857, 0.6428571428571429, 0.5142857142857142], "precision": [0.48248299319727894, 0.5591519591519591, 0.4741830065359478, 0.40476190476190477, 0.5396320346320346, 0.5066144381933855, 0.5806714944042134, 0.46824548001018595, 0.6510704474097332, 0.48034632034632035], "recall": [0.5, 0.5857142857142857, 0.4857142857142857, 0.37142857142857144, 0.5, 0.5285714285714286, 0.5142857142857142, 0.4857142857142857, 0.6428571428571429, 0.5142857142857142], "f1_valid": [0.4838998682476944, 0.5654081632653062, 0.4670748299319728, 0.3510204081632653, 0.4919015444015443, 0.5132894976172288, 0.507928852756439, 0.4535489985467871, 0.6124311743679353, 0.48644450883801793], "f1_train": [0.6301120728576, 0.6187342647696116, 0.6114123672909139, 0.6254917302444624, 0.6220852693072411, 0.6107403506003108, 0.6093760497069355, 0.627801123351675, 0.585114857276056, 0.6256149372959381], "time": [0.3124277591705322, 0.3124213218688965, 0.3280513286590576, 0.29680562019348145, 0.35929250717163086, 0.34369921684265137, 0.3280491828918457, 0.3280489444732666, 0.3124253749847412, 0.3436715602874756]}
