{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6, 0.62, 0.6175, 0.54, 0.625, 0.6125, 0.59, 0.59, 0.5775, 0.63, 0.57, 0.58], "precision": [0.694066317626527, 0.6234078045874293, 0.7683311855670104, 0.6533086602516655, 0.6273483906616936, 0.6614285714285714, 0.6365949930886193, 0.6900507614213198, 0.613966578715919, 0.7122105263157894, 0.6316793893129771, 0.6073137766856089], "recall": [0.6, 0.62, 0.6175, 0.54, 0.625, 0.6125, 0.59, 0.59, 0.5775, 0.63, 0.57, 0.58], "f1": [0.4883865401207938, 0.52600819586004, 0.49915050194520766, 0.41018469484488906, 0.5191831001432191, 0.5290311280559833, 0.4736594354053678, 0.4521890492453454, 0.4655894806845636, 0.5281674312742274, 0.42998506348020904, 0.46128386540120786], "time": [2.8899548053741455, 1.7690136432647705, 1.7086374759674072, 1.7847678661346436, 2.084353446960449, 2.1329853534698486, 2.0384671688079834, 1.927159309387207, 2.1107940673828125, 1.8267838954925537, 1.9310576915740967, 1.9089694023132324]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002218FCEF438>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.5875, 0.595, 0.6475, 0.6525, 0.565, 0.6025, 0.58, 0.6375, 0.5775, 0.53, 0.5725, 0.6075], "precision": [0.6440746753246753, 0.6571614583333333, 0.6590301827801828, 0.6732039200904637, 0.5676980014803849, 0.6719835180448992, 0.6951052631578947, 0.7008463541666667, 0.6496776315789474, 0.6093333333333333, 0.6376028138528138, 0.6534445646945648], "recall": [0.5875, 0.595, 0.6475, 0.6525, 0.565, 0.6025, 0.58, 0.6375, 0.5775, 0.53, 0.5725, 0.6075], "f1": [0.4796480793999546, 0.47812861195754963, 0.552187125205701, 0.5569007586342708, 0.43731492666091454, 0.4735619658710623, 0.4684737905228649, 0.530221785252745, 0.46460041471918667, 0.3897890658631081, 0.4620084412208827, 0.5035110712770288], "time": [0.1929948329925537, 0.19332432746887207, 0.20457172393798828, 0.19511961936950684, 0.18942856788635254, 0.2316737174987793, 0.19138193130493164, 0.2194843292236328, 0.2735912799835205, 0.2536430358886719, 0.2821381092071533, 0.32852864265441895]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.625, 0.555, 0.6525, 0.645, 0.69, 0.63, 0.6075, 0.6525, 0.64, 0.66, 0.57, 0.68], "precision": [0.6300157889700487, 0.6402295964449582, 0.6419549961861175, 0.6597730352303524, 0.6834677871148459, 0.6219510851602952, 0.6024502398693478, 0.6476064818787722, 0.6490090968161144, 0.6613663133097762, 0.5966563122880588, 0.6902996787973111], "recall": [0.625, 0.555, 0.6525, 0.645, 0.69, 0.63, 0.6075, 0.6525, 0.64, 0.66, 0.57, 0.68], "f1": [0.6039225260416667, 0.5205354140395123, 0.6393097288543208, 0.6042936698361918, 0.6647908402452218, 0.5990702120264164, 0.5803046198703953, 0.642893811040127, 0.6017610284406093, 0.6456423255312265, 0.5682789243277049, 0.6598573450639728], "time": [1.9512808322906494, 1.8932464122772217, 2.0803275108337402, 2.030026912689209, 2.0830185413360596, 2.216022491455078, 2.20695161819458, 2.115758180618286, 1.970348834991455, 1.9812724590301514, 1.931429147720337, 1.9499766826629639]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.69, 0.5875, 0.64, 0.58, 0.665, 0.67, 0.6625, 0.64, 0.6475, 0.61, 0.645, 0.65], "precision": [0.6814782608695653, 0.5823837058212058, 0.6339065567334118, 0.5834214177792854, 0.6575381818181818, 0.6667462064512342, 0.6558965102286402, 0.6379899622806197, 0.6399682554287817, 0.60496, 0.6460952332346638, 0.6503844246031747], "recall": [0.69, 0.5875, 0.64, 0.58, 0.665, 0.67, 0.6625, 0.64, 0.6475, 0.61, 0.645, 0.65], "f1": [0.6790938166311301, 0.5627603187730246, 0.6254828440042447, 0.559200510651879, 0.6556476014258115, 0.6589477941969584, 0.6518796992481204, 0.6223321323288569, 0.632535137143211, 0.6062444218272919, 0.6206786555942094, 0.6331718864792795], "time": [0.19321274757385254, 0.18088912963867188, 0.17439723014831543, 0.23018217086791992, 0.20269155502319336, 0.17250680923461914, 0.18503856658935547, 0.2613084316253662, 0.2423107624053955, 0.2013251781463623, 0.2606668472290039, 0.20650792121887207]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002218FCEF438>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6325, 0.63, 0.615, 0.6325, 0.6425, 0.65, 0.6525, 0.67, 0.6975, 0.6225, 0.6625, 0.65], "precision": [0.6222545454545454, 0.6490355477855477, 0.6543395686199657, 0.6348154291360814, 0.6521729390681003, 0.642555756479807, 0.6491747269377549, 0.6798666666666666, 0.692953869047619, 0.6111695837275307, 0.6699634093844168, 0.6452570514783438], "recall": [0.6325, 0.63, 0.615, 0.6325, 0.6425, 0.65, 0.6525, 0.67, 0.6975, 0.6225, 0.6625, 0.65], "f1": [0.6224220746039857, 0.5984039133473096, 0.561874631442387, 0.6041549991183214, 0.6135646270499029, 0.6194126665880411, 0.6317846316482193, 0.6486508839610334, 0.6847065409217419, 0.5979652130198044, 0.6411688311688312, 0.6266991117795624], "time": [0.4061582088470459, 0.4669055938720703, 0.4031240940093994, 0.4297177791595459, 0.410001277923584, 0.4048762321472168, 0.4904663562774658, 0.5176503658294678, 0.6724247932434082, 0.3555729389190674, 0.39011693000793457, 0.7674870491027832]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002218FCEF438>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.5725, 0.58, 0.5975, 0.565, 0.55, 0.6025, 0.57, 0.5575, 0.6, 0.6025, 0.5725, 0.5625], "precision": [0.32775624999999997, 0.7577329974811083, 0.3593984962406015, 0.7556549118387909, 0.7530075187969925, 0.36300625000000003, 0.32489999999999997, 0.753796992481203, 0.7604010025062656, 0.7609022556390976, 0.32775624999999997, 0.5353329145728644], "recall": [0.5725, 0.58, 0.5975, 0.565, 0.55, 0.6025, 0.57, 0.5575, 0.6, 0.6025, 0.5725, 0.5625], "f1": [0.41686009538950713, 0.4335992067863832, 0.44882629107981226, 0.41579032258064513, 0.3930065080550517, 0.45304992199687993, 0.41388535031847135, 0.40178366124200465, 0.4526258756143814, 0.4556731694514915, 0.41686009538950713, 0.4094378848473306], "time": [0.31012487411499023, 0.20503854751586914, 0.22987651824951172, 0.2048790454864502, 0.2249007225036621, 0.2237262725830078, 0.37378883361816406, 0.22999334335327148, 0.173567533493042, 0.1808767318725586, 0.201873779296875, 0.23488140106201172]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002218FCEF438>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.59, 0.54, 0.6125, 0.54, 0.5075, 0.595, 0.62, 0.5425, 0.535, 0.55, 0.5775, 0.5875], "precision": [0.59, 0.5672583915296238, 0.6114120351697991, 0.5620499999999999, 0.5162541440723704, 0.5839648409333449, 0.6327064739157763, 0.5937607357300297, 0.557916793197692, 0.5515398550724637, 0.5662414463124102, 0.5833945040466779], "recall": [0.59, 0.54, 0.6125, 0.54, 0.5075, 0.595, 0.62, 0.5425, 0.535, 0.55, 0.5775, 0.5875], "f1": [0.59, 0.5452172598044157, 0.6059889654108069, 0.5451280366035542, 0.5047450184478731, 0.5834568847301767, 0.6207222889155662, 0.5487520027142669, 0.5365703564727956, 0.5506470899871339, 0.569283276450512, 0.580884943759317], "time": [2.6715505123138428, 2.4848501682281494, 2.349966526031494, 2.2996740341186523, 2.70995831489563, 2.3428826332092285, 2.3634846210479736, 2.530555009841919, 2.385336399078369, 2.480825424194336, 2.451288938522339, 2.4581549167633057]}
