{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6140350877192983, 0.6257309941520468, 0.6352941176470588, 0.6411764705882353, 0.6058823529411764, 0.5882352941176471, 0.6529411764705882, 0.6529411764705882, 0.6705882352941176, 0.5823529411764706, 0.6941176470588235, 0.6882352941176471], "precision": [0.597222662845581, 0.5893305930548305, 0.6005476673427992, 0.6303203409023184, 0.579437037973084, 0.5586365777032055, 0.6113298402664347, 0.63699538638985, 0.6574212715389186, 0.5900695037373238, 0.68122453991032, 0.6789606471959413], "recall": [0.6140350877192983, 0.6257309941520468, 0.6352941176470588, 0.6411764705882353, 0.6058823529411764, 0.5882352941176471, 0.6529411764705882, 0.6529411764705882, 0.6705882352941176, 0.5823529411764706, 0.6941176470588235, 0.6882352941176471], "f1": [0.6029101792594009, 0.6037935619314271, 0.617319759252145, 0.6346344454750259, 0.5907179000632513, 0.5720453781512604, 0.6310468446018318, 0.6427847750865051, 0.6605611721477909, 0.5819207098897954, 0.6857780051150895, 0.6769944931349641], "time": [0.6272449493408203, 0.6524839401245117, 0.6447250843048096, 0.6245219707489014, 0.6379861831665039, 0.6000258922576904, 0.621445894241333, 0.6171338558197021, 0.6084480285644531, 0.6128709316253662, 0.6187598705291748, 0.6228599548339844]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6549707602339181, 0.6374269005847953, 0.6529411764705882, 0.6294117647058823, 0.5823529411764706, 0.6294117647058823, 0.5823529411764706, 0.6235294117647059, 0.6470588235294118, 0.6764705882352942, 0.5470588235294118, 0.6941176470588235], "precision": [0.6371387672966413, 0.6174932611121122, 0.6311426748335741, 0.5858463968431946, 0.5634309481368305, 0.6167177459218981, 0.5518685840485148, 0.6215623049719158, 0.6159622374858524, 0.6554229384061316, 0.5279803450295604, 0.681470588235294], "recall": [0.6549707602339181, 0.6374269005847953, 0.6529411764705882, 0.6294117647058823, 0.5823529411764706, 0.6294117647058823, 0.5823529411764706, 0.6235294117647059, 0.6470588235294118, 0.6764705882352942, 0.5470588235294118, 0.6941176470588235], "f1": [0.6435445707210024, 0.6255821026021174, 0.6404071301247772, 0.6066917833547297, 0.568919590708222, 0.6224736048265461, 0.566393680030684, 0.6177305177351459, 0.6306555769391683, 0.6653189422558013, 0.5318396681749623, 0.6845751633986928], "time": [1.488062858581543, 1.4960911273956299, 1.499298095703125, 1.51202392578125, 1.5033841133117676, 1.4997422695159912, 1.532876968383789, 1.514091968536377, 1.516951084136963, 1.5189828872680664, 1.5007569789886475, 1.5282979011535645]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x103993560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.5555555555555556, 0.4678362573099415, 0.47058823529411764, 0.5823529411764706, 0.5352941176470588, 0.5235294117647059, 0.5117647058823529, 0.5352941176470588, 0.5588235294117647, 0.5235294117647059, 0.49411764705882355, 0.5529411764705883], "precision": [0.5584869302804919, 0.46591132208401187, 0.4650118809511599, 0.5841997264021888, 0.5358415603734622, 0.5258436241629519, 0.5145203967962066, 0.5237766660916565, 0.6362745098039215, 0.515596967531952, 0.5009523809523809, 0.535796107916851], "recall": [0.5555555555555556, 0.4678362573099415, 0.47058823529411764, 0.5823529411764706, 0.5352941176470588, 0.5235294117647059, 0.5117647058823529, 0.5352941176470588, 0.5588235294117647, 0.5235294117647059, 0.49411764705882355, 0.5529411764705883], "f1": [0.5268387713952146, 0.4355671208856804, 0.44896155830753354, 0.5712098501214595, 0.5124503301689135, 0.5005606979219698, 0.4892824420233094, 0.5057113371664456, 0.5473588104865006, 0.5056437857274572, 0.4597393448397322, 0.5266968325791855], "time": [0.4855790138244629, 0.4914360046386719, 0.48075318336486816, 0.4866518974304199, 0.4810922145843506, 0.4814589023590088, 0.5233519077301025, 0.4787461757659912, 0.4924159049987793, 0.4838383197784424, 0.4979362487792969, 0.4820370674133301]}
