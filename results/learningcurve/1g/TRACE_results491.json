{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [392, 393, 393, 393, 393], "accuracy": [0.7070707070707071, 0.6428571428571429, 0.6326530612244898, 0.673469387755102, 0.6428571428571429], "precision": [0.6863275613275613, 0.6709956709956709, 0.6071613830717473, 0.7922077922077921, 0.6671899529042387], "recall": [0.7070707070707071, 0.6428571428571429, 0.6326530612244898, 0.673469387755102, 0.6428571428571429], "f1_valid": [0.6418808305600758, 0.5770308123249299, 0.5904953530708058, 0.6092796092796093, 0.5565016293493114], "f1_train": [0.8583042516175222, 0.8452233189223207, 0.8442275644677375, 0.8143138313232609, 0.8159947790822539], "time": [1.6089670658111572, 0.5623362064361572, 0.5311100482940674, 0.5780129432678223, 0.6248428821563721]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [392, 393, 393, 393, 393], "accuracy": [0.6868686868686869, 0.6224489795918368, 0.6530612244897959, 0.6020408163265306, 0.6632653061224489], "precision": [0.7050098688029721, 0.66921768707483, 0.6028618343889279, 0.6142707866845798, 0.6517857142857143], "recall": [0.6868686868686869, 0.6224489795918368, 0.6530612244897959, 0.6020408163265306, 0.6632653061224489], "f1_valid": [0.633257655309429, 0.5376926280716368, 0.5914339419978517, 0.528791962754227, 0.5872581590775846], "f1_train": [0.8433855246479912, 0.821439065713875, 0.8559140769253095, 0.7854867590236546, 0.8552456455197018], "time": [0.1561870574951172, 0.18746018409729004, 0.15621304512023926, 0.18749070167541504, 0.17183136940002441]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [392, 393, 393, 393, 393], "accuracy": [0.7171717171717171, 0.6632653061224489, 0.7346938775510204, 0.5306122448979592, 0.6122448979591837], "precision": [0.7101571268237935, 0.6578278863993149, 0.7375850340136054, 0.6416966786714686, 0.6064869857427805], "recall": [0.7171717171717171, 0.6632653061224489, 0.7346938775510204, 0.5306122448979592, 0.6122448979591837], "f1_valid": [0.7056290517111414, 0.659922168114079, 0.7349149659863946, 0.5211930926216641, 0.6091237631416202], "f1_train": [1.0, 1.0, 0.9949185572697025, 0.9696556147885601, 1.0], "time": [0.6461970806121826, 0.5623519420623779, 0.5623550415039062, 0.6404612064361572, 0.5779767036437988]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [392, 393, 393, 393, 393], "accuracy": [0.6363636363636364, 0.7142857142857143, 0.6530612244897959, 0.6326530612244898, 0.6326530612244898], "precision": [0.633618323005726, 0.725417439703154, 0.6448861682371552, 0.6384717408976169, 0.6859641815910037], "recall": [0.6363636363636364, 0.7142857142857143, 0.6530612244897959, 0.6326530612244898, 0.6326530612244898], "f1_valid": [0.6347319347319347, 0.6903361344537815, 0.6478854478854479, 0.6345169780385728, 0.6427130544130361], "f1_train": [1.0, 1.0, 1.0, 1.0, 1.0], "time": [0.19555902481079102, 0.18749594688415527, 0.1561877727508545, 0.17188215255737305, 0.15622496604919434]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [392, 393, 393, 393, 393], "accuracy": [0.7272727272727273, 0.673469387755102, 0.6938775510204082, 0.673469387755102, 0.5918367346938775], "precision": [0.7166584766584767, 0.6613782904466134, 0.7146274777853725, 0.6674563531706389, 0.5736151603498543], "recall": [0.7272727272727273, 0.673469387755102, 0.6938775510204082, 0.673469387755102, 0.5918367346938775], "f1_valid": [0.7153007948032946, 0.6521808723489395, 0.6692370129870131, 0.669322967282151, 0.5766138623281482], "f1_train": [0.937884024577573, 0.9538049062147085, 0.9563674511484184, 0.9539476933949896, 0.9511887567458772], "time": [0.2655651569366455, 0.2968423366546631, 0.23432326316833496, 0.2499542236328125, 0.29680943489074707]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [392, 393, 393, 393, 393], "accuracy": [0.6666666666666666, 0.6326530612244898, 0.6632653061224489, 0.6020408163265306, 0.5714285714285714], "precision": [0.78125, 0.4002498958767181, 0.4399208663057059, 0.7620450241952451, 0.7569955817378499], "recall": [0.6666666666666666, 0.6326530612244898, 0.6632653061224489, 0.6020408163265306, 0.5714285714285714], "f1_valid": [0.5602322206095791, 0.49030612244897964, 0.5289845999749593, 0.4628333788275341, 0.42609364319890636], "f1_train": [0.7367661896657235, 0.7338691047040921, 0.816531531758251, 0.6945870971309706, 0.6960732279949231], "time": [0.15624523162841797, 0.17183399200439453, 0.2030787467956543, 0.15621495246887207, 0.15622782707214355]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [392, 393, 393, 393, 393], "accuracy": [0.5656565656565656, 0.5510204081632653, 0.5612244897959183, 0.47959183673469385, 0.5408163265306123], "precision": [0.6709412970282536, 0.5695387592343096, 0.6798802817538182, 0.573396501457726, 0.5883112993625123], "recall": [0.5656565656565656, 0.5510204081632653, 0.5612244897959183, 0.47959183673469385, 0.5408163265306123], "f1_valid": [0.5545939003882929, 0.5449963633252043, 0.5678663573164938, 0.46336152858707275, 0.5481937393249855], "f1_train": [0.7123681356600612, 0.7268748956501181, 0.71546172180566, 0.7597065595691902, 0.7234783311119188], "time": [0.6665740013122559, 0.593585729598999, 0.8289492130279541, 0.6133303642272949, 0.5935852527618408]}
