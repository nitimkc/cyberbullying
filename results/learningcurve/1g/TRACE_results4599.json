{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.7, 0.6923913043478261, 0.7, 0.6902173913043478, 0.6833514689880305], "precision": [0.6975192061459669, 0.6925489384374197, 0.6952406134320465, 0.6860078530280016, 0.6855404946572244], "recall": [0.7, 0.6923913043478261, 0.7, 0.6902173913043478, 0.6833514689880305], "f1_valid": [0.6974099364257123, 0.6862955084833074, 0.6945233265720081, 0.6862646459698197, 0.6757490533378743], "f1_train": [0.7549221331890938, 0.7422046870785833, 0.7514888803591211, 0.7628228735604995, 0.753184022422088], "time": [5.474557876586914, 5.670523405075073, 5.389408588409424, 5.295609474182129, 5.420625686645508]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.6967391304347826, 0.7054347826086956, 0.6967391304347826, 0.7032608695652174, 0.6931447225244831], "precision": [0.6936527203551738, 0.7042563745890144, 0.6946054774295419, 0.7020445280072194, 0.6892377879042977], "recall": [0.6967391304347826, 0.7054347826086956, 0.6967391304347826, 0.7032608695652174, 0.6931447225244831], "f1_valid": [0.6911167665768765, 0.6987972984123881, 0.6948699925589118, 0.6992505126493479, 0.6881348215966371], "f1_train": [0.8236584972558366, 0.8199217786901528, 0.8253796397748062, 0.8177966469173463, 0.8228501522781211], "time": [1.624647855758667, 1.7808318138122559, 1.7339656352996826, 1.6246051788330078, 1.5777645111083984]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.7032608695652174, 0.675, 0.7195652173913043, 0.6902173913043478, 0.6724700761697497], "precision": [0.7007613490944771, 0.6827632182346248, 0.7180067001675041, 0.7036652641421225, 0.6685867936350659], "recall": [0.7032608695652174, 0.675, 0.7195652173913043, 0.6902173913043478, 0.6724700761697497], "f1_valid": [0.7013006941907198, 0.6615205444691464, 0.714679015478305, 0.6921156542778223, 0.6688865844552532], "f1_train": [0.7888975672854556, 0.7713053228130416, 0.772039927695823, 0.7849682224710864, 0.7887676704586187], "time": [5.467468738555908, 5.358259916305542, 5.452024221420288, 5.620060920715332, 5.655076503753662]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.658695652173913, 0.6826086956521739, 0.6554347826086957, 0.6880434782608695, 0.7105549510337323], "precision": [0.6574556640873835, 0.6826086956521739, 0.6555591493925361, 0.6913848244147158, 0.7103754836906154], "recall": [0.658695652173913, 0.6826086956521739, 0.6554347826086957, 0.6880434782608695, 0.7105549510337323], "f1_valid": [0.6579227731905155, 0.6826086956521739, 0.6554961934230189, 0.6892857129697083, 0.7104618382145473], "f1_train": [0.9537594786764814, 0.9526538445128426, 0.9494220490972693, 0.9532064697952727, 0.9508131703162473], "time": [2.6059446334838867, 1.734006404876709, 1.8433647155761719, 1.9683091640472412, 1.7340428829193115]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.7, 0.6739130434782609, 0.7054347826086956, 0.6739130434782609, 0.6985854189336235], "precision": [0.6997284956189066, 0.6716682869598333, 0.7046540731077908, 0.6724007083495317, 0.6982488778371215], "recall": [0.7, 0.6739130434782609, 0.7054347826086956, 0.6739130434782609, 0.6985854189336235], "f1_valid": [0.6998609220571699, 0.6724024057783647, 0.7050032942873073, 0.6725278519815568, 0.6956697800647439], "f1_train": [0.8940938051507882, 0.885290194621271, 0.8894054316167994, 0.8877345444749167, 0.8822866943387523], "time": [7.670247554779053, 7.748388051986694, 8.954574346542358, 8.310645341873169, 8.010636329650879]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.6532608695652173, 0.6413043478260869, 0.6434782608695652, 0.6489130434782608, 0.6583242655059848], "precision": [0.6753648083463554, 0.6873447151707127, 0.6654327122153209, 0.6508065061773501, 0.6839516495639036], "recall": [0.6532608695652173, 0.6413043478260869, 0.6434782608695652, 0.6489130434782608, 0.6583242655059848], "f1_valid": [0.6153961562206373, 0.586500322501863, 0.5971208466690415, 0.6162768348574447, 0.6143978781128423], "f1_train": [0.8215611278836904, 0.8086657176035628, 0.8250775609496318, 0.8362938079394203, 0.8302320675105485], "time": [1.8863122463226318, 1.7015855312347412, 2.1069886684417725, 1.6019346714019775, 1.5160906314849854]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [3679, 3679, 3679, 3679, 3680], "accuracy": [0.5641304347826087, 0.5597826086956522, 0.5695652173913044, 0.5771739130434783, 0.5331882480957563], "precision": [0.6442218914155673, 0.6016285095053688, 0.6120826056316451, 0.6085413606272774, 0.6059036595250847], "recall": [0.5641304347826087, 0.5597826086956522, 0.5695652173913044, 0.5771739130434783, 0.5331882480957563], "f1_valid": [0.558862656513972, 0.5501101839837632, 0.5525993940476471, 0.56791731605662, 0.5231866600539663], "f1_train": [0.6191455287496309, 0.6214326570202799, 0.6259616649466186, 0.6233130890164741, 0.6255934254472523], "time": [5.576785564422607, 5.238033294677734, 5.392400503158569, 5.2446630001068115, 5.507297039031982]}
