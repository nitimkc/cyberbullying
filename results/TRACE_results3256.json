{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.681586978636826, 0.6856561546286877, 0.7039674465920651, 0.7070193285859614, 0.6917599186164801], "precision": [0.6789036534664108, 0.6823519785754478, 0.7058449220587373, 0.7038280045944943, 0.6878121168394118], "recall": [0.681586978636826, 0.6856561546286877, 0.7039674465920651, 0.7070193285859614, 0.6917599186164801], "f1": [0.6751888910237401, 0.683219701250213, 0.6976999632168887, 0.7033861344700648, 0.685816888354378], "time": [21.49466633796692, 20.718425512313843, 21.86765718460083, 22.592991828918457, 18.142400979995728]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6897253306205493, 0.6907426246185148, 0.7060020345879959, 0.7090539165818922, 0.6897253306205493], "precision": [0.6855919626316398, 0.6899281351808473, 0.704149760328198, 0.7080831576849653, 0.6900296296461624], "recall": [0.6897253306205493, 0.6907426246185148, 0.7060020345879959, 0.7090539165818922, 0.6897253306205493], "f1": [0.6852719149175305, 0.6821279775674137, 0.6989610214158454, 0.708521117668489, 0.6852800162315754], "time": [1.2340881824493408, 1.1247305870056152, 1.2497355937957764, 1.2184584140777588, 1.1403498649597168]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7090539165818922, 0.6724313326551373, 0.7019328585961343, 0.7070193285859614, 0.6856561546286877], "precision": [0.7089734117015501, 0.6764759555073254, 0.7140043720489967, 0.7055252176228369, 0.6837010508086857], "recall": [0.7090539165818922, 0.6724313326551373, 0.7019328585961343, 0.7070193285859614, 0.6856561546286877], "f1": [0.7033263282628086, 0.6738306612898568, 0.7043270750060177, 0.7059765371936735, 0.6842309604533475], "time": [16.76216721534729, 16.355971097946167, 16.512213468551636, 16.605952262878418, 16.15287184715271]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6836215666327569, 0.698880976602238, 0.6917599186164801, 0.6622583926754833, 0.6998982706002035], "precision": [0.6837027511658673, 0.7068200962350166, 0.6959486210691327, 0.6700087849037496, 0.7102959295112667], "recall": [0.6836215666327569, 0.698880976602238, 0.6917599186164801, 0.6622583926754833, 0.6998982706002035], "f1": [0.6836614509175886, 0.70090582759855, 0.6926535492954559, 0.6641460971033034, 0.7025947501132298], "time": [1.1560149192810059, 1.1560747623443604, 1.124800205230713, 1.4840576648712158, 1.7152702808380127]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.71617497456765, 0.7182095625635809, 0.7080366225839267, 0.6978636826042727, 0.681586978636826], "precision": [0.7168354027748555, 0.71830817982922, 0.7090098555762997, 0.6971530052984566, 0.6894954212938529], "recall": [0.71617497456765, 0.7182095625635809, 0.7080366225839267, 0.6978636826042727, 0.681586978636826], "f1": [0.7164687865005274, 0.718258124122724, 0.7084345461362266, 0.6974495665607036, 0.683860000769915], "time": [8.630701065063477, 7.140181303024292, 7.185269594192505, 6.730418682098389, 7.585053443908691]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6358087487283826, 0.6449643947100712, 0.6042726347914548, 0.6307222787385555, 0.6286876907426246], "precision": [0.7239450024261432, 0.6814108176030301, 0.6651094929592546, 0.6819710084126959, 0.7111240092445409], "recall": [0.6358087487283826, 0.6449643947100712, 0.6042726347914548, 0.6307222787385555, 0.6286876907426246], "f1": [0.5530297232587861, 0.5752512321434994, 0.5107573671208562, 0.5518598166361035, 0.5482350965657014], "time": [1.2398808002471924, 1.4700522422790527, 1.0419135093688965, 1.0893166065216064, 1.0621845722198486]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.6286876907426246, 0.62970498474059, 0.6541200406917599, 0.6093591047812817, 0.6327568667344863], "precision": [0.6207478133808556, 0.6268123610316229, 0.657206673474679, 0.606143592239707, 0.6315044021071496], "recall": [0.6286876907426246, 0.62970498474059, 0.6541200406917599, 0.6093591047812817, 0.6327568667344863], "f1": [0.603876280359586, 0.608149892049202, 0.6553378762390292, 0.6071062276555664, 0.6320516170547346], "time": [19.716296672821045, 18.84337043762207, 19.555126905441284, 18.865490436553955, 20.284940719604492]}
