{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250]], "accuracy": [0.7, 0.724, 0.66, 0.668, 0.708, 0.712], "precision": [0.73125, 0.7277992721700823, 0.6581593406593406, 0.664343267280387, 0.7313230088495575, 0.7194751702375021], "recall": [0.7, 0.724, 0.66, 0.668, 0.708, 0.712], "f1_valid": [0.6421235857267189, 0.6674446699762784, 0.5941807274090738, 0.6091639154800584, 0.6487803793510734, 0.6589543446244477], "f1_train": [0.7497514261266642, 0.7801034259857789, 0.7549160173160173, 0.7757473482121431, 0.7636299307384985, 0.7646109555001144], "time": [2.5434279441833496, 2.5456418991088867, 2.5268588066101074, 2.5421969890594482, 2.5641229152679443, 2.5619990825653076]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250]], "accuracy": [0.664, 0.664, 0.748, 0.668, 0.704, 0.68], "precision": [0.6532660550458715, 0.6348221225710015, 0.7307417840375587, 0.6752706043956043, 0.7245122072745391, 0.720723643169058], "recall": [0.664, 0.664, 0.748, 0.668, 0.704, 0.68], "f1_valid": [0.6095181880576528, 0.6016929370057268, 0.7179845758354757, 0.6037294161759191, 0.6487923850574712, 0.6087919280065449], "f1_train": [0.8129353401737515, 0.8178313116567131, 0.8285219575896634, 0.7959227287254996, 0.8119489373838795, 0.8095954837638465], "time": [0.8614315986633301, 0.8109598159790039, 0.8000950813293457, 0.7954037189483643, 0.8085222244262695, 0.7987556457519531]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250]], "accuracy": [0.68, 0.716, 0.688, 0.676, 0.724, 0.688], "precision": [0.6809599127352058, 0.7083428571428572, 0.6714129586260733, 0.6675339366515837, 0.7118703976435935, 0.6799215686274509], "recall": [0.68, 0.716, 0.688, 0.676, 0.724, 0.688], "f1_valid": [0.656027833296321, 0.7094491180461331, 0.6729324355700024, 0.6712464006581653, 0.7067646535885735, 0.6558232044198894], "f1_train": [0.9283421176470589, 0.9156526734028779, 0.9259903750633588, 0.92817636068323, 0.9206945769407783, 0.9167738417707649], "time": [2.7019741535186768, 2.540605068206787, 2.539113998413086, 2.5514259338378906, 2.564761161804199, 2.5452239513397217]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250]], "accuracy": [0.72, 0.668, 0.704, 0.652, 0.712, 0.68], "precision": [0.7167675058176434, 0.6571701880659391, 0.6903064516129032, 0.6456953528399311, 0.7026285714285715, 0.6747676419965577], "recall": [0.72, 0.668, 0.704, 0.652, 0.712, 0.68], "f1_valid": [0.7179583333333334, 0.6607561187037754, 0.6932788483863478, 0.6480008395865036, 0.7055970961887478, 0.6766778900112234], "f1_train": [0.9992002136928335, 0.9992002042993825, 0.9992001990173172, 1.0, 1.0, 0.9992002123388889], "time": [0.8037929534912109, 0.8048532009124756, 0.8109939098358154, 0.8447160720825195, 0.80352783203125, 0.8019921779632568]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250], [1250, 250]], "accuracy": [0.672, 0.74, 0.724, 0.692, 0.708, 0.736], "precision": [0.6597079365079365, 0.7460528360528361, 0.7142482435597189, 0.6737349137931035, 0.6958769230769231, 0.7284842284739983], "recall": [0.672, 0.74, 0.724, 0.692, 0.708, 0.736], "f1_valid": [0.6604029512747461, 0.7182554638173871, 0.7096086732059886, 0.6754192694442799, 0.68780349062702, 0.7109721413721415], "f1_train": [0.9488359503278941, 0.9472070188522109, 0.9455692199076129, 0.944800229489386, 0.9471574064504099, 0.9347365201473689], "time": [1.675063133239746, 1.692490816116333, 1.6933801174163818, 1.686856985092163, 1.661072015762329, 1.7201578617095947]}
