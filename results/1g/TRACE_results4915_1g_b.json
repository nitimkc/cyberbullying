{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.7195121951219512, 0.7048780487804878, 0.6780487804878049, 0.7024390243902439, 0.6878048780487804, 0.6853658536585366, 0.6902439024390243, 0.6821515892420538, 0.706601466992665, 0.7359413202933985, 0.6650366748166259, 0.6528117359413202], "precision": [0.7247390386721043, 0.7028527874564461, 0.677011215815187, 0.7005948839976205, 0.6838288080994017, 0.6840193289422376, 0.6866678432822837, 0.6804533829995427, 0.7069737725247378, 0.732076662197334, 0.6606510332187626, 0.6458843388476976], "recall": [0.7195121951219512, 0.7048780487804878, 0.6780487804878049, 0.7024390243902439, 0.6878048780487804, 0.6853658536585366, 0.6902439024390243, 0.6821515892420538, 0.706601466992665, 0.7359413202933985, 0.6650366748166259, 0.6528117359413202], "f1_valid": [0.7137855539718613, 0.6996687869005471, 0.6684696210801394, 0.7009074605451937, 0.685218485670755, 0.676711865365001, 0.6872455562916525, 0.676412461017617, 0.7017078476455605, 0.7297847797594869, 0.6610595626952259, 0.64214748008273], "f1_train": [0.7458595895408742, 0.746837151757886, 0.7521533190285358, 0.7483154357943214, 0.7515130047918068, 0.7506817755100673, 0.7460978479491265, 0.7509388347686466, 0.7456206781865963, 0.7473038709263314, 0.747250635303895, 0.7487834439843117], "time": [6.644729375839233, 5.824861764907837, 6.650787353515625, 5.924757242202759, 5.637398958206177, 5.51714301109314, 5.603781461715698, 5.4723851680755615, 5.514203071594238, 5.50901985168457, 5.366689205169678, 5.637322902679443]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001FB481225E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.748780487804878, 0.724390243902439, 0.7, 0.6853658536585366, 0.7146341463414634, 0.6560975609756098, 0.7048780487804878, 0.684596577017115, 0.6797066014669927, 0.7310513447432763, 0.7163814180929096, 0.7163814180929096], "precision": [0.7469943714821763, 0.7241986393298467, 0.6967692307692307, 0.6822554419092579, 0.7120891188092102, 0.6534572554308171, 0.7014978221685689, 0.6786996449571209, 0.678116318936828, 0.7282136505659581, 0.7277653347134724, 0.7120673214875182], "recall": [0.748780487804878, 0.724390243902439, 0.7, 0.6853658536585366, 0.7146341463414634, 0.6560975609756098, 0.7048780487804878, 0.684596577017115, 0.6797066014669927, 0.7310513447432763, 0.7163814180929096, 0.7163814180929096], "f1_valid": [0.7458492092459367, 0.7210528752724724, 0.6964995411383514, 0.6826120168825857, 0.7101402663498887, 0.6504069771567661, 0.6970973732487503, 0.6790072367421912, 0.6717309280628493, 0.7278223837876867, 0.7086627292099976, 0.7101588140707946], "f1_train": [0.8175239265275028, 0.8179272165027314, 0.8211119861473904, 0.8136115436288983, 0.8174652941220265, 0.8182247588723854, 0.8209045972112304, 0.8208081653020961, 0.8176295691614758, 0.8125965621690336, 0.8153163858648759, 0.8186697479911508], "time": [1.6838841438293457, 1.6662614345550537, 1.6537806987762451, 1.6689164638519287, 1.7273128032684326, 1.8451659679412842, 1.6325879096984863, 1.7106168270111084, 1.8582961559295654, 1.9238722324371338, 1.678187608718872, 1.7110445499420166]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.6634146341463415, 0.6951219512195121, 0.7, 0.6878048780487804, 0.7073170731707317, 0.7121951219512195, 0.7292682926829268, 0.6797066014669927, 0.6674816625916871, 0.6968215158924206, 0.7212713936430318, 0.6943765281173594], "precision": [0.662903542490611, 0.6934798369808702, 0.6983310290341694, 0.6878048780487804, 0.7059682151235892, 0.7098772362647916, 0.7329897886939126, 0.6799992645746831, 0.6682335948308097, 0.6910087546336462, 0.7192699290424573, 0.6914071047349044], "recall": [0.6634146341463415, 0.6951219512195121, 0.7, 0.6878048780487804, 0.7073170731707317, 0.7121951219512195, 0.7292682926829268, 0.6797066014669927, 0.6674816625916871, 0.6968215158924206, 0.7212713936430318, 0.6943765281173594], "f1_valid": [0.6631433004285094, 0.6939628326483647, 0.6984530892448512, 0.6878048780487804, 0.7055494575986087, 0.7096321100502285, 0.7306038964512616, 0.6798488689037396, 0.6631203524691694, 0.6875080909310739, 0.719143166890145, 0.6923820526215934], "f1_train": [0.778592522325972, 0.7820866904260974, 0.7837157257809453, 0.7759053973957702, 0.7794442820974294, 0.7801507162079708, 0.7800710800584337, 0.7782756358230603, 0.7741360721767754, 0.7747075341979159, 0.7732107261978658, 0.7776949294735829], "time": [5.940903186798096, 5.974058151245117, 6.053192615509033, 6.323367118835449, 6.280407190322876, 6.2403576374053955, 6.165160655975342, 6.420185327529907, 6.093051910400391, 6.274742841720581, 6.560431957244873, 6.214915990829468]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.6926829268292682, 0.6780487804878049, 0.7219512195121951, 0.6463414634146342, 0.697560975609756, 0.6902439024390243, 0.675609756097561, 0.6650366748166259, 0.687041564792176, 0.6748166259168704, 0.6503667481662592, 0.6821515892420538], "precision": [0.6933737848344431, 0.6780487804878049, 0.7205796777417763, 0.6547590432426064, 0.6943183362099765, 0.6895288064176323, 0.6812654648285614, 0.6662513568634654, 0.688128500237095, 0.6738683457802462, 0.6506219312446161, 0.6814742507419538], "recall": [0.6926829268292682, 0.6780487804878049, 0.7219512195121951, 0.6463414634146342, 0.697560975609756, 0.6902439024390243, 0.675609756097561, 0.6650366748166259, 0.687041564792176, 0.6748166259168704, 0.6503667481662592, 0.6821515892420538], "f1_valid": [0.6930118454410351, 0.6780487804878049, 0.7208304932863673, 0.6491800160044224, 0.6934780487804879, 0.6886867003480747, 0.6775397667020149, 0.6655448695192667, 0.6874373425502435, 0.6742413153726065, 0.6504904615614918, 0.6817965627792343], "f1_train": [0.9361790270119815, 0.9339551489744874, 0.9346196133409741, 0.935128394897817, 0.9316998163451339, 0.9333088355153752, 0.9348983424317171, 0.9335928023377978, 0.9317326962769908, 0.9360035914202817, 0.9307136337238391, 0.9335608730752546], "time": [1.9916086196899414, 2.099266529083252, 1.9699125289916992, 1.881500482559204, 1.9972271919250488, 2.0398967266082764, 2.008068799972534, 1.8386187553405762, 2.0349202156066895, 2.034214973449707, 1.8802669048309326, 1.8752837181091309]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001FB481225E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.7219512195121951, 0.7024390243902439, 0.7170731707317073, 0.7121951219512195, 0.6390243902439025, 0.7195121951219512, 0.697560975609756, 0.6723716381418093, 0.7114914425427873, 0.6650366748166259, 0.7114914425427873, 0.6992665036674817], "precision": [0.7218183705241309, 0.7027634926010223, 0.7153635940124832, 0.7102832113790581, 0.6328919860627177, 0.7197723920571967, 0.6981447269848475, 0.6723716381418093, 0.7092129726338294, 0.6599605148525982, 0.7100563410226427, 0.6974752931113393], "recall": [0.7219512195121951, 0.7024390243902439, 0.7170731707317073, 0.7121951219512195, 0.6390243902439025, 0.7195121951219512, 0.697560975609756, 0.6723716381418093, 0.7114914425427873, 0.6650366748166259, 0.7114914425427873, 0.6992665036674817], "f1_valid": [0.720773188521948, 0.6997151391968465, 0.7157857478810915, 0.7098378063178508, 0.6282915883014398, 0.7196379890886657, 0.6978362177364396, 0.6723716381418093, 0.7099198483921425, 0.6613342429377145, 0.7106190236843739, 0.6980286648580663], "f1_train": [0.8767027881361681, 0.877930648767009, 0.8762526941865426, 0.8765121169467074, 0.8797501877988712, 0.8774098643965141, 0.8768436417303962, 0.8831098686604206, 0.8774402130917723, 0.8802187061710398, 0.8807692415722187, 0.878165117015229], "time": [9.935106754302979, 9.710997819900513, 9.885111093521118, 9.620126962661743, 11.040173053741455, 10.620368957519531, 10.65978717803955, 10.659719228744507, 10.460091590881348, 10.620346546173096, 11.048649549484253, 10.133752822875977]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001FB481225E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.6414634146341464, 0.6439024390243903, 0.6682926829268293, 0.6560975609756098, 0.6097560975609756, 0.6731707317073171, 0.651219512195122, 0.6748166259168704, 0.628361858190709, 0.6430317848410758, 0.6356968215158925, 0.6601466992665037], "precision": [0.6970661010957936, 0.6477856839053737, 0.7127576259077384, 0.6652641688989145, 0.6184190171711441, 0.7179670910138045, 0.6587443238980885, 0.7010183616793515, 0.6642295122194968, 0.6470331395397226, 0.6700732871015953, 0.6619745513989194], "recall": [0.6414634146341464, 0.6439024390243903, 0.6682926829268293, 0.6560975609756098, 0.6097560975609756, 0.6731707317073171, 0.651219512195122, 0.6748166259168704, 0.628361858190709, 0.6430317848410758, 0.6356968215158925, 0.6601466992665037], "f1_valid": [0.5931653214822992, 0.6028436336295074, 0.6213900230481292, 0.6096447241155409, 0.5581323026345016, 0.6246159929543289, 0.6048748937647029, 0.6472684914313105, 0.5703001973223363, 0.5973188723799971, 0.5899941370252465, 0.6139347442464655], "f1_train": [0.7976902201138455, 0.8149668729641343, 0.8012293592255295, 0.8142708030806322, 0.8038195691185237, 0.809747601662331, 0.8204141113393421, 0.8045957956611055, 0.8068817697717715, 0.8140234313584984, 0.8017519849247121, 0.8115855613135055], "time": [2.0397725105285645, 1.9880380630493164, 2.0299134254455566, 2.322077512741089, 2.0500855445861816, 1.953937292098999, 1.8302326202392578, 1.885195255279541, 2.157057762145996, 1.854738473892212, 2.444782018661499, 2.960210084915161]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000001FB481225E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [4505, 4505, 4505, 4505, 4505, 4505, 4505, 4506, 4506, 4506, 4506, 4506], "accuracy": [0.6073170731707317, 0.5902439024390244, 0.5487804878048781, 0.5585365853658537, 0.5975609756097561, 0.5756097560975609, 0.6097560975609756, 0.5965770171149144, 0.5770171149144254, 0.5794621026894865, 0.5916870415647921, 0.5941320293398533], "precision": [0.6229150233245746, 0.5911286605560386, 0.5724885118416402, 0.58165691808867, 0.6396185458891247, 0.6234207773010434, 0.6185115697310819, 0.6364939140540983, 0.6429246710909303, 0.5915135109775022, 0.6372782323305661, 0.6049826277184404], "recall": [0.6073170731707317, 0.5902439024390244, 0.5487804878048781, 0.5585365853658537, 0.5975609756097561, 0.5756097560975609, 0.6097560975609756, 0.5965770171149144, 0.5770171149144254, 0.5794621026894865, 0.5916870415647921, 0.5941320293398533], "f1_valid": [0.6101879370878301, 0.5879072637764905, 0.5498518089238589, 0.5574148730401384, 0.599134621679691, 0.5768218379061171, 0.6099139632173021, 0.596929126224363, 0.5832057327758191, 0.5786875758502954, 0.595259269195951, 0.5959234251036575], "f1_train": [0.6184686068065052, 0.6236200744921263, 0.6207142103498927, 0.6231986449207545, 0.6170669593053892, 0.623830872660316, 0.6221727102929056, 0.6312167185092306, 0.623557060141875, 0.624514679295996, 0.6214890700972435, 0.6245913563151044], "time": [6.344274044036865, 7.12003231048584, 6.205158710479736, 6.201115846633911, 6.220091342926025, 6.371779918670654, 6.170021295547485, 6.255042552947998, 6.229928255081177, 6.306175470352173, 6.809856414794922, 6.200017929077148]}
