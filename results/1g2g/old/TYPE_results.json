{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6491228070175439, 0.7076023391812866, 0.6470588235294118, 0.6705882352941176, 0.6941176470588235, 0.7, 0.6823529411764706, 0.6470588235294118, 0.6588235294117647, 0.6588235294117647, 0.6588235294117647, 0.6235294117647059], "precision": [0.6148042779621727, 0.6938914380310572, 0.6494117647058824, 0.6300691156309761, 0.6812717780610696, 0.6680904570121201, 0.66530058177117, 0.6228636512371315, 0.6351096314517155, 0.6275687118695797, 0.637279519544737, 0.5892058641638473], "recall": [0.6491228070175439, 0.7076023391812866, 0.6470588235294118, 0.6705882352941176, 0.6941176470588235, 0.7, 0.6823529411764706, 0.6470588235294118, 0.6588235294117647, 0.6588235294117647, 0.6588235294117647, 0.6235294117647059], "f1": [0.6285785318526793, 0.6987028520195848, 0.6451832907075874, 0.647749019607843, 0.6845509527919095, 0.6833590263691683, 0.6705851910002901, 0.6340951180861829, 0.6462840027245785, 0.6427262033544373, 0.6474818628474935, 0.6024593107330377], "time": [1.0488619804382324, 1.0160892009735107, 1.1214261054992676, 1.0876142978668213, 1.1180319786071777, 1.172447919845581, 1.0521950721740723, 1.0098187923431396, 1.1685569286346436, 1.2365996837615967, 1.135864019393921, 1.1079659461975098]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6140350877192983, 0.6140350877192983, 0.6647058823529411, 0.6823529411764706, 0.6882352941176471, 0.6352941176470588, 0.6235294117647059, 0.6588235294117647, 0.6235294117647059, 0.6529411764705882, 0.6529411764705882, 0.6470588235294118], "precision": [0.5842865933128872, 0.5841395519875723, 0.6519877613591377, 0.6677521008403361, 0.6768074020113721, 0.589941731409545, 0.6021695129664769, 0.644823983647513, 0.6093460480731455, 0.6543937057961013, 0.6120738636363636, 0.6318172704867613], "recall": [0.6140350877192983, 0.6140350877192983, 0.6647058823529411, 0.6823529411764706, 0.6882352941176471, 0.6352941176470588, 0.6235294117647059, 0.6588235294117647, 0.6235294117647059, 0.6529411764705882, 0.6529411764705882, 0.6470588235294118], "f1": [0.5986552682672543, 0.594140667367212, 0.6526377075939915, 0.6717787813031493, 0.6811415920886373, 0.6112871287128713, 0.6124567474048443, 0.6468665401644529, 0.6131857506078191, 0.6496400682011936, 0.6303802342320117, 0.6371003423101274], "time": [3.198704957962036, 3.0891149044036865, 3.048802137374878, 3.1395277976989746, 2.9377379417419434, 3.027750253677368, 3.182141065597534, 3.0371320247650146, 3.042926073074341, 2.97680401802063, 2.9793050289154053, 3.0281760692596436]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.5730994152046783, 0.5614035087719298, 0.5352941176470588, 0.5705882352941176, 0.5882352941176471, 0.5529411764705883, 0.5294117647058824, 0.4764705882352941, 0.4823529411764706, 0.5705882352941176, 0.5588235294117647, 0.5647058823529412], "precision": [0.5888835360712996, 0.5645294052741918, 0.5555702430785048, 0.5651611449177372, 0.6229185842188938, 0.5584967320261439, 0.533641109694652, 0.4825037707390648, 0.4744591169801254, 0.5901778893375532, 0.5515104313035246, 0.5617880485527544], "recall": [0.5730994152046783, 0.5614035087719298, 0.5352941176470588, 0.5705882352941176, 0.5882352941176471, 0.5529411764705883, 0.5294117647058824, 0.4764705882352941, 0.4823529411764706, 0.5705882352941176, 0.5588235294117647, 0.5647058823529412], "f1": [0.5506909983756305, 0.5249113552450416, 0.5130396967913416, 0.5440412951621957, 0.5730125167550832, 0.5313521582546061, 0.5081757299590771, 0.45905603348058593, 0.4513780337309749, 0.5494713402368674, 0.5337012442894796, 0.5471965862736681], "time": [0.7047700881958008, 0.7344858646392822, 0.7098369598388672, 0.6919307708740234, 0.7006170749664307, 0.6654260158538818, 0.6730828285217285, 0.7023990154266357, 0.6757590770721436, 0.6640620231628418, 0.6736249923706055, 0.6853680610656738]}
