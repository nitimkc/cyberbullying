{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.6987060998151571, 0.722735674676525, 0.6968576709796673, 0.7037037037037037, 0.6796296296296296], "precision": [0.7022869391406971, 0.7183079147113108, 0.7000402391733855, 0.7044753086419754, 0.6732510288065844], "recall": [0.6987060998151571, 0.722735674676525, 0.6968576709796673, 0.7037037037037037, 0.6796296296296296], "f1_valid": [0.6910708392270725, 0.7171509374879285, 0.6893728359063809, 0.694093362435071, 0.6741366963197779], "f1_train": [0.7731628201624009, 0.7842041099696767, 0.7878999442036795, 0.7835828643294492, 0.7786936893819678], "time": [3.3898162841796875, 3.608891010284424, 3.6054511070251465, 3.9318315982818604, 3.5936362743377686]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.6968576709796673, 0.7079482439926063, 0.7024029574861368, 0.7129629629629629, 0.725925925925926], "precision": [0.6952153859858919, 0.7061579981470956, 0.7041921225724368, 0.7116939288726999, 0.7217274552470083], "recall": [0.6968576709796673, 0.7079482439926063, 0.7024029574861368, 0.7129629629629629, 0.725925925925926], "f1_valid": [0.689233695200355, 0.7006446043979532, 0.6962193928190502, 0.7066336354793906, 0.7214845071055577], "f1_train": [0.8458392354273705, 0.8420584648288569, 0.8407720156538512, 0.8430391699257793, 0.8492149433188114], "time": [1.0154621601104736, 1.0467140674591064, 0.9842939376831055, 0.9373853206634521, 0.8791263103485107]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.7171903881700554, 0.6931608133086876, 0.7190388170055453, 0.6981481481481482, 0.6962962962962963], "precision": [0.7155678640181752, 0.7063740159558173, 0.716229240827293, 0.6959872916394656, 0.6952701619368287], "recall": [0.7171903881700554, 0.6931608133086876, 0.7190388170055453, 0.6981481481481482, 0.6962962962962963], "f1_valid": [0.7162571957211005, 0.6947581887010078, 0.7143120247514291, 0.6952849371360438, 0.6932833966935765], "f1_train": [0.8436996454836291, 0.8461576719303529, 0.8506519856020741, 0.8382790035265578, 0.8448049606167983], "time": [3.2588648796081543, 3.624673366546631, 3.3421454429626465, 3.610560178756714, 3.4690043926239014]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.6617375231053605, 0.6931608133086876, 0.66728280961183, 0.6796296296296296, 0.6555555555555556], "precision": [0.660741952291463, 0.6961972855121742, 0.6632533033378029, 0.6861494252873563, 0.6580870279146142], "recall": [0.6617375231053605, 0.6931608133086876, 0.66728280961183, 0.6796296296296296, 0.6555555555555556], "f1_valid": [0.6609652370168577, 0.6943389722111731, 0.6631752781691728, 0.6814412063215891, 0.6565978428640459], "f1_train": [0.9902875703926163, 0.9865797715439367, 0.9925983410385596, 0.9879796578825705, 0.988446046248433], "time": [1.0466368198394775, 1.4215376377105713, 0.937279462814331, 1.4726555347442627, 1.0778613090515137]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.7153419593345656, 0.7264325323475046, 0.6876155268022182, 0.6907407407407408, 0.6888888888888889], "precision": [0.7137068294368081, 0.7237952349809071, 0.6844998320636746, 0.6902091906721537, 0.6841616545320248], "recall": [0.7153419593345656, 0.7264325323475046, 0.6876155268022182, 0.6907407407407408, 0.6888888888888889], "f1_valid": [0.7119461466530113, 0.7242895675059452, 0.6851962881267568, 0.6881566427684165, 0.6846231681389768], "f1_train": [0.9093787563192307, 0.9069346013450565, 0.9162646693594749, 0.9177183777899316, 0.9135961705893595], "time": [2.84308123588562, 2.811833381652832, 2.874333143234253, 3.155461311340332, 2.9055216312408447]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.6192236598890942, 0.6765249537892791, 0.6155268022181146, 0.674074074074074, 0.6185185185185185], "precision": [0.659435339272658, 0.7048101842428516, 0.6808728774873342, 0.7136121067303863, 0.6268671974899424], "recall": [0.6192236598890942, 0.6765249537892791, 0.6155268022181146, 0.674074074074074, 0.6185185185185185], "f1_valid": [0.5543581192693016, 0.6318403180891695, 0.5325446703635244, 0.6240413883271027, 0.558264007597341], "f1_train": [0.8074032487586504, 0.8419249550140251, 0.8124887443877703, 0.8384750863313151, 0.8300568753295904], "time": [0.8904483318328857, 0.8279032707214355, 0.8591549396514893, 0.8279290199279785, 0.8279263973236084]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [2162, 2162, 2162, 2163, 2163], "accuracy": [0.4602587800369686, 0.4454713493530499, 0.43253234750462105, 0.4462962962962963, 0.46296296296296297], "precision": [0.6077511023653377, 0.6076658059232889, 0.6187284741398587, 0.5916992345663364, 0.6124106562703053], "recall": [0.4602587800369686, 0.4454713493530499, 0.43253234750462105, 0.4462962962962963, 0.46296296296296297], "f1_valid": [0.3384242409654589, 0.33433218406568177, 0.3151384018455365, 0.32504444822446654, 0.3371986531986532], "f1_train": [0.6144139427326855, 0.6259944187091239, 0.6252940918075701, 0.6206442074789815, 0.6141005738814873], "time": [3.1867334842681885, 3.202383041381836, 3.1710963249206543, 3.4078176021575928, 3.8045084476470947]}
