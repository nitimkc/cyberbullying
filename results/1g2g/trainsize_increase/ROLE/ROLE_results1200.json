{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120]], "accuracy": [0.675, 0.5083333333333333, 0.6083333333333333, 0.6333333333333333, 0.575, 0.625, 0.5916666666666667, 0.65, 0.55, 0.6583333333333333], "precision": [0.6824972360420122, 0.36652687590187594, 0.4852865388579674, 0.5132405260633195, 0.4182246591051622, 0.6264870423693953, 0.5121651295564339, 0.6922993230174082, 0.4884791666666667, 0.5654320987654321], "recall": [0.675, 0.5083333333333333, 0.6083333333333333, 0.6333333333333333, 0.575, 0.625, 0.5916666666666667, 0.65, 0.55, 0.6583333333333333], "f1_valid": [0.6362793590914108, 0.4219801520834808, 0.5333623693379792, 0.5586291038154393, 0.481014248767872, 0.5702979862018942, 0.5165175136191796, 0.5775108060005086, 0.49250051805704353, 0.5939017321331153], "f1_train": [0.8604533537654024, 0.8515738210167694, 0.8654800118746592, 0.8607763629364201, 0.8670342905299161, 0.8701352945804333, 0.8549772682438069, 0.8521347395590576, 0.8491746825223895, 0.8655331618158771], "time": [1.4215407371520996, 1.4684367179870605, 1.4684052467346191, 1.4371602535247803, 1.5465121269226074, 1.4527544975280762, 1.3121616840362549, 1.484020471572876, 1.249711513519287, 1.3121888637542725]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120]], "accuracy": [0.575, 0.625, 0.625, 0.6666666666666666, 0.625, 0.65, 0.6333333333333333, 0.6333333333333333, 0.625, 0.6166666666666667], "precision": [0.49875629113433995, 0.5343431550775135, 0.5404761904761906, 0.5953691520467836, 0.6104828757356563, 0.6297464001719321, 0.6014160219213411, 0.5018751090179662, 0.58429076819407, 0.5473049645390071], "recall": [0.575, 0.625, 0.625, 0.6666666666666666, 0.625, 0.65, 0.6333333333333333, 0.6333333333333333, 0.625, 0.6166666666666667], "f1_valid": [0.5207870485606617, 0.5683387518913835, 0.564176245210728, 0.6188449394094555, 0.58550135501355, 0.6080861376881259, 0.6132674264189901, 0.5582647726455695, 0.5861407566591841, 0.5770641496184974], "f1_train": [0.9692936043813235, 0.9642402654963047, 0.9705484672433702, 0.9696475514597359, 0.97537403894559, 0.9756035962986428, 0.971943243481277, 0.9710419720629493, 0.969504863412783, 0.9724463592844563], "time": [1.9214236736297607, 1.9682908058166504, 1.952666997909546, 1.8589370250701904, 1.8901667594909668, 1.8745307922363281, 1.9058351516723633, 1.890178918838501, 1.8901803493499756, 1.8901472091674805]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x000002008231FB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120], [1080, 120]], "accuracy": [0.4666666666666667, 0.5166666666666667, 0.475, 0.5416666666666666, 0.5166666666666667, 0.55, 0.43333333333333335, 0.575, 0.5416666666666666, 0.5666666666666667], "precision": [0.420637077294686, 0.5496254636348977, 0.4555304928989139, 0.5086410256410256, 0.4733565438754118, 0.5127929230949042, 0.41768909592822634, 0.6089682539682539, 0.5687784420801556, 0.520382572856257], "recall": [0.4666666666666667, 0.5166666666666667, 0.475, 0.5416666666666666, 0.5166666666666667, 0.55, 0.43333333333333335, 0.575, 0.5416666666666666, 0.5666666666666667], "f1_valid": [0.44000445632798574, 0.5067764077441497, 0.4539935554641437, 0.51015539549153, 0.49037552948843266, 0.5290123439065169, 0.41028255859374047, 0.5886494925839187, 0.5422391672718471, 0.5414071772008645], "f1_train": [0.654449306591863, 0.6491234181310324, 0.6326872983291758, 0.6335874809852686, 0.6581768070630672, 0.6366980299221808, 0.6399475261462568, 0.6495747420758757, 0.6321930212250643, 0.645177342569005], "time": [0.5623338222503662, 0.5779831409454346, 0.6092278957366943, 0.5936136245727539, 0.6092321872711182, 0.7029597759246826, 0.5623648166656494, 0.577988862991333, 0.5779895782470703, 0.5623664855957031]}
