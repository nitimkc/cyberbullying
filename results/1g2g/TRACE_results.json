{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6878048780487804, 0.7024390243902439, 0.7121951219512195, 0.6804878048780488, 0.724390243902439, 0.7170731707317073, 0.6829268292682927, 0.684596577017115, 0.6674816625916871, 0.7090464547677262, 0.706601466992665, 0.6894865525672371], "precision": [0.6813613676702983, 0.7004327222295731, 0.7087328820056261, 0.6775785625028926, 0.7233992395290559, 0.7149750852347233, 0.6801296789946316, 0.6829922510170618, 0.6637595219875592, 0.7070005378373843, 0.7037820709641178, 0.6875696821515893], "recall": [0.6878048780487804, 0.7024390243902439, 0.7121951219512195, 0.6804878048780488, 0.724390243902439, 0.7170731707317073, 0.6829268292682927, 0.684596577017115, 0.6674816625916871, 0.7090464547677262, 0.706601466992665, 0.6894865525672371], "f1": [0.681247819539575, 0.7011651173859409, 0.7087172049435053, 0.6772129129554043, 0.7189586136313981, 0.7142736473746927, 0.6760299429987577, 0.6797345952479964, 0.6624099418072336, 0.7057188118314728, 0.7022355491996126, 0.686675541625525], "time": [35.8097448348999, 30.88630485534668, 32.39983797073364, 30.843870878219604, 30.478883743286133, 30.520099878311157, 31.221054315567017, 31.396791219711304, 28.704941272735596, 29.565940856933594, 25.304354906082153, 33.80028009414673]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7195121951219512, 0.7365853658536585, 0.7024390243902439, 0.6951219512195121, 0.7, 0.7341463414634146, 0.7048780487804878, 0.7555012224938875, 0.6650366748166259, 0.7237163814180929, 0.687041564792176, 0.687041564792176], "precision": [0.7148550221364891, 0.7363073575734579, 0.6998557239619719, 0.6857718578615624, 0.6993765984654732, 0.7334649632210607, 0.7073078169111862, 0.7555012224938875, 0.6615446670305549, 0.7252079640609503, 0.6840272220939835, 0.6871435319802471], "recall": [0.7195121951219512, 0.7365853658536585, 0.7024390243902439, 0.6951219512195121, 0.7, 0.7341463414634146, 0.7048780487804878, 0.7555012224938875, 0.6650366748166259, 0.7237163814180929, 0.687041564792176, 0.687041564792176], "f1": [0.7148190700865235, 0.7317274849062105, 0.6977524390243901, 0.6872911688521548, 0.693012729844413, 0.7287837338739758, 0.7007810087254192, 0.7555012224938875, 0.6582453243497964, 0.7217067658819981, 0.6851186799097928, 0.6832988693461007], "time": [1.8865530490875244, 2.065593719482422, 2.0030689239501953, 1.795177936553955, 1.7713758945465088, 1.97017502784729, 1.91579008102417, 1.8576099872589111, 1.9379291534423828, 1.8908710479736328, 1.7566242218017578, 1.755460262298584]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6951219512195121, 0.7536585365853659, 0.7390243902439024, 0.6878048780487804, 0.6682926829268293, 0.6853658536585366, 0.7170731707317073, 0.6821515892420538, 0.6894865525672371, 0.7481662591687042, 0.6748166259168704, 0.6919315403422983], "precision": [0.6934317073170733, 0.7528286341047552, 0.7362710656770451, 0.688878465707734, 0.668757498968142, 0.6844429136469604, 0.7170731707317073, 0.7107876516692184, 0.6971053310562593, 0.746704587420825, 0.673892927206529, 0.7086723795369885], "recall": [0.6951219512195121, 0.7536585365853659, 0.7390243902439024, 0.6878048780487804, 0.6682926829268293, 0.6853658536585366, 0.7170731707317073, 0.6821515892420538, 0.6894865525672371, 0.7481662591687042, 0.6748166259168704, 0.6919315403422983], "f1": [0.6924173412201677, 0.7486641667948195, 0.7311082584510057, 0.6882762422013465, 0.6657958148980029, 0.684802024008431, 0.7170731707317073, 0.6854335033706559, 0.6902896767060844, 0.7428631371694626, 0.6743183909596624, 0.6961169646526859], "time": [27.95189380645752, 31.000370979309082, 31.120900869369507, 30.22770404815674, 26.95922088623047, 29.316114902496338, 27.871948957443237, 27.116096019744873, 26.229063987731934, 25.84481978416443, 26.00995707511902, 27.757930040359497]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6951219512195121, 0.7097560975609756, 0.7097560975609756, 0.675609756097561, 0.7317073170731707, 0.675609756097561, 0.6634146341463415, 0.6723716381418093, 0.7188264058679706, 0.7139364303178484, 0.726161369193154, 0.6894865525672371], "precision": [0.7005668225180419, 0.714117191592084, 0.7179819819819819, 0.6849862316909541, 0.7317073170731707, 0.6776876754141428, 0.6640565041525048, 0.6857938714754271, 0.7213130331728255, 0.7189835408717188, 0.7428676045025572, 0.6949933002259996], "recall": [0.6951219512195121, 0.7097560975609756, 0.7097560975609756, 0.675609756097561, 0.7317073170731707, 0.675609756097561, 0.6634146341463415, 0.6723716381418093, 0.7188264058679706, 0.7139364303178484, 0.726161369193154, 0.6894865525672371], "f1": [0.696654630442888, 0.7109895698483178, 0.7120179584983484, 0.6781895136347074, 0.7317073170731707, 0.6764524101722685, 0.6636724983927278, 0.6744478784821083, 0.7197218140082913, 0.7152305459555891, 0.7301112249625473, 0.6904253293136222], "time": [1.6807386875152588, 1.9463670253753662, 1.7516632080078125, 1.7729969024658203, 1.7697298526763916, 1.8154098987579346, 1.813337802886963, 1.8292722702026367, 1.9493169784545898, 1.7479839324951172, 1.7589402198791504, 1.8331336975097656]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7219512195121951, 0.6902439024390243, 0.724390243902439, 0.724390243902439, 0.748780487804878, 0.6902439024390243, 0.7390243902439024, 0.684596577017115, 0.6943765281173594, 0.7139364303178484, 0.6723716381418093, 0.6968215158924206], "precision": [0.7216082020960071, 0.6925337335615253, 0.7406803594351733, 0.7236881838520867, 0.7486901405816819, 0.6908443670989755, 0.7392136998404973, 0.682566713846371, 0.7025087674795101, 0.7198348156809942, 0.6714894768442272, 0.7016132606164536], "recall": [0.7219512195121951, 0.6902439024390243, 0.724390243902439, 0.724390243902439, 0.748780487804878, 0.6902439024390243, 0.7390243902439024, 0.684596577017115, 0.6943765281173594, 0.7139364303178484, 0.6723716381418093, 0.6968215158924206], "f1": [0.7217624421825072, 0.6911880317449062, 0.7278813131271854, 0.7236874756580541, 0.7487308488954502, 0.6905071813978352, 0.7391146269410173, 0.6828824385175921, 0.6969558041652533, 0.7156571711788122, 0.6718660883525209, 0.6984045094448041], "time": [9.870094060897827, 9.662758111953735, 9.633853912353516, 9.824718952178955, 9.760980129241943, 9.426790952682495, 10.14370608329773, 9.858040809631348, 10.100794792175293, 9.90430998802185, 10.20039415359497, 10.02382206916809]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6146341463414634, 0.6487804878048781, 0.6121951219512195, 0.6439024390243903, 0.6731707317073171, 0.6317073170731707, 0.6390243902439025, 0.58679706601467, 0.6161369193154034, 0.6552567237163814, 0.6381418092909535, 0.6161369193154034], "precision": [0.6543324203295509, 0.6848436777050697, 0.7189736532040989, 0.7195121951219512, 0.7631462148309456, 0.6832128940916612, 0.7225793111492451, 0.6824677484642085, 0.6904354248178155, 0.6677589724205087, 0.7041896890307648, 0.7080780841005627], "recall": [0.6146341463414634, 0.6487804878048781, 0.6121951219512195, 0.6439024390243903, 0.6731707317073171, 0.6317073170731707, 0.6390243902439025, 0.58679706601467, 0.6161369193154034, 0.6552567237163814, 0.6381418092909535, 0.6161369193154034], "f1": [0.5316045010567202, 0.5785388067978906, 0.5258236187836808, 0.5645127230493084, 0.5963532766299001, 0.545558079861623, 0.5625030275873762, 0.502722426098925, 0.5414961566001614, 0.5805777481796727, 0.563788389726088, 0.5217615466650529], "time": [1.7651638984680176, 1.6101031303405762, 1.6489768028259277, 1.5857658386230469, 1.5967381000518799, 1.5942211151123047, 1.6808269023895264, 1.8060131072998047, 1.67826509475708, 1.7889659404754639, 1.6738057136535645, 1.7306110858917236]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1081c8560>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5707317073170731, 0.6121951219512195, 0.5829268292682926, 0.6170731707317073, 0.6073170731707317, 0.624390243902439, 0.5878048780487805, 0.6552567237163814, 0.5965770171149144, 0.5672371638141809, 0.6063569682151589, 0.6185819070904646], "precision": [0.5777473537805508, 0.6080855329101237, 0.5579388944574377, 0.6268452701792211, 0.6081697402339877, 0.6700665188470067, 0.5991544308434323, 0.6783413056782787, 0.5749634343987677, 0.5199844681404866, 0.6566127044016591, 0.6615320005752913], "recall": [0.5707317073170731, 0.6121951219512195, 0.5829268292682926, 0.6170731707317073, 0.6073170731707317, 0.624390243902439, 0.5878048780487805, 0.6552567237163814, 0.5965770171149144, 0.5672371638141809, 0.6063569682151589, 0.6185819070904646], "f1": [0.5089632633299037, 0.5395148889883683, 0.5048007126333041, 0.5487236163360653, 0.5288684983597671, 0.5433291640608713, 0.4965213926880706, 0.5897784082081858, 0.5235317697066495, 0.46054085218579455, 0.538865454645392, 0.5338283816861897], "time": [29.693971157073975, 29.87171769142151, 29.543258905410767, 30.17993187904358, 28.367528915405273, 28.394377946853638, 29.97062921524048, 26.61478090286255, 25.818034887313843, 26.308128833770752, 27.891806840896606, 29.45121169090271]}
