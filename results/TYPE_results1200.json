{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100]], "accuracy": [0.74, 0.65, 0.75, 0.64, 0.61, 0.67, 0.61, 0.61, 0.64, 0.63, 0.54, 0.64], "precision": [0.7133590733590733, 0.6137567567567568, 0.727373949579832, 0.6181887532693985, 0.5782208886464205, 0.652805316091954, 0.575945945945946, 0.6024069478908188, 0.6154464285714285, 0.5898616600790514, 0.5077054437804945, 0.6260763888888888], "recall": [0.74, 0.65, 0.75, 0.64, 0.61, 0.67, 0.61, 0.61, 0.64, 0.63, 0.54, 0.64], "f1_valid": [0.7229609810479376, 0.6296, 0.7347118644067797, 0.6284286542083152, 0.5925752811130169, 0.6589201848218242, 0.5921666666666667, 0.6032706626954579, 0.6253575757575758, 0.606629746835443, 0.5194676434676435, 0.6282105263157896], "f1_train": [0.901026024087497, 0.9071891467079903, 0.9032050739957717, 0.9053743125645409, 0.9058151684987811, 0.9001293997794695, 0.9097700876395342, 0.9063389606597713, 0.9037181914903997, 0.9094176912518898, 0.904915805120681, 0.9014217731277693], "time": [0.9701371192932129, 1.0627295970916748, 0.8899805545806885, 0.8299691677093506, 1.2212281227111816, 0.9651045799255371, 1.071836233139038, 0.9505233764648438, 0.8388254642486572, 1.0781340599060059, 1.108827829360962, 1.0109403133392334]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100]], "accuracy": [0.67, 0.63, 0.65, 0.64, 0.65, 0.63, 0.67, 0.65, 0.67, 0.72, 0.58, 0.57], "precision": [0.6432189168573608, 0.595429002670382, 0.6289459930313589, 0.6250641025641025, 0.5913252643804634, 0.6071066555277082, 0.625703564727955, 0.6532241521918941, 0.6557775974025973, 0.692483414418399, 0.5637837837837838, 0.5302587350729765], "recall": [0.67, 0.63, 0.65, 0.64, 0.65, 0.63, 0.67, 0.65, 0.67, 0.72, 0.58, 0.57], "f1_valid": [0.6563110643110643, 0.6089981548060645, 0.6348429348429349, 0.6311616438356163, 0.6153367131536146, 0.6178829379575648, 0.6436636702745673, 0.6498701298701299, 0.6607839328057594, 0.7029565217391304, 0.5715492957746479, 0.5463804565732443], "f1_train": [0.9765293263332635, 0.9789290124152221, 0.9757799161180721, 0.9807416231856513, 0.9746343553519513, 0.9738172267358941, 0.9789207869987792, 0.9778874661725161, 0.9810178251651409, 0.9779779643544587, 0.9769485093405254, 0.9741408455054662], "time": [2.092893600463867, 1.9127895832061768, 1.951416015625, 2.6020548343658447, 1.8216383457183838, 1.7520740032196045, 1.855769157409668, 1.892343282699585, 2.1574952602386475, 1.9089679718017578, 2.1001856327056885, 1.8476085662841797]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000222493FFB88>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "size": [[1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100], [1100, 100]], "accuracy": [0.59, 0.57, 0.54, 0.5, 0.5, 0.62, 0.59, 0.58, 0.61, 0.46, 0.6, 0.54], "precision": [0.572862460815047, 0.5745261121856866, 0.6020598072004721, 0.4865272682807533, 0.4586973180076629, 0.6264908722109532, 0.5777170490328385, 0.5923750543242069, 0.5996423076923076, 0.5043245436105477, 0.5973809523809523, 0.5264741379310345], "recall": [0.59, 0.57, 0.54, 0.5, 0.5, 0.62, 0.59, 0.58, 0.61, 0.46, 0.6, 0.54], "f1_valid": [0.5714780323725346, 0.5542087542087542, 0.5436946584824345, 0.4849642857142857, 0.47155080213903744, 0.5908631578947368, 0.5733656074147633, 0.548576859882282, 0.5871323058190208, 0.4115898822562979, 0.5795219785290228, 0.5115695942204762], "f1_train": [0.6408999816161451, 0.6379409862535687, 0.6279436749364389, 0.6469534929997578, 0.6429024445496943, 0.6336421608035439, 0.6393364796941065, 0.6418247084560016, 0.638868241020997, 0.6526207811435084, 0.6409127380839077, 0.6467890951929366], "time": [0.5927460193634033, 0.5758492946624756, 0.596682071685791, 0.578758716583252, 0.6892766952514648, 0.6822609901428223, 0.5759932994842529, 0.7860870361328125, 0.6652202606201172, 0.5835280418395996, 0.7576701641082764, 0.7310168743133545]}
