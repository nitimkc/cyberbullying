{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6917599186164801, 0.7070193285859614, 0.6907426246185148, 0.6998982706002035, 0.669379450661241], "precision": [0.6872608864596701, 0.7040138395165904, 0.687548971487134, 0.701607710558272, 0.6655633287144104], "recall": [0.6917599186164801, 0.7070193285859614, 0.6907426246185148, 0.6998982706002035, 0.669379450661241], "f1": [0.6855356987097881, 0.7017797290785459, 0.685727574620931, 0.6915812693639236, 0.6617532619926357], "time": [17.792880296707153, 18.57240056991577, 19.718544960021973, 17.790911436080933, 17.231411457061768]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7060020345879959, 0.6917599186164801, 0.7202441505595117, 0.6876907426246185, 0.7080366225839267], "precision": [0.7073693935752324, 0.6897344882326814, 0.7175295687031173, 0.6840395735988044, 0.7069120684975067], "recall": [0.7060020345879959, 0.6917599186164801, 0.7202441505595117, 0.6876907426246185, 0.7080366225839267], "f1": [0.6984506723842067, 0.6829909811479457, 0.7158061047122412, 0.6836562702017938, 0.7001313596502731], "time": [1.2028450965881348, 1.1090900897979736, 1.0778818130493164, 1.2184712886810303, 1.216108798980713]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7212614445574771, 0.6795523906408952, 0.6734486266531028, 0.6826042726347915, 0.688708036622584], "precision": [0.7206335790247361, 0.6881892942109972, 0.66741886251401, 0.6842438266969809, 0.6914969597331001], "recall": [0.7212614445574771, 0.6795523906408952, 0.6734486266531028, 0.6826042726347915, 0.688708036622584], "f1": [0.7153941505955793, 0.6812641473781593, 0.6668001862330095, 0.6831956017326629, 0.6897585594005318], "time": [18.952006578445435, 20.70532488822937, 19.584043264389038, 20.17733120918274, 19.574132680892944]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.7131230925737538, 0.6917599186164801, 0.6632756866734486, 0.698880976602238, 0.6958290946083419], "precision": [0.7192882337347226, 0.7030795677364765, 0.6762333160092989, 0.6995720958658815, 0.6957338526832423], "recall": [0.7131230925737538, 0.6917599186164801, 0.6632756866734486, 0.698880976602238, 0.6958290946083419], "f1": [0.7147827988325838, 0.6940294318871948, 0.665916553973971, 0.6991542300551294, 0.6957807515615286], "time": [1.7797520160675049, 1.5100135803222656, 1.2028820514678955, 1.2585022449493408, 1.1804640293121338]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.7019328585961343, 0.6948118006103764, 0.6785350966429298, 0.7060020345879959, 0.6775178026449644], "precision": [0.7027609661352775, 0.69432619292095, 0.6825674438094428, 0.7088333258590155, 0.676540279152725], "recall": [0.7019328585961343, 0.6948118006103764, 0.6785350966429298, 0.7060020345879959, 0.6775178026449644], "f1": [0.7023112499558266, 0.6945430196607657, 0.6800793201630503, 0.7070318248392505, 0.676588399725075], "time": [5.841578483581543, 5.909718751907349, 5.811161756515503, 6.079714059829712, 5.811163425445557]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6459816887080366, 0.5981688708036622, 0.624618514750763, 0.6215666327568667, 0.6256358087487284], "precision": [0.723434746692427, 0.6531055141270594, 0.7207933398791424, 0.6797561501925942, 0.7060286196095436], "recall": [0.6459816887080366, 0.5981688708036622, 0.624618514750763, 0.6215666327568667, 0.6256358087487284], "f1": [0.5656937502912377, 0.4970382324785702, 0.5314906295577709, 0.5372108150364742, 0.5349107649948237], "time": [1.0466341972351074, 1.0309810638427734, 1.1404039859771729, 1.1403639316558838, 1.1378288269042969]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.6317395727365208, 0.6307222787385555, 0.6225839267548321, 0.6154628687690743, 0.6836215666327569], "precision": [0.622406615131519, 0.6491596048815357, 0.6233587330152113, 0.6124446830095023, 0.6811745230272332], "recall": [0.6317395727365208, 0.6307222787385555, 0.6225839267548321, 0.6154628687690743, 0.6836215666327569], "f1": [0.6230210798782791, 0.6298758910590093, 0.6229399253037687, 0.6135884647198662, 0.6818767369656958], "time": [16.36956024169922, 16.333244800567627, 16.689620971679688, 19.079127311706543, 19.431403160095215]}
