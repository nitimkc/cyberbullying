{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [[5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000]], "accuracy": [0.705, 0.678, 0.698, 0.736, 0.699, 0.672], "precision": [0.6956265395605479, 0.6676945392872287, 0.6996117452135493, 0.7342854050153346, 0.6989199656218756, 0.6627304778554778], "recall": [0.705, 0.678, 0.698, 0.736, 0.699, 0.672], "f1_valid": [0.680301533562991, 0.6566762083374461, 0.67503731283867, 0.7201935784085621, 0.6753568170340996, 0.6438721961774252], "f1_train": [0.7194621317887487, 0.7305164001762051, 0.7264648816354238, 0.7199856098668636, 0.7287144876334135, 0.7294844167592907], "time": [9.111968994140625, 9.359344959259033, 9.131012916564941, 8.785688161849976, 9.429331302642822, 9.407537937164307]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [[5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000]], "accuracy": [0.691, 0.729, 0.71, 0.704, 0.693, 0.7], "precision": [0.6910660079051384, 0.7212540057431228, 0.701708572917941, 0.7080907029478458, 0.6880821582533722, 0.6935420179252818], "recall": [0.691, 0.729, 0.71, 0.704, 0.693, 0.7], "f1_valid": [0.6686413434765486, 0.7119007542569945, 0.6948017765310892, 0.679870538415003, 0.6724837176931637, 0.6806774116220371], "f1_train": [0.8153881648662257, 0.8173781959270368, 0.8192640441176471, 0.8126449570909091, 0.8179539767053249, 0.8169401617949188], "time": [3.855347156524658, 3.460906982421875, 3.458752155303955, 3.4640488624572754, 3.7279796600341797, 3.838192939758301]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [[5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000]], "accuracy": [0.677, 0.684, 0.7, 0.694, 0.699, 0.708], "precision": [0.6730494583382103, 0.6748537247484616, 0.7024496116755438, 0.6838981918180878, 0.6927245598543961, 0.7100081300813008], "recall": [0.677, 0.684, 0.7, 0.694, 0.699, 0.708], "f1_valid": [0.674366097225639, 0.6679099647181351, 0.6759900931713646, 0.6837476923076922, 0.674538109285556, 0.676694670846395], "f1_train": [0.7733384411190919, 0.75325510467565, 0.7347871863219273, 0.7581451018750575, 0.7448308458655534, 0.7245386258754859], "time": [9.171031951904297, 8.72999119758606, 9.795351028442383, 9.216313123703003, 9.617911100387573, 8.941322088241577]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [[5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000]], "accuracy": [0.693, 0.677, 0.678, 0.688, 0.682, 0.683], "precision": [0.6931981861433244, 0.6711586826347306, 0.6723018183757264, 0.681750158299412, 0.6756339635844796, 0.6764192073170732], "recall": [0.693, 0.677, 0.678, 0.688, 0.682, 0.683], "f1_valid": [0.6930983984046399, 0.673138399189463, 0.6736862905835401, 0.6833812891050872, 0.6753496423561198, 0.6769070430242369], "f1_train": [0.9600393621209165, 0.9606066981713582, 0.9616403594983125, 0.9606419116561272, 0.9628294586190059, 0.9624020763992747], "time": [3.594062089920044, 3.646103858947754, 3.8837969303131104, 3.484363079071045, 3.498070001602173, 3.4973251819610596]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer(language=None, lemma=None)),\n                ('vectorize',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=0.85,\n                                 max_features=13000, min_df=2,\n                                 ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_wo...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x107d21050>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [[5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000], [5000, 1000]], "accuracy": [0.685, 0.696, 0.694, 0.692, 0.693, 0.692], "precision": [0.6760861021881914, 0.6878691873349907, 0.6874589856413861, 0.6833314285714286, 0.6843381027581238, 0.6821550912462281], "recall": [0.685, 0.696, 0.694, 0.692, 0.693, 0.692], "f1_valid": [0.6727569485511531, 0.6864635614574522, 0.6816285499139415, 0.6836862566071851, 0.6845693433108901, 0.6805364184158308], "f1_train": [0.9025815474489944, 0.9106380500597248, 0.9028013746392635, 0.9054108337969236, 0.9055254064719884, 0.9011922124079746], "time": [18.394134998321533, 18.300954818725586, 18.1114981174469, 18.40570902824402, 18.437453031539917, 18.17860984802246]}
