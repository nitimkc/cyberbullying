{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.724390243902439, 0.7, 0.6780487804878049, 0.7219512195121951, 0.6780487804878049, 0.6682926829268293, 0.6780487804878049, 0.7090464547677262, 0.6943765281173594, 0.7237163814180929, 0.7041564792176039, 0.6723716381418093], "precision": [0.7224561856452936, 0.7035950072209615, 0.6730730579325974, 0.7273075083692013, 0.6754120457052767, 0.6663235970452356, 0.679099387904842, 0.7055753164850206, 0.691234336797066, 0.7200243391269056, 0.6988183696372201, 0.670585523002906], "recall": [0.724390243902439, 0.7, 0.6780487804878049, 0.7219512195121951, 0.6780487804878049, 0.6682926829268293, 0.6780487804878049, 0.7090464547677262, 0.6943765281173594, 0.7237163814180929, 0.7041564792176039, 0.6723716381418093], "f1": [0.7201531183370701, 0.6944836610944211, 0.6727542000277688, 0.7149322026783089, 0.6718914634146341, 0.6625619670830856, 0.6737771416640046, 0.7063339366991972, 0.6915818849932864, 0.7213309026697322, 0.6999176420987715, 0.6624636572742381], "time": [6.21133279800415, 4.921634197235107, 4.2916419506073, 4.146260023117065, 4.25241494178772, 4.3086888790130615, 4.759037256240845, 4.30492901802063, 4.524741172790527, 4.352542161941528, 4.340605735778809, 4.274745225906372]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x105bbe560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.724390243902439, 0.6878048780487804, 0.6731707317073171, 0.7073170731707317, 0.7048780487804878, 0.7121951219512195, 0.6878048780487804, 0.6919315403422983, 0.7114914425427873, 0.7359413202933985, 0.7408312958435208, 0.7090464547677262], "precision": [0.7214438530527835, 0.6855873182387737, 0.6706564751883706, 0.7041682812142664, 0.7035974146817319, 0.7086211710601954, 0.6880702032784805, 0.6947374954513977, 0.7065889145125106, 0.7331578257137923, 0.7430618585976111, 0.7056060592686862], "recall": [0.724390243902439, 0.6878048780487804, 0.6731707317073171, 0.7073170731707317, 0.7048780487804878, 0.7121951219512195, 0.6878048780487804, 0.6919315403422983, 0.7114914425427873, 0.7359413202933985, 0.7408312958435208, 0.7090464547677262], "f1": [0.7196303163229227, 0.6861001526189997, 0.6715100138806266, 0.6985352983816172, 0.6988534898174238, 0.7058369073552404, 0.6817767202062264, 0.6890018257332293, 0.7076148475777327, 0.7314277560046023, 0.7378319519443847, 0.7018279734514293], "time": [1.2181146144866943, 1.2005479335784912, 1.1984350681304932, 1.1825690269470215, 1.1724967956542969, 1.2725441455841064, 1.2744388580322266, 1.2086730003356934, 1.3112149238586426, 1.1892409324645996, 1.3296022415161133, 1.2791359424591064]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7341463414634146, 0.7390243902439024, 0.6829268292682927, 0.6829268292682927, 0.6731707317073171, 0.697560975609756, 0.697560975609756, 0.6919315403422983, 0.706601466992665, 0.687041564792176, 0.6943765281173594, 0.6821515892420538], "precision": [0.7364363143631436, 0.7379335298847495, 0.6814740129476268, 0.6834868944741822, 0.6810115474084495, 0.6964110424386211, 0.6969558007122544, 0.68857281529063, 0.7163089344669724, 0.6821735556089232, 0.6931841803801655, 0.681644507457936], "recall": [0.7341463414634146, 0.7390243902439024, 0.6829268292682927, 0.6829268292682927, 0.6731707317073171, 0.697560975609756, 0.697560975609756, 0.6919315403422983, 0.706601466992665, 0.687041564792176, 0.6943765281173594, 0.6821515892420538], "f1": [0.7282426965016371, 0.7319997845670559, 0.6793908417025347, 0.6831906108094333, 0.6743881182514221, 0.6945416915080844, 0.6902514551189669, 0.6878023094824285, 0.7094262235789413, 0.6653702373974308, 0.6884762022155403, 0.6818816958093404], "time": [4.196752071380615, 4.072192907333374, 4.158164978027344, 4.727853775024414, 4.353048086166382, 4.308176279067993, 4.922173023223877, 4.1780688762664795, 4.678947925567627, 4.764134883880615, 4.59701132774353, 4.405081033706665]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6878048780487804, 0.697560975609756, 0.6634146341463415, 0.7048780487804878, 0.6804878048780488, 0.7, 0.6804878048780488, 0.6797066014669927, 0.6577017114914425, 0.6748166259168704, 0.687041564792176, 0.7334963325183375], "precision": [0.6897058429865779, 0.6969445260087478, 0.6615418495557869, 0.7052307414000999, 0.678415625619671, 0.7003012048192772, 0.6818350091191729, 0.6791881931894689, 0.6561640711363266, 0.6814653084520859, 0.6886895143811163, 0.7330904995117516], "recall": [0.6878048780487804, 0.697560975609756, 0.6634146341463415, 0.7048780487804878, 0.6804878048780488, 0.7, 0.6804878048780488, 0.6797066014669927, 0.6577017114914425, 0.6748166259168704, 0.687041564792176, 0.7334963325183375], "f1": [0.6886083213773315, 0.697102778742293, 0.6622246149294954, 0.7050501790365754, 0.6789574445520916, 0.7001464237391803, 0.6810603760731172, 0.6792477311654074, 0.6566804253286144, 0.677242145704165, 0.6877179218499513, 0.7332537344632082], "time": [1.2348048686981201, 1.2358410358428955, 1.2765932083129883, 1.223506212234497, 1.2205681800842285, 1.2286579608917236, 1.2678101062774658, 1.2730379104614258, 1.2423839569091797, 1.2288928031921387, 1.2115850448608398, 1.2137069702148438]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x105bbe560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6682926829268293, 0.6951219512195121, 0.697560975609756, 0.6707317073170732, 0.7390243902439024, 0.6780487804878049, 0.7463414634146341, 0.684596577017115, 0.6748166259168704, 0.7334963325183375, 0.6699266503667481, 0.7334963325183375], "precision": [0.6749294497077203, 0.6991270022285261, 0.6929063224034122, 0.6679270840944859, 0.7393020027761253, 0.6764427000278159, 0.7463414634146341, 0.6825725592330485, 0.6716846280469115, 0.7354902182270013, 0.6679402955648238, 0.7318680979365779], "recall": [0.6682926829268293, 0.6951219512195121, 0.697560975609756, 0.6707317073170732, 0.7390243902439024, 0.6780487804878049, 0.7463414634146341, 0.684596577017115, 0.6748166259168704, 0.7334963325183375, 0.6699266503667481, 0.7334963325183375], "f1": [0.6708411589092318, 0.6966260095171863, 0.6927614549663464, 0.6675398466555832, 0.7391587543729616, 0.6744583931133429, 0.7463414634146341, 0.6813743824764408, 0.6720677034912698, 0.7312228000736808, 0.6686071470411941, 0.7302828197358467], "time": [5.45815110206604, 5.4482128620147705, 5.44027304649353, 5.392674207687378, 5.653321981430054, 5.727626085281372, 5.550265073776245, 5.501800298690796, 5.618758916854858, 5.8008339405059814, 5.8970770835876465, 5.6024699211120605]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x105bbe560>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6682926829268293, 0.6536585365853659, 0.7073170731707317, 0.6146341463414634, 0.6536585365853659, 0.6317073170731707, 0.6317073170731707, 0.6503667481662592, 0.6650366748166259, 0.6577017114914425, 0.6430317848410758, 0.5990220048899756], "precision": [0.6754109138724524, 0.6885967995126637, 0.7320325203252033, 0.6471356755103601, 0.715127745801578, 0.6472869865847859, 0.6741408880573844, 0.6745894367873293, 0.6537107374704089, 0.6881719166870529, 0.6666624632492884, 0.6129262315813915], "recall": [0.6682926829268293, 0.6536585365853659, 0.7073170731707317, 0.6146341463414634, 0.6536585365853659, 0.6317073170731707, 0.6317073170731707, 0.6503667481662592, 0.6650366748166259, 0.6577017114914425, 0.6430317848410758, 0.5990220048899756], "f1": [0.6297654980264685, 0.6054913793528476, 0.6779035284149367, 0.5562755817213345, 0.6020556490047658, 0.5711814019237608, 0.5786295254312707, 0.6074797973055759, 0.6349971833725282, 0.6086803828073363, 0.5909800467474067, 0.5413745142018093], "time": [1.2426679134368896, 1.3281431198120117, 1.2158520221710205, 1.1874167919158936, 1.2276160717010498, 1.4845499992370605, 1.2960178852081299, 1.2099699974060059, 1.1890449523925781, 1.2640588283538818, 1.2604320049285889, 1.2969748973846436]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x105bbe560>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.551219512195122, 0.5853658536585366, 0.6097560975609756, 0.5682926829268292, 0.624390243902439, 0.5780487804878048, 0.5975609756097561, 0.5819070904645477, 0.589242053789731, 0.5721271393643031, 0.5672371638141809, 0.5770171149144254], "precision": [0.572725540465632, 0.5966161283234455, 0.6299534693460143, 0.6079912750347015, 0.6562951041207218, 0.6003427836892095, 0.6227553514954157, 0.6357789284256383, 0.5982172669375798, 0.58663260333978, 0.5954522876344699, 0.5942406319528893], "recall": [0.551219512195122, 0.5853658536585366, 0.6097560975609756, 0.5682926829268292, 0.624390243902439, 0.5780487804878048, 0.5975609756097561, 0.5819070904645477, 0.589242053789731, 0.5721271393643031, 0.5672371638141809, 0.5770171149144254], "f1": [0.5452026797794373, 0.5853658536585367, 0.6128369704749679, 0.5673037951263047, 0.6268081018593897, 0.5822624466955756, 0.5992744406783796, 0.5866622217232335, 0.5885887002826754, 0.5702987213585343, 0.5725656938557613, 0.5765012358688543], "time": [4.36676287651062, 4.138727903366089, 4.183520078659058, 4.020136117935181, 4.048878908157349, 4.582576751708984, 4.127976894378662, 4.227322816848755, 4.277292966842651, 4.393898010253906, 4.10823392868042, 4.155237913131714]}
