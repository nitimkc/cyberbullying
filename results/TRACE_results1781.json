{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6775178026449644, 0.6775178026449644, 0.6978636826042727, 0.6734486266531028, 0.6958290946083419], "precision": [0.681045485514338, 0.6707671258201514, 0.6941067248922743, 0.6755076199650862, 0.6985730499043018], "recall": [0.6775178026449644, 0.6775178026449644, 0.6978636826042727, 0.6734486266531028, 0.6958290946083419], "f1": [0.6635809423972135, 0.6665456574076281, 0.6912612651500155, 0.6602757488019367, 0.6836827795020547], "time": [11.257258415222168, 12.160326957702637, 10.90381383895874, 11.288026571273804, 10.966301441192627]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6856561546286877, 0.6744659206510681, 0.6856561546286877, 0.7049847405900305, 0.7019328585961343], "precision": [0.6879782342042717, 0.6697579955041305, 0.6874381083331496, 0.706281370455526, 0.6984703936580529], "recall": [0.6856561546286877, 0.6744659206510681, 0.6856561546286877, 0.7049847405900305, 0.7019328585961343], "f1": [0.6740281662304015, 0.6596801999107799, 0.6745886053778904, 0.6937772383681449, 0.6966025679716504], "time": [0.7341992855072021, 0.9372460842132568, 0.874840259552002, 0.9000496864318848, 0.9097921848297119]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6805696846388606, 0.688708036622584, 0.6510681586978637, 0.6927772126144456, 0.6805696846388606], "precision": [0.6853299495065877, 0.6873392309562508, 0.680816277940799, 0.6961599086485128, 0.6776343803433028], "recall": [0.6805696846388606, 0.688708036622584, 0.6510681586978637, 0.6927772126144456, 0.6805696846388606], "f1": [0.6821400012643167, 0.6859053378571176, 0.6514870683057814, 0.6939082954742671, 0.6785435736518615], "time": [12.180022478103638, 13.465046405792236, 11.490105390548706, 11.705016613006592, 13.067719221115112]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6703967446592065, 0.6622583926754833, 0.7029501525940997, 0.6663275686673449, 0.6948118006103764], "precision": [0.6739507177235948, 0.6779587142920329, 0.7050143023946731, 0.6737022789138374, 0.6940463247934785], "recall": [0.6703967446592065, 0.6622583926754833, 0.7029501525940997, 0.6663275686673449, 0.6948118006103764], "f1": [0.6714659367256912, 0.6645675026905954, 0.7037467567148554, 0.6681605041391415, 0.6943828112942123], "time": [0.8998327255249023, 0.849937915802002, 0.8647191524505615, 0.8651297092437744, 0.799931526184082]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6785350966429298, 0.6948118006103764, 0.6998982706002035, 0.6917599186164801, 0.6937945066124109], "precision": [0.6830722338226948, 0.6940021569516619, 0.7019075642617516, 0.689903956533201, 0.6906365621000631], "recall": [0.6785350966429298, 0.6948118006103764, 0.6998982706002035, 0.6917599186164801, 0.6937945066124109], "f1": [0.6799959931789825, 0.6943607129219282, 0.7006418083102421, 0.6896103719183941, 0.69121454015909], "time": [2.649833917617798, 3.285139560699463, 3.4299747943878174, 2.6299383640289307, 2.639991283416748]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6327568667344863, 0.6093591047812817, 0.6144455747711088, 0.6215666327568667, 0.5920651068158698], "precision": [0.6773069421600473, 0.6876844240283579, 0.7177041553544006, 0.6695954213043758, 0.6435113294557673], "recall": [0.6327568667344863, 0.6093591047812817, 0.6144455747711088, 0.6215666327568667, 0.5920651068158698], "f1": [0.5442386563613228, 0.5121876708855345, 0.5036724221760934, 0.543512920735726, 0.48966639021033614], "time": [0.7900633811950684, 0.7795863151550293, 0.8301575183868408, 0.7898190021514893, 0.7400600910186768]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4496439471007121, 0.43540183112919634, 0.4231943031536114, 0.3906408952187182, 0.4476093591047813], "precision": [0.7543958658504714, 0.6847328951143804, 0.6669796232390917, 0.15260030901728158, 0.6349802007862094], "recall": [0.4496439471007121, 0.43540183112919634, 0.4231943031536114, 0.3906408952187182, 0.4476093591047813], "f1": [0.2857955584159964, 0.27278845552063075, 0.27144444642160814, 0.2194675987768658, 0.29096661012120295], "time": [11.750897407531738, 11.300310850143433, 11.759115934371948, 11.465858459472656, 12.494358777999878]}
