{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 tokenizer=<function identity at 0x1a1dc6b9e0>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='multinomial', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='newton-cg', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6176470588235294, 0.5823529411764706, 0.6, 0.5117647058823529, 0.5470588235294118, 0.6176470588235294, 0.5823529411764706, 0.5823529411764706, 0.6294117647058823, 0.5941176470588235, 0.5325443786982249, 0.5798816568047337], "precision": [0.6122236586942469, 0.6003324728584244, 0.5673784106239673, 0.472642613878246, 0.48326734539969834, 0.5744268667062785, 0.5209576337532213, 0.5364798309896349, 0.5618008048289738, 0.544236034036256, 0.4745083975853207, 0.4989194141327787], "recall": [0.6176470588235294, 0.5823529411764706, 0.6, 0.5117647058823529, 0.5470588235294118, 0.6176470588235294, 0.5823529411764706, 0.5823529411764706, 0.6294117647058823, 0.5941176470588235, 0.5325443786982249, 0.5798816568047337], "f1": [0.586861886995014, 0.556862775228722, 0.5694472190498917, 0.4732861652252365, 0.5005715691515434, 0.5799498079774896, 0.5192349209279182, 0.5371646919951762, 0.5875257320776079, 0.5475728236548797, 0.4920210673367964, 0.5207557417481047], "time": [2.77158784866333, 0.7595958709716797, 0.7276363372802734, 0.660862922668457, 0.8334870338439941, 0.691687822341919, 0.7277688980102539, 0.6972651481628418, 0.7273998260498047, 0.6700108051300049, 0.6740338802337646, 0.708733081817627]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a1dc6b9e0>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6470588235294118, 0.6, 0.5764705882352941, 0.5352941176470588, 0.6, 0.6235294117647059, 0.5823529411764706, 0.6235294117647059, 0.5647058823529412, 0.5294117647058824, 0.5976331360946746, 0.5325443786982249], "precision": [0.6431238227040326, 0.6082957772450672, 0.5239676214196762, 0.4795016822630462, 0.5894568070635962, 0.6055378125710607, 0.5735880267599998, 0.5909724932326742, 0.5417420112869703, 0.5095401809372397, 0.54758123118447, 0.4641834825266778], "recall": [0.6470588235294118, 0.6, 0.5764705882352941, 0.5352941176470588, 0.6, 0.6235294117647059, 0.5823529411764706, 0.6235294117647059, 0.5647058823529412, 0.5294117647058824, 0.5976331360946746, 0.5325443786982249], "f1": [0.6243429141125372, 0.5744715721199718, 0.5448483130928997, 0.504288079115989, 0.5719645932055177, 0.6041219196103168, 0.550936393551021, 0.5985416634925992, 0.543077152525637, 0.5034816069600443, 0.5705213302284663, 0.4856075548383241], "time": [1.6992590427398682, 1.86739182472229, 1.9486660957336426, 1.7702441215515137, 1.880094051361084, 1.8468029499053955, 1.7997052669525146, 1.7358572483062744, 1.9033539295196533, 1.8294508457183838, 1.9090211391448975, 1.8939259052276611]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x1a1dc6b9e0>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                      metric='minkowski', metric_params=None,\n                                      n_jobs=None, n_neighbors=8, p=2,\n                                      weights='uniform'))],\n         verbose=False)", "name": "KNeighborsClassifier", "accuracy": [0.4588235294117647, 0.4117647058823529, 0.3941176470588235, 0.4647058823529412, 0.5, 0.4764705882352941, 0.47058823529411764, 0.4647058823529412, 0.5176470588235295, 0.5176470588235295, 0.4911242603550296, 0.48520710059171596], "precision": [0.4628531847521617, 0.3696252949445389, 0.42569378603171465, 0.4661666791981497, 0.5196275946275948, 0.4986294338844661, 0.517110844337735, 0.4312301587301587, 0.4938003131752742, 0.5401684970663457, 0.48883528658173, 0.5318553360771607], "recall": [0.4588235294117647, 0.4117647058823529, 0.3941176470588235, 0.4647058823529412, 0.5, 0.4764705882352941, 0.47058823529411764, 0.4647058823529412, 0.5176470588235295, 0.5176470588235295, 0.4911242603550296, 0.48520710059171596], "f1": [0.454612424409016, 0.3784625828226755, 0.3736544507643939, 0.45901591405793085, 0.49335496023827213, 0.4724960057784968, 0.4730959209807583, 0.43800984180896735, 0.5004167371197701, 0.5133191108692098, 0.46285496583162866, 0.48916136447476133], "time": [0.5392510890960693, 0.5342121124267578, 0.5427231788635254, 0.5649547576904297, 0.5017940998077393, 0.5180001258850098, 0.5231428146362305, 0.5178408622741699, 0.5332210063934326, 0.5682659149169922, 0.5220947265625, 0.521651029586792]}
