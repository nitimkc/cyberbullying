{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.7015113350125944, 0.6826196473551638, 0.6771752837326608, 0.7049180327868853, 0.6847414880201765], "precision": [0.6993627546067737, 0.6796899317255233, 0.6725960503372616, 0.7015876095321241, 0.6832531906932916], "recall": [0.7015113350125944, 0.6826196473551638, 0.6771752837326608, 0.7049180327868853, 0.6847414880201765], "f1_valid": [0.6941723235520817, 0.6749335604075098, 0.6693689070085458, 0.6995899314692727, 0.6781014046217407], "f1_train": [0.7566354900810313, 0.7640265785837362, 0.7632012184971514, 0.7518621527525068, 0.7561609535115886], "time": [4.764722585678101, 4.888986825942993, 4.795957565307617, 4.817377805709839, 6.592325210571289]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.7292191435768262, 0.7216624685138538, 0.6885245901639344, 0.6897856242118537, 0.7276166456494325], "precision": [0.7247773588372808, 0.7209200483237356, 0.6869288716726762, 0.6875451989301081, 0.7276131358131164], "recall": [0.7292191435768262, 0.7216624685138538, 0.6885245901639344, 0.6897856242118537, 0.7276166456494325], "f1_valid": [0.7245398717681403, 0.7158759370631247, 0.6826112459996259, 0.6816139417606896, 0.723567986278092], "f1_train": [0.8295700537963835, 0.8233230457800502, 0.8291973473647105, 0.8293873857701956, 0.8264134850244065], "time": [1.5778076648712158, 1.718381404876709, 1.3434138298034668, 3.4815919399261475, 1.5352249145507812]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.6775818639798489, 0.6599496221662469, 0.6784363177805801, 0.699873896595208, 0.7049180327868853], "precision": [0.6872831732122726, 0.6557212289793243, 0.6767938445840304, 0.6973421533985513, 0.7086221207719444], "recall": [0.6775818639798489, 0.6599496221662469, 0.6784363177805801, 0.699873896595208, 0.7049180327868853], "f1_valid": [0.6799734527419313, 0.6539593442662629, 0.6774317174233114, 0.6961873305013861, 0.6940495580631674], "f1_train": [0.8095316424218484, 0.7996268196954287, 0.7996165511039488, 0.7962375174318338, 0.7892891297353168], "time": [4.764624834060669, 5.24571967124939, 5.0685648918151855, 4.952051639556885, 4.717727184295654]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.6624685138539043, 0.6473551637279596, 0.7023959646910467, 0.6607818411097099, 0.6834804539722572], "precision": [0.6660257440633354, 0.6487903774930945, 0.7006732923433475, 0.6594849040448715, 0.6860581118302558], "recall": [0.6624685138539043, 0.6473551637279596, 0.7023959646910467, 0.6607818411097099, 0.6834804539722572], "f1_valid": [0.6638294836546429, 0.6479253873395003, 0.7013133204182742, 0.6600481646496956, 0.6842912180327452], "f1_train": [0.9624681322458944, 0.9710234127591127, 0.9621548180744293, 0.9669005442639442, 0.9628422556593526], "time": [1.457146167755127, 2.27182674407959, 1.4059500694274902, 1.9903435707092285, 1.4996747970581055]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.7015113350125944, 0.6939546599496221, 0.6860025220680959, 0.7023959646910467, 0.7061790668348046], "precision": [0.6995060738547941, 0.6915862899615531, 0.6841720201853339, 0.6999307464908109, 0.7052205312001353], "recall": [0.7015113350125944, 0.6939546599496221, 0.6860025220680959, 0.7023959646910467, 0.7061790668348046], "f1_valid": [0.7001019955574135, 0.6921804826039073, 0.6846017303113413, 0.7001397501659606, 0.7056081721729895], "f1_train": [0.888579022177524, 0.8951330531856261, 0.8922863146224719, 0.8897671008565373, 0.89511530474869], "time": [5.639297962188721, 5.326908349990845, 5.545602798461914, 5.545580148696899, 5.811208724975586]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.6410579345088161, 0.6158690176322418, 0.6443883984867591, 0.694829760403531, 0.6670870113493065], "precision": [0.6792228153358653, 0.6610655083253341, 0.6868447431422657, 0.7142052996615119, 0.6906997312456848], "recall": [0.6410579345088161, 0.6158690176322418, 0.6443883984867591, 0.694829760403531, 0.6670870113493065], "f1_valid": [0.5874645806670132, 0.5492258203607031, 0.5908962093210798, 0.6556171349514287, 0.6243438973298251], "f1_train": [0.8110902856178446, 0.8102062958694641, 0.8084656355180698, 0.8448950768039729, 0.8287207759045676], "time": [1.484740972518921, 1.53090238571167, 1.6714916229248047, 1.4996583461761475, 1.5152792930603027]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000244BB522678>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "size": [3173, 3173, 3174, 3174, 3174], "accuracy": [0.5188916876574308, 0.49496221662468515, 0.532156368221942, 0.5510718789407314, 0.5044136191677175], "precision": [0.6150147372328558, 0.6000581092081412, 0.5891132147870405, 0.6378353773410259, 0.6173616967140049], "recall": [0.5188916876574308, 0.49496221662468515, 0.532156368221942, 0.5510718789407314, 0.5044136191677175], "f1_valid": [0.4880168769982215, 0.448260044497209, 0.5009280877335279, 0.5228161105676248, 0.46406012827675586], "f1_train": [0.6230029955023998, 0.6243038540223764, 0.615259018935702, 0.6271440658866521, 0.6208753692537216], "time": [4.702086687088013, 4.905087947845459, 5.872414588928223, 5.030080556869507, 6.09233832359314]}
