{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.6192592592592593, 0.6133333333333333, 0.6385185185185185, 0.6503703703703704, 0.6518518518518519], "precision": [0.6517131541523071, 0.6425225723235672, 0.6532785884549626, 0.6954205607476636, 0.6708285353504527], "recall": [0.6192592592592593, 0.6133333333333333, 0.6385185185185185, 0.6503703703703704, 0.6518518518518519], "f1": [0.5493327976471012, 0.5395390663876977, 0.5819635145349432, 0.6032662801163837, 0.5985835665356314], "time": [4.984279632568359, 5.051272630691528, 5.019558668136597, 4.985435485839844, 4.884989261627197]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.6148148148148148, 0.6251851851851852, 0.6518518518518519, 0.605925925925926, 0.6755555555555556], "precision": [0.6486161318948205, 0.6363864734299517, 0.679846869018321, 0.6799851107388796, 0.6901710022181892], "recall": [0.6148148148148148, 0.6251851851851852, 0.6518518518518519, 0.605925925925926, 0.6755555555555556], "f1": [0.5361914063817871, 0.5710601200756641, 0.5973611153174435, 0.5357314814814814, 0.6269521759694143], "time": [0.4704725742340088, 0.4051821231842041, 0.3901681900024414, 0.41027379035949707, 0.4251241683959961]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.6607407407407407, 0.6414814814814814, 0.6696296296296296, 0.6533333333333333, 0.6414814814814814], "precision": [0.6778949709943862, 0.6442953471817771, 0.6690823307383817, 0.6546188516558887, 0.6511593749674245], "recall": [0.6607407407407407, 0.6414814814814814, 0.6696296296296296, 0.6533333333333333, 0.6414814814814814], "f1": [0.6639547560206499, 0.6186641931103674, 0.6574053990194341, 0.6538846499770681, 0.6438541849556887], "time": [5.369932174682617, 5.178850173950195, 6.190865516662598, 5.353634595870972, 5.153862714767456]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6503703703703704, 0.6607407407407407, 0.6474074074074074, 0.6814814814814815, 0.6488888888888888], "precision": [0.6486194554012542, 0.6622199292808613, 0.650385231626611, 0.6806023839931886, 0.6443896025126342], "recall": [0.6503703703703704, 0.6607407407407407, 0.6474074074074074, 0.6814814814814815, 0.6488888888888888], "f1": [0.6492143749687609, 0.6614096282045102, 0.6486199450333727, 0.6756347117805634, 0.6434246899783623], "time": [0.4054725170135498, 0.4450359344482422, 0.40503454208374023, 0.42466044425964355, 0.4680790901184082]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6888888888888889, 0.6637037037037037, 0.6637037037037037, 0.6711111111111111, 0.6548148148148148], "precision": [0.6829493087557604, 0.6794597812722449, 0.666889961232712, 0.6658293838862559, 0.649455261751265], "recall": [0.6888888888888889, 0.6637037037037037, 0.6637037037037037, 0.6711111111111111, 0.6548148148148148], "f1": [0.6801724137931036, 0.6419233784639105, 0.6400800840569913, 0.6612928334124347, 0.6476519454749977], "time": [0.7199761867523193, 0.7300360202789307, 0.75486159324646, 0.6808362007141113, 0.8132879734039307]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6, 0.5807407407407408, 0.6, 0.5703703703703704, 0.5851851851851851], "precision": [0.7405039742939286, 0.7573037918871253, 0.6143483023001095, 0.6835741571167581, 0.7167919799498748], "recall": [0.6, 0.5807407407407408, 0.6, 0.5703703703703704, 0.5851851851851851], "f1": [0.4749270403542317, 0.4313776897476417, 0.46464868007293286, 0.42306878306878304, 0.44650691981184243], "time": [0.35995054244995117, 0.3851048946380615, 0.3501548767089844, 0.3601987361907959, 0.3476104736328125]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x00000213BCB725E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.4325925925925926, 0.39555555555555555, 0.4666666666666667, 0.44296296296296295, 0.42074074074074075], "precision": [0.7564629905613512, 0.6002693602693602, 0.7528067560854447, 0.5985440213296238, 0.6117802210006108], "recall": [0.4325925925925926, 0.39555555555555555, 0.4666666666666667, 0.44296296296296295, 0.42074074074074075], "f1": [0.2679975500196905, 0.24581957083473766, 0.3035567755836949, 0.31883879987596003, 0.2609697867115178], "time": [5.120373010635376, 5.080293655395508, 5.234997749328613, 5.229867458343506, 5.210048198699951]}
