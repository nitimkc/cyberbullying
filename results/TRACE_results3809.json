{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression (TruncatedSVD)", "accuracy": [0.7, 0.7121951219512195, 0.7048780487804878, 0.7048780487804878, 0.697560975609756, 0.7073170731707317, 0.6731707317073171, 0.7041564792176039, 0.6577017114914425, 0.6968215158924206, 0.7457212713936431, 0.7041564792176039], "precision": [0.6982464017560925, 0.7087816015146453, 0.7036579387852702, 0.7019533408884101, 0.6945395686878144, 0.7057175915212663, 0.6681689068029261, 0.7034630598501782, 0.6534405173321863, 0.6915875676611114, 0.7439649043773633, 0.7023972839088616], "recall": [0.7, 0.7121951219512195, 0.7048780487804878, 0.7048780487804878, 0.697560975609756, 0.7073170731707317, 0.6731707317073171, 0.7041564792176039, 0.6577017114914425, 0.6968215158924206, 0.7457212713936431, 0.7041564792176039], "f1": [0.6975921243548693, 0.7083694132427338, 0.7035514849995361, 0.7026935196050289, 0.6926806044727364, 0.7040282578474486, 0.6690362691103192, 0.6990381878173305, 0.650878704858077, 0.6897263168235261, 0.7436708406303663, 0.6971968272132621], "time": [1489.2160971164703, 22.10935115814209, 25.299970626831055, 23.68921995162964, 21.990220069885254, 21.7108211517334, 21.665254831314087, 21.244919300079346, 21.475163459777832, 21.109822034835815, 21.294842004776, 21.05955147743225]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='warn', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='warn', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)", "name": "LogisticRegression", "accuracy": [0.7317073170731707, 0.748780487804878, 0.7146341463414634, 0.675609756097561, 0.7195121951219512, 0.6951219512195121, 0.651219512195122, 0.7212713936430318, 0.6968215158924206, 0.6772616136919315, 0.7237163814180929, 0.6992665036674817], "precision": [0.7280798002430309, 0.7483479061205707, 0.7140371948096447, 0.6738070781245447, 0.7216185657471395, 0.6897578328301268, 0.6591493033264048, 0.7185802388087908, 0.6950122723529886, 0.6739286295824587, 0.7225202211253094, 0.6963159932533142], "recall": [0.7317073170731707, 0.748780487804878, 0.7146341463414634, 0.675609756097561, 0.7195121951219512, 0.6951219512195121, 0.651219512195122, 0.7212713936430318, 0.6968215158924206, 0.6772616136919315, 0.7237163814180929, 0.6992665036674817], "f1": [0.7280813704709874, 0.7446322432739843, 0.7133683336181577, 0.6729077412513256, 0.7151094062869412, 0.6905599067096146, 0.6455253524820572, 0.7173495982336013, 0.6895789615540111, 0.6751864560004932, 0.7230786250654127, 0.6936043843805708], "time": [1.3754255771636963, 1.3355562686920166, 1.6104865074157715, 1.5400738716125488, 1.3154239654541016, 1.3299124240875244, 1.3308520317077637, 1.6599063873291016, 1.530061960220337, 1.724750280380249, 1.3580670356750488, 1.2877957820892334]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier (TruncatedSVD)", "accuracy": [0.7195121951219512, 0.6902439024390243, 0.6951219512195121, 0.6634146341463415, 0.7170731707317073, 0.7121951219512195, 0.7195121951219512, 0.6503667481662592, 0.7334963325183375, 0.6943765281173594, 0.6894865525672371, 0.6894865525672371], "precision": [0.7198325189867181, 0.7121102782128738, 0.7066812041703142, 0.6759404287724294, 0.7197991391678623, 0.7108868219732966, 0.7232942210503186, 0.6705032008183021, 0.7517211573999808, 0.6934525030359632, 0.6952083396965772, 0.6902300013601695], "recall": [0.7195121951219512, 0.6902439024390243, 0.6951219512195121, 0.6634146341463415, 0.7170731707317073, 0.7121951219512195, 0.7195121951219512, 0.6503667481662592, 0.7334963325183375, 0.6943765281173594, 0.6894865525672371, 0.6894865525672371], "f1": [0.7196680421095973, 0.6920674208203381, 0.6983417250282555, 0.6666139131021048, 0.7181628553397917, 0.7112670819887797, 0.7206813821772717, 0.6501535533356068, 0.7344565336604145, 0.6935762387445783, 0.6844214274679283, 0.6810000559176971], "time": [21.198467254638672, 21.458223342895508, 20.258976459503174, 22.06527018547058, 21.81234049797058, 22.050457239151, 22.45469379425049, 21.908053874969482, 21.7612361907959, 22.29133367538452, 21.993016958236694, 22.75596261024475]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=None,\n                               shuffle=True, tol=0.001, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)", "name": "SGDClassifier", "accuracy": [0.6829268292682927, 0.6878048780487804, 0.6902439024390243, 0.724390243902439, 0.675609756097561, 0.6560975609756098, 0.7024390243902439, 0.7139364303178484, 0.684596577017115, 0.6919315403422983, 0.6968215158924206, 0.6894865525672371], "precision": [0.6978044079005793, 0.6930304648481845, 0.6960059912493739, 0.7414097765311239, 0.6785739505027262, 0.6646794256199766, 0.7020593276203033, 0.7195689538125706, 0.7026345883335146, 0.6968062947253505, 0.7038137687867254, 0.6915711594543019], "recall": [0.6829268292682927, 0.6878048780487804, 0.6902439024390243, 0.724390243902439, 0.675609756097561, 0.6560975609756098, 0.7024390243902439, 0.7139364303178484, 0.684596577017115, 0.6919315403422983, 0.6968215158924206, 0.6894865525672371], "f1": [0.6851130696180912, 0.6893753292172695, 0.6922066765528344, 0.7270277926807996, 0.6767677624602333, 0.658338541719504, 0.7021821433772718, 0.7148739214438438, 0.6874151752362134, 0.6930326322422493, 0.6983089748228388, 0.6901956886869686], "time": [1.3199667930603027, 1.2851855754852295, 1.3253588676452637, 1.2901229858398438, 1.250074863433838, 1.2748217582702637, 1.3050501346588135, 1.3300609588623047, 1.6299188137054443, 1.2828612327575684, 1.320199966430664, 1.4062035083770752]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_word...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3,\n                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n                     probability=False, random_state=None, shrinking=True,\n                     tol=0.001, verbose=False))],\n         verbose=False)", "name": "SVC", "accuracy": [0.6878048780487804, 0.7121951219512195, 0.6804878048780488, 0.6926829268292682, 0.7560975609756098, 0.6878048780487804, 0.7170731707317073, 0.706601466992665, 0.706601466992665, 0.6821515892420538, 0.706601466992665, 0.7017114914425427], "precision": [0.6887713795358626, 0.7208944049298817, 0.680385109114249, 0.7029634949584674, 0.7597546800729534, 0.6913346517248956, 0.7180810010865085, 0.7135343830405012, 0.7068540158380678, 0.6797410031811105, 0.714222259214494, 0.7050813353991837], "recall": [0.6878048780487804, 0.7121951219512195, 0.6804878048780488, 0.6926829268292682, 0.7560975609756098, 0.6878048780487804, 0.7170731707317073, 0.706601466992665, 0.706601466992665, 0.6821515892420538, 0.706601466992665, 0.7017114914425427], "f1": [0.6881409732757614, 0.7151806946168386, 0.6804324066414305, 0.6958484115779281, 0.7572772727922706, 0.6889827164251473, 0.7164790135873869, 0.7087067331810823, 0.7067108387401785, 0.6792861804807752, 0.709051657641697, 0.7029779144729678], "time": [7.534895420074463, 7.804995059967041, 8.455247402191162, 8.419169664382935, 8.354617834091187, 7.6600682735443115, 7.65990686416626, 8.109550952911377, 7.88515567779541, 7.853100538253784, 7.599973440170288, 7.76037859916687]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)", "name": "MultinomialNB", "accuracy": [0.6195121951219512, 0.6292682926829268, 0.6073170731707317, 0.6390243902439025, 0.6317073170731707, 0.6146341463414634, 0.6902439024390243, 0.6381418092909535, 0.6136919315403423, 0.6136919315403423, 0.5990220048899756, 0.6674816625916871], "precision": [0.7037437733860261, 0.674247317815644, 0.6428629243904206, 0.6775988000521717, 0.6953896932017448, 0.6903699095682355, 0.7202224542963661, 0.7073889435747627, 0.659499812871201, 0.6854255193964672, 0.7047092829777695, 0.6826057964571837], "recall": [0.6195121951219512, 0.6292682926829268, 0.6073170731707317, 0.6390243902439025, 0.6317073170731707, 0.6146341463414634, 0.6902439024390243, 0.6381418092909535, 0.6136919315403423, 0.6136919315403423, 0.5990220048899756, 0.6674816625916871], "f1": [0.5466597855878985, 0.5598965262379897, 0.5241493616875978, 0.560835562599594, 0.5522887185739179, 0.5334476188493789, 0.6354675908337033, 0.5588999401281312, 0.533265876871705, 0.529769091886112, 0.5031033942532511, 0.6066336287837655], "time": [1.3001518249511719, 1.1700279712677002, 1.2700233459472656, 1.2248787879943848, 1.2304449081420898, 1.058415412902832, 1.121351957321167, 1.274949312210083, 1.2947633266448975, 1.2097673416137695, 1.4400687217712402, 1.5703558921813965]}
{"model": "Pipeline(memory=None,\n         steps=[('normalize', TextNormalizer_lemmatize(language=None)),\n                ('ngram_vect',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=False, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 2), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words=None, strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function identity at 0x0000021DBE0C25E8>,\n                                 use_idf=True, vocabulary=None)),\n                ('reduction',\n                 TruncatedSVD(algorithm='randomized', n_components=600,\n                              n_iter=5, random_state=None, tol=0.0)),\n                ('classifier', GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)", "name": "GaussianNB (TruncatedSVD)", "accuracy": [0.5756097560975609, 0.6048780487804878, 0.5975609756097561, 0.6414634146341464, 0.6268292682926829, 0.6365853658536585, 0.6317073170731707, 0.6308068459657702, 0.589242053789731, 0.6308068459657702, 0.6039119804400978, 0.6430317848410758], "precision": [0.5562771790508603, 0.6090950320155581, 0.5840030453816244, 0.66133997620464, 0.6190660321237359, 0.6605674212819477, 0.6264915572232646, 0.6225744649211691, 0.5908762144168819, 0.6273477505300952, 0.6067822075855607, 0.6395178051210014], "recall": [0.5756097560975609, 0.6048780487804878, 0.5975609756097561, 0.6414634146341464, 0.6268292682926829, 0.6365853658536585, 0.6317073170731707, 0.6308068459657702, 0.589242053789731, 0.6308068459657702, 0.6039119804400978, 0.6430317848410758], "f1": [0.5195405826345146, 0.5610485858847951, 0.5436455463193276, 0.6058449607575992, 0.5906851226838541, 0.5717266949132797, 0.5980286351536083, 0.6070917329190009, 0.5377295361410517, 0.5906961932227415, 0.5344965910759054, 0.6080672835175723], "time": [21.05150008201599, 22.16722273826599, 21.35132908821106, 21.931190729141235, 22.74591064453125, 22.03602385520935, 22.011219263076782, 21.67982053756714, 22.224912643432617, 24.11998414993286, 23.259934902191162, 24.58227300643921]}
